{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random as rand\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from numpy import unique\n",
    "from sklearn.cluster import MeanShift\n",
    "from scipy.stats import poisson\n",
    "from fbm import FBM\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from numpy import absolute\n",
    "from numpy import mean as m\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if I use this for distributing the resource of vnf among the micro vnfs. Run this defination for each vnf and get the info of how many micro vnfs can be formed.\n",
    "for example the vnf 0 requested resource is 5, and the requested resources are divided into 3,2. so we can say that we need three microvnfs for that vnf.\n",
    "This will provide two things,  a way to say each vnf can be divded into so and so micro services, which is basically the info provide by the service provides.\n",
    "secondly, we will know how much resource each micro vnfs are requested for and from there we can try to implement the concept of decompostions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extra_note = 'Introducing Traffic'\n",
    "trial = 'Trial_DDQN_100'\n",
    "model = 'DDQN'\n",
    "pr_HL = '6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as TT\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(number, decimals=0):\n",
    "    \"\"\"\n",
    "    Returns a value truncated to a specific number of decimal places.\n",
    "    \"\"\"\n",
    "    if not isinstance(decimals, int):\n",
    "        raise TypeError(\"decimal places must be an integer.\")\n",
    "    elif decimals < 0:\n",
    "        raise ValueError(\"decimal places has to be 0 or more.\")\n",
    "    elif decimals == 0:\n",
    "        return math.trunc(number)\n",
    "\n",
    "    factor = 10.0 ** decimals\n",
    "    return math.trunc(number * factor) / factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trail():\n",
    "    def __init__(self, num_episode, num_sfc, vNetwork_service_er, vNetwork_service_details_er, max_vnf_resource,\n",
    "                 vnf_per_service, vnf_node_resources, vnf_link_resources, mean, var):\n",
    "        \n",
    "        self.num_episode = num_episode\n",
    "        self.num_sfc = num_sfc\n",
    "        self.vNetwork_service_er = vNetwork_service_er\n",
    "        self.vNetwork_service_details_er = vNetwork_service_details_er\n",
    "        self.vnf_node_resources = vnf_node_resources\n",
    "        self.vnf_link_resources= vnf_link_resources\n",
    "        self.vnf_per_service = vnf_per_service\n",
    "        #print(self.vnf_per_service)\n",
    "        self.overall_episode_serviceclass = []\n",
    "        \n",
    "        for episode in range(self.num_episode): #1 Runtime = 200 episode\n",
    "            #print(\"episode\",episode)\n",
    "            self.episode = episode\n",
    "            service =[]\n",
    "            service_details =[]\n",
    "            rand_seed_collection =[]\n",
    "            vnf_per_episode = []\n",
    "            self.episode_serviceclass = []\n",
    "            norm_QoS_policy_val = []\n",
    "            env_priority = []\n",
    "            \n",
    "            # Generating seeds to control the isolated nodes\n",
    "            seed_array = [1,3,4,7,8,9,10,13,14,16,18,21,22,25,26,28,29,30,31,32,38,39,41,42,43,44,45,47,48,49,50,51,54,58,59,60,62,63,65,66,67,69,71,73,75,76,77,78,79,80,85,86,88,89,90,91,94,96,98,99,100 ]\n",
    "            \n",
    "            # QoS metric\n",
    "            #Flow behaviour: each SFC can be either elastic or inelastic flow. \n",
    "            elastic_flow, inelastic_flow = 0, 1\n",
    "            dict_flow_behviour = {}\n",
    "            flo_mu, flo_sigma = 0.5, 0.1\n",
    "            normal_flow_distribution = np.random.normal(flo_mu, flo_sigma, self.num_sfc)\n",
    "            int_normal_flow_distribution = np.rint(normal_flow_distribution)\n",
    "            #print(normal_flow_distribution)\n",
    "            #print(int_normal_flow_distribution)\n",
    "            #normalizing\n",
    "            norm_flow_distribution = preprocessing.normalize(np.reshape(int_normal_flow_distribution, (1, self.num_sfc)))\n",
    "            #print(\"norm_flow_distribution: \", norm_flow_distribution)\n",
    "                                           \n",
    "            #Create the bins and histogram\n",
    "            #count, bins, ignored = plt.hist(normal_flow_distribution, 20) \n",
    "            #Plot the distribution curve\n",
    "            #plt.plot(bins, 1/(flo_sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - flo_mu)**2 / (2 * flo_sigma**2) ), linewidth=3, color='y')\n",
    "            #plt.show()\n",
    "            \n",
    "            # QoS Rating: G.1010 rating \n",
    "            # Delay: Responsive(~ 1s), Interactive(<< 1s), Timely(~10s), non_critical/Background(>> 10s)  \n",
    "            dict_QoSRating = {}\n",
    "            qos_mu, qos_sigma = 2, 0.8\n",
    "            normal_QoSRating_distribution     = np.random.normal(qos_mu, qos_sigma, self.num_sfc) \n",
    "            int_normal_QoSRating_distribution = np.rint(normal_QoSRating_distribution)\n",
    "            #print(normal_QoSRating_distribution)\n",
    "            #print(\"QoS Rating: \", int_normal_QoSRating_distribution)\n",
    "            #normalizing\n",
    "            norm_QoSRating = preprocessing.normalize(np.reshape(int_normal_QoSRating_distribution, (1, self.num_sfc)))\n",
    "            #print(\"norm_QoSRating: \", norm_QoSRating)\n",
    "\n",
    "            #common \n",
    "            mu, sigma =  4, 10             \n",
    "            # Jitter  # (1 microsec (0.001 ms) to 30 ms)\n",
    "            arr_seed = [215, 283, 284, 414, 531, 702, 717, 763, 813]\n",
    "            seed = rand.choices(arr_seed)\n",
    "            np.random.seed(seed[0])\n",
    "            normal_jitter_distribution = abs(np.random.normal(mu, sigma, self.num_sfc))\n",
    "            \n",
    "            #print(\"normal_jitter_distribution: \", normal_jitter_distribution)\n",
    "            #normalizing\n",
    "            norm_jitter_distribution = preprocessing.normalize(np.reshape(normal_jitter_distribution, (1, self.num_sfc)))\n",
    "            #print(\"norm_jitter_distribution: \", norm_jitter_distribution)\n",
    "\n",
    "            # Delay: range: 0.1 ms to 30ms: URLLC to Best Effort\n",
    "            arr_seed = [11, 13, 27, 31, 58, 76, 78, 87, 91, 119] \n",
    "            seed = rand.choices(arr_seed)\n",
    "            np.random.seed(seed[0])\n",
    "            normal_delay_distribution = abs(np.random.normal(mu, sigma, self.num_sfc))\n",
    "            #normal_delay_distribution = truncate(normal_delay_distribution,2)\n",
    "            #print(\"normal_delay_distribution: \", normal_delay_distribution)\n",
    "            # normalised this delay \n",
    "            norm_normal_delay_distribution = preprocessing.normalize(np.reshape(normal_delay_distribution, (1, self.num_sfc)))\n",
    "            #print(\"norm_normal_delay_distribution: \", norm_normal_delay_distribution)\n",
    "            \n",
    "            #Reliability: range:  1-10^-9 to 1-10^-2    # not using this 03052022        \n",
    "            r_mu, r_sigma = 5, 1.5\n",
    "            arr_seed = [0, 1, 2, 3, 4, 6, 7, 9, 11, 12, 13, 18]\n",
    "            seed = rand.choices(arr_seed)\n",
    "            np.random.seed(seed[0])\n",
    "            #normal_reliability_distribution = abs(np.random.normal(r_mu, r_sigma, self.num_sfc))\n",
    "            #(\"normal_reliability_distribution: \", normal_reliability_distribution)\n",
    "            #norm_reliability_distribution = preprocessing.normalize(np.reshape(normal_reliability_distribution, (1, self.num_sfc)))\n",
    "            #print(\"norm_reliability_distribution: \", norm_reliability_distribution)\n",
    "            #print(max(normal_reliability_distribution), min(normal_reliability_distribution))\n",
    "            #normal_reliability_distribution =  truncate(normal_reliability_distribution,2)\n",
    "            \n",
    "            # waiting time\n",
    "            wt_mu, wt_sigma = 5, 10\n",
    "            normal_watingtime_distribution = abs(np.random.normal(wt_mu, wt_sigma, self.num_sfc))\n",
    "            #normal_watingtime_distribution = truncate(normal_watingtime_distribution,2)\n",
    "            #print(\"normal_watingtime_distribution: \",  normal_watingtime_distribution)\n",
    "            #print(max(normal_watingtime_distribution), min(normal_watingtime_distribution))\n",
    "            #count, bins, ignored = plt.hist(normal_watingtime_distribution, num_sfc) \n",
    "            #Plot the distribution curve\n",
    "            #plt.plot(bins, 1/(wt_sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - wt_mu)**2 / (2 * wt_sigma**2) ), linewidth=3, color='y')\n",
    "            #plt.show()\n",
    "            \n",
    "            # normalised this delay \n",
    "            norm_watingtime_distribution = preprocessing.normalize(np.reshape(normal_watingtime_distribution, (1, self.num_sfc)))\n",
    "            #print(\"norm_watingtime_distribution: \", norm_watingtime_distribution)\n",
    "            #print(\"--> \",max(norm_watingtime_distribution[0]), min(norm_watingtime_distribution[0]))\n",
    "\n",
    "            \n",
    "            #Packet Loss: Each service can tolerate 0%- <5% packet Loss depending upon the application or service type.\n",
    "            pl_mu, pl_sigma = 2, 1\n",
    "            normal_packetloss_distribution = abs(np.random.normal(pl_mu, pl_sigma, self.num_sfc)) \n",
    "            # (0% to less than 10%) packetloss per serivce \n",
    "            #print(\"normal_packetloss_distribution: \", normal_packetloss_distribution)\n",
    "            norm_packetloss_distribution = preprocessing.normalize(np.reshape(normal_packetloss_distribution, (1, self.num_sfc)))\n",
    "            #print(\"norm_packetloss_distribution: \", norm_packetloss_distribution)\n",
    "            \n",
    "            for i in range(self.num_sfc):\n",
    "                data = [truncate(1-norm_normal_delay_distribution[0][i],2), #delay\n",
    "                        #norm_flow_distribution[0][i], # flow dist.\n",
    "                        truncate(1-norm_jitter_distribution[0][i],2), #jitter\n",
    "                        truncate(1-norm_packetloss_distribution[0][i],2), #packetloss\n",
    "                        #truncate(1-norm_watingtime_distribution[0][i],2) #wating time \n",
    "                       ]\n",
    "                data = np.array(data,dtype='float64')\n",
    "                #print(data)\n",
    "                sum_data = sum(data)\n",
    "                #print(sum_data)\n",
    "                \n",
    "                norm_QoS_policy_val.append(sum_data)\n",
    "                #print(\"\")\n",
    "               \n",
    "            #print(\"norm_QoS_policy_val: \", type(norm_QoS_policy_val))\n",
    "            #print(\"\")\n",
    "            #print(\"MAX: \", max(norm_QoS_policy_val))\n",
    "\n",
    "            env_priority = Normalization(norm_QoS_policy_val, max(norm_QoS_policy_val), min(norm_QoS_policy_val))\n",
    "            #preprocessing.normalize(np.reshape(norm_QoS_policy_val, (1, self.num_sfc)))\n",
    "            #print(env_priority)\n",
    "            \n",
    "            #print(\"norm_QoS_policy_val: \", norm_QoS_policy_val)\n",
    "            #print(\"SFC, flow behaviour, delay, jitter, Packetloss, reliability, wating time, env priority\")\n",
    "            \n",
    "            #reliability\n",
    "            overall_pow = [1,2,3,4,5,6,7,8,9]\n",
    "            rel_factor, rel_pow, rel_norm_val = [], [], []\n",
    "            for y in overall_pow:\n",
    "                a = (1 - (pow(10, -y)))*100\n",
    "                norm_val = (Normalization(y, max(overall_pow), min(overall_pow)))\n",
    "                rel_factor.append(a)\n",
    "                rel_pow.append(y)\n",
    "                rel_norm_val.append(norm_val)\n",
    "\n",
    "            #print(rel_factor, rel_pow, rel_norm_val)\n",
    "            \n",
    "            #for each episode generating services.\n",
    "            for sfc in range(self.num_sfc): # number of VNF-FGs # time-step =100\n",
    "                #print(\"Service\", sfc)\n",
    "                self.sfc = sfc\n",
    "                requested_services = []\n",
    "                requested_nodes    = []\n",
    "                \n",
    "                #self.vnf_per_service     = []\n",
    "                rand_seed =  rand.choice(seed_array)\n",
    "                #print(\"Seed\", rand_seed)\n",
    "                rand_seed_collection.append(rand_seed)\n",
    "                #n = self.vnf_per_service #2to 10 #rand.randrange(2) #no.of nodes=VNF \n",
    "                #self.vnf_per_service = n\n",
    "                #print(vnf_per_service)\n",
    "                #vnf_per_episode.append(self.vnf_per_service)\n",
    "                \n",
    "                #reliability\n",
    "                y = rand.choice(overall_pow)\n",
    "                reliability_factor   = rel_factor[y-1]\n",
    "                reliability_pow      = rel_pow[y-1]\n",
    "                reliability_norm_val = rel_norm_val[y-1]\n",
    "                #print(reliability_factor, rel_pow[y-1], rel_norm_val[y-1])\n",
    "                \n",
    "                #Assigning \n",
    "                # Flow Behaviour\n",
    "                #print(int_normal_flow_distribution[self.sfc])  \n",
    "                if int_normal_flow_distribution[self.sfc] == 1:\n",
    "                    dict_flow_behviour[self.sfc] = \"inelastic_flow\"\n",
    "                else: \n",
    "                    dict_flow_behviour[self.sfc] = \"elastic_flow\"\n",
    "                #print(dict_flow_behviour[self.sfc])\n",
    "                \n",
    "                # QoS Rating: Initilization of this QoS rating based on the threshold delay\n",
    "                #if normal_delay_distribution[self.sfc] >>= 10:\n",
    "                \n",
    "                #if int_normal_QoSRating_distribution[self.sfc] == 1:\n",
    "                #    dict_QoSRating[self.sfc] = \"Responsive\" \n",
    "                #elif int_normal_QoSRating_distribution[self.sfc] == 2:\n",
    "                #    dict_QoSRating[self.sfc] = \"Interactive\"\n",
    "                #    dict_flow_behviour[self.sfc] = \"inelastic_flow\" \n",
    "                #elif int_normal_QoSRating_distribution[self.sfc] == 3:\n",
    "                #    dict_QoSRating[self.sfc] = \"Timely\"        \n",
    "                #else:\n",
    "                 #   dict_QoSRating[self.sfc] = \"non-critical\"\n",
    "                 #   dict_flow_behviour[self.sfc] = \"elastic_flow\"\n",
    "                #print(dict_QoSRating)\n",
    "\n",
    "                eps_value = 0.3\n",
    "                p = (1 + eps_value)*(math.log(self.vnf_per_service)/self.vnf_per_service) #prob of connectivity\n",
    "                #print(p)\n",
    "                # Generating Graphs\n",
    "                # E-R graph\n",
    "                self.er_graph_seed = nx.erdos_renyi_graph(n= self.vnf_per_service, p = p, seed=rand_seed, directed =True)\n",
    "                #nx.draw(self.er_graph_seed, with_labels=True) \n",
    "                #plt.show()\n",
    "                #flow_behaviour = dict_flow_behviour[self.sfc]\n",
    "                #QoSrating = dict_QoSRating[self.sfc]\n",
    "                \n",
    "                sfc_serviceclass = [self.sfc, \n",
    "                                    int_normal_flow_distribution[self.sfc], \n",
    "                                    dict_flow_behviour[self.sfc],\n",
    "                                    #dict_QoSRating[self.sfc], \n",
    "                                    truncate(normal_delay_distribution[self.sfc],2),\n",
    "                                    truncate(normal_jitter_distribution[self.sfc],2),\n",
    "                                    truncate(normal_packetloss_distribution[self.sfc],2),\n",
    "                                    reliability_factor,\n",
    "                                    truncate(normal_watingtime_distribution[self.sfc],2),\n",
    "                                    env_priority[self.sfc]\n",
    "                                   ]\n",
    "            \n",
    "                #print(\"sfc_serviceclass: \", sfc_serviceclass)\n",
    "                self.episode_serviceclass.append(sfc_serviceclass)\n",
    "                \n",
    "                for vnf in self.er_graph_seed.nodes():\n",
    "                    self.vnf = vnf\n",
    "                    #print(\"VNF\", vnf)\n",
    "                    mu, sigma = mean, var\n",
    "                    normal_cpu_res = np.rint(np.random.normal(mu, sigma, 1))\n",
    "                    self.cpu_req   = normal_cpu_res[0]\n",
    "                    #print(\"self.cpu_req\", self.cpu_req)\n",
    "                    if self.cpu_req > max_vnf_resource:\n",
    "                        print(\"CPU greater than max:\", self.cpu_req)\n",
    "                    \n",
    "                    self.er_graph_seed.add_node(vnf,\n",
    "                                                episode = self.episode,\n",
    "                                                sfc     = self.sfc,\n",
    "                                                numVNF  = self.vnf_per_service,\n",
    "                                                vnf     = self.vnf,\n",
    "                                                cpu_req = self.cpu_req,\n",
    "                                                delay           = normal_delay_distribution[self.sfc], #delay\n",
    "                                                norm_delay      = 1-norm_normal_delay_distribution[0][self.sfc], #rule 1\n",
    "                                                jitter          = normal_jitter_distribution[self.sfc],\n",
    "                                                norm_jitter     = 1-norm_jitter_distribution[0][self.sfc], #rule 2\n",
    "                                                packetloss      = normal_packetloss_distribution[self.sfc],\n",
    "                                                norm_packetloss = 1-norm_packetloss_distribution[0][self.sfc], #rule 3\n",
    "                                                reliability     = reliability_norm_val,\n",
    "                                                reliability_factor = reliability_factor, \n",
    "                                                flow_behaviour  = dict_flow_behviour[self.sfc],  \n",
    "                                                int_flow_behaviour = int_normal_flow_distribution[self.sfc],\n",
    "                                                norm_flow       = norm_flow_distribution[0][self.sfc], #rule 4\n",
    "                                                theo_wating_time = normal_watingtime_distribution[self.sfc],\n",
    "                                                norm_waiting_time = 1-norm_watingtime_distribution[0][self.sfc], #rule 5\n",
    "                                                norm_QoS_policy_val = norm_QoS_policy_val[self.sfc],\n",
    "                                                env_priority   = env_priority[self.sfc],\n",
    "                                                QoSRating      = None, #using clustering \n",
    "                                                app_category   = None, #using clustering\n",
    "                                                priority_level = None, #Using clustering    \n",
    "                                                priority_class = None,\n",
    "                                                microvnf_done_status = None,\n",
    "                                                numMicroVNF = None, #self.decompose_candidate[0],\n",
    "                                                microvnf_res_component = None, #self.decompose_candidate[1],\n",
    "                                                decompose_candidate = None, #self.decompose_candidate[2]\n",
    "                                                emb_status = None,\n",
    "                                                sfc_deployment_time = None,\n",
    "                                                traffic_packets = None, \n",
    "                                                traffic_load = None,\n",
    "                                                classifier_class = None\n",
    "                                                \n",
    "                                               )\n",
    "                                \n",
    "                self.vnf_node_resources = list(self.er_graph_seed.nodes(data=True))\n",
    "                #print(self.vnf_node_resources)\n",
    "\n",
    "                self.num_of_vl = nx.number_of_edges(self.er_graph_seed)\n",
    "\n",
    "                link_count = 0\n",
    "                for link in self.er_graph_seed.edges():\n",
    "                    #print(link)\n",
    "\n",
    "                    #INITIALIZATION RESOURCE\n",
    "                    # Allocating the bandwidth resouce.\n",
    "                    self.er_graph_seed.add_edge(link[0],link[1],\n",
    "                                                src_vnf  = link[0],\n",
    "                                                dest_vnf = link[1],\n",
    "                                                #src_microvnf = src_microvnf,\n",
    "                                                #dest_microvnf = dest_microvnf,\n",
    "                                                episode = self.episode,\n",
    "                                                sfc = self.sfc,\n",
    "                                                numVNF = self.vnf_per_service,\n",
    "                                                numvl = self.num_of_vl,\n",
    "                                                link_num = link_count,\n",
    "                                                bw  = rand.randrange(1, 20),  #Mbps\n",
    "                                                latency = rand.randrange(1,5) #ms #uniform or normalizations #check this out from the literatue \n",
    "                                               )\n",
    "                    link_count += 1\n",
    "                self.vnf_link_resources= list(self.er_graph_seed.edges(data=True))\n",
    "                #print(self.vnf_link_resources)\n",
    "                requested_services.append(self.er_graph_seed.nodes) \n",
    "                requested_nodes.append(self.vnf_node_resources)   \n",
    "                requested_services.append(self.er_graph_seed.edges)\n",
    "                requested_nodes.append(self.vnf_link_resources) \n",
    "                service.append(requested_services)\n",
    "                service_details.append(requested_nodes)\n",
    "            \n",
    "            self.vNetwork_service_er.append(service)\n",
    "            self.vNetwork_service_details_er.append(service_details)\n",
    "            self.overall_episode_serviceclass.append(self.episode_serviceclass)\n",
    "        #print(\"here\",self.vNetwork_service_details_er)\n",
    "        #print(\"\")\n",
    "                \n",
    "    def update_vnf(self, episode, sfc, vnf, decompose_candidate, microvnf_done_status):\n",
    "        count_epi = 0\n",
    "        for epi in self.vNetwork_service_details_er:\n",
    "            if episode == count_epi:\n",
    "                #print(\"vnf\", vnf, epi[sfc][0][vnf])\n",
    "                epi[sfc][0][vnf][1]['microvnf_done_status'] = microvnf_done_status\n",
    "                epi[sfc][0][vnf][1]['numMicroVNF'] =  decompose_candidate[0]\n",
    "                epi[sfc][0][vnf][1]['microvnf_res_component'] =  decompose_candidate[1]\n",
    "                epi[sfc][0][vnf][1]['decompose_candidate'] =  decompose_candidate[2]\n",
    "                #print(\"==vnf\", vnf, epi[sfc][0][vnf][1])  \n",
    "                #print(\"decompose_candidate[1]\", decompose_candidate[1])\n",
    "            count_epi += 1\n",
    "            \n",
    "    def reset(self, episode, sfc, vnf):\n",
    "        #print(\"in rest function\")\n",
    "        count_epi = 0\n",
    "        for epi in self.vNetwork_service_details_er:\n",
    "            if episode == count_epi:\n",
    "                #print(\"vnf\", vnf, epi[sfc][0][vnf])\n",
    "                epi[sfc][0][vnf][1]['microvnf_done_status'] = None\n",
    "                epi[sfc][0][vnf][1]['numMicroVNF'] =  None\n",
    "                epi[sfc][0][vnf][1]['microvnf_res_component'] =  None\n",
    "                epi[sfc][0][vnf][1]['decompose_candidate'] =  None\n",
    "                \n",
    "                #print(\"==vnf\", vnf, epi[sfc][0][vnf][1])  \n",
    "            count_epi += 1\n",
    "            \n",
    "    def priority_update(self, episode, sfc, vnf_per_service, priority_level, priority_class):\n",
    "        self.episode = episode\n",
    "        self.sfc = sfc\n",
    "        self.vnf_per_service = vnf_per_service\n",
    "        self.priority_level = priority_level\n",
    "        self.priority_class = priority_class\n",
    "        for vnf in range(self.vnf_per_service):\n",
    "            self.vNetwork_service_details_er[self.episode][self.sfc][0][vnf][1]['priority_level'] = self.priority_level\n",
    "            self.vNetwork_service_details_er[self.episode][self.sfc][0][vnf][1]['priority_class'] = self.priority_class  \n",
    "            \n",
    "            \n",
    "    def emb_update(self, episode, sfc, vnf_per_service, emb_status, sfc_deployment_time):\n",
    "        self.episode = episode\n",
    "        self.sfc = sfc\n",
    "        self.vnf_per_service = vnf_per_service\n",
    "        self.emb_status = emb_status\n",
    "        self.sfc_deployment_time = sfc_deployment_time\n",
    "        #print(\"emb updated: \", episode, sfc, vnf_per_service, emb_status, sfc_deployment_time)\n",
    "        for vnf in range(self.vnf_per_service):\n",
    "            self.vNetwork_service_details_er[self.episode][self.sfc][0][vnf][1]['emb_status'] = emb_status\n",
    "            self.vNetwork_service_details_er[self.episode][self.sfc][0][vnf][1]['sfc_deployment_time'] = sfc_deployment_time\n",
    "    \n",
    "    def traffic_update(self, episode, deployed_sfc, vnf_per_service, epi_traffic_packets_dict, epi_traffic_load_dict):\n",
    "        self.episode = episode\n",
    "        self.deployed_sfc = deployed_sfc\n",
    "        self.vnf_per_service = vnf_per_service\n",
    "        self.epi_traffic_packets_dict = epi_traffic_packets_dict\n",
    "        self.epi_traffic_load_dict = epi_traffic_load_dict        \n",
    "        for vnf in range(self.vnf_per_service):\n",
    "            self.vNetwork_service_details_er[self.episode][self.deployed_sfc][0][vnf][1]['traffic_packets'] = self.epi_traffic_packets_dict[self.deployed_sfc]\n",
    "            self.vNetwork_service_details_er[self.episode][self.deployed_sfc][0][vnf][1]['traffic_load'] = self.epi_traffic_load_dict[self.deployed_sfc]\n",
    "                  \n",
    "\n",
    "    def classifier_class_update(self, episode, sfc, vnf_per_service, traffic_pred, pred_num, pred_traffic_packet_per_sfc):\n",
    "        #print(\"updating Classifer class\")\n",
    "        self.episode = episode\n",
    "        self.sfc = sfc\n",
    "        self.vnf_per_service = vnf_per_service\n",
    "        self.traffic_pred = traffic_pred\n",
    "        self.pred_num = pred_num\n",
    "        self.pred_traffic_packet_per_sfc = pred_traffic_packet_per_sfc\n",
    "        for vnf in range(self.vnf_per_service):\n",
    "            self.vNetwork_service_details_er[self.episode][self.sfc][0][vnf][1]['classifier_class'] = self.traffic_pred\n",
    "            self.vNetwork_service_details_er[self.episode][self.sfc][0][vnf][1]['traffic_packets'] = self.pred_num\n",
    "            self.vNetwork_service_details_er[self.episode][self.sfc][0][vnf][1]['traffic_load'] = self.pred_traffic_packet_per_sfc\n",
    "                  \n",
    "        \n",
    "    def overall_reset(self, num_episode, num_sfc, vnf_per_service):\n",
    "        self.num_episode = num_episode\n",
    "        self.num_sfc = num_sfc\n",
    "        self.vnf_per_service = vnf_per_service\n",
    "        for epi in self.vNetwork_service_details_er:\n",
    "            for sfc in range(num_sfc):\n",
    "                for vnf in range(vnf_per_service):\n",
    "                    #print(\"vnf\", vnf, epi[sfc][0][vnf])\n",
    "                    epi[sfc][0][vnf][1]['microvnf_done_status'] = None\n",
    "                    epi[sfc][0][vnf][1]['numMicroVNF'] =  None\n",
    "                    epi[sfc][0][vnf][1]['microvnf_res_component'] =  None\n",
    "                    epi[sfc][0][vnf][1]['decompose_candidate'] =  None\n",
    "                    epi[sfc][0][vnf][1]['emb_status'] = None\n",
    "                    epi[sfc][0][vnf][1]['sfc_deployment_time'] = None\n",
    "                    epi[sfc][0][vnf][1]['traffic_packets'] = None \n",
    "                    epi[sfc][0][vnf][1]['traffic_load'] = None\n",
    "                    epi[sfc][0][vnf][1]['classifier_class'] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded parameters\n",
    "## Parameters\n",
    "Algorithm = ['Algorithm: DDQN+ Target Networks + Experience Replay']\n",
    "Hidden_layer, Neurons_per_HiddenLayer,drop_prob, Opimizer = 6, 300, 0, 'Optimizer: Adam' #Neural Network\n",
    "learning_rate, gamma,  eps_min = 0.0001, 0.99, 0.01 #Learning = 0.001#\n",
    "threshold_delay = 100 # Latency \n",
    "amount_local_reward, amount_local_penalty = 1000, -100 #Local Rewards\n",
    "mem_size, batch_size, replace  = 500000, 32, 10000 # Memory size\n",
    "\n",
    "alpha, beta, tau = 0.0001, 0.001, 0.001\n",
    "\n",
    "num_episode, num_sfc = 1 ,50 #e and num of SFC\n",
    "tr_mem_size, tr_batch_size = 10000, 32\n",
    "test_mem_size, test_batch_size = 500, 128\n",
    "acc_theshold_limit, n_splits, n_repeat = 0.8, 2, 2\n",
    "vnf_per_service = 5 #[5, 10, 15]\n",
    "max_vnf_resource = 5 #2\n",
    "mean, var = 3, 0.4\n",
    "overall_sfc = num_episode * num_sfc\n",
    "\n",
    "threshold_percentage = 800 #80 # #abve 20% of resource the decompose is activated\n",
    "threshold_decompose = (max_vnf_resource*threshold_percentage)/100\n",
    "print(threshold_decompose)\n",
    "eps_value = 0.3\n",
    "\n",
    "full_nodal_avail = 1\n",
    "threshold_nodal_decomp_percentage =  0 # 10 #\n",
    "threshold_nodal_decomp = (full_nodal_avail*threshold_nodal_decomp_percentage)/100\n",
    "print(threshold_nodal_decomp)\n",
    "\n",
    "\n",
    "traffic_batch_size = 32\n",
    "n_clusters = 2\n",
    "\n",
    "classifier_models ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "eps_dec = 1e-4 #3,4,5,6,7\n",
    "print(eps_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added on 12th April 2022\n",
    "def Normalization(data, data_max, data_min):\n",
    "    norm_data = ((data - data_min) / (data_max - data_min))\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "global_services =Trail(num_episode= num_episode, \n",
    "                          num_sfc =num_sfc, \n",
    "                          max_vnf_resource = max_vnf_resource, \n",
    "                          vnf_per_service = vnf_per_service,\n",
    "                          vNetwork_service_er =[], \n",
    "                          vNetwork_service_details_er=[],\n",
    "                          vnf_link_resources = [], \n",
    "                          vnf_node_resources=[], \n",
    "                          mean = mean, \n",
    "                          var = var\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, beta, n_state, fc_unit, n_actions, name):\n",
    "        \n",
    "        #print(\"Critic Network\")\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        self.beta      = beta\n",
    "        self.n_actions = n_actions\n",
    "        self.n_state   = n_state\n",
    "        self.fc_unit   = fc_unit\n",
    "        #self.drop      = nn.Dropout(p=drop_prob)\n",
    "        self.name      = name\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.n_state, self.fc_unit)\n",
    "        self.fc2 = nn.Linear(self.fc_unit, self.fc_unit)\n",
    "        self.fc3 = nn.Linear(self.fc_unit, self.fc_unit) #hidden 3\n",
    "        self.fc4 = nn.Linear(self.fc_unit, self.fc_unit) #hidden 4\n",
    "        self.fc5 = nn.Linear(self.fc_unit, self.fc_unit) #hidden 5      \n",
    "        self.fc6 = nn.Linear(self.fc_unit, self.fc_unit) #hidden 6\n",
    "        \n",
    "        #Layer Normalization\n",
    "        #self.ln1 = nn.LayerNorm(self.fc_unit)\n",
    "        #self.ln2 = nn.LayerNorm(self.fc_unit)\n",
    "        #self.bn3 = nn.LayerNorm(self.fc_unit)\n",
    "        #self.bn4 = nn.LayerNorm(self.fc_unit)\n",
    "        #self.bn5 = nn.LayerNorm(self.fc_unit)\n",
    "        #self.bn6 = nn.LayerNorm(self.fc_unit)\n",
    "        \n",
    "        #Batch Normalization\n",
    "        self.bn1 = nn.BatchNorm1d(self.fc_unit)\n",
    "        self.bn2 = nn.BatchNorm1d(self.fc_unit)\n",
    "        self.bn3 = nn.BatchNorm1d(self.fc_unit)\n",
    "        self.bn4 = nn.BatchNorm1d(self.fc_unit)\n",
    "        self.bn5 = nn.BatchNorm1d(self.fc_unit)\n",
    "        self.bn6 = nn.BatchNorm1d(self.fc_unit)\n",
    "        \n",
    "        self.action_value  = nn.Linear(self.n_actions, self.fc_unit)\n",
    "        self.q             = nn.Linear(self.fc_unit, 1)\n",
    "        \n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        self.fc1.weight.data.uniform_(-f1, f1)\n",
    "        self.fc1.bias.data.uniform_(-f1, f1)\n",
    "\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        self.fc2.weight.data.uniform_(-f2, f2)\n",
    "        self.fc2.bias.data.uniform_(-f2, f2)\n",
    "        \n",
    "        f3 = 1./np.sqrt(self.fc3.weight.data.size()[0])\n",
    "        self.fc3.weight.data.uniform_(-f3, f3)\n",
    "        self.fc3.bias.data.uniform_(-f3, f3)\n",
    "        \n",
    "        f4 = 1./np.sqrt(self.fc4.weight.data.size()[0])\n",
    "        self.fc4.weight.data.uniform_(-f4, f4)\n",
    "        self.fc4.bias.data.uniform_(-f4, f4)\n",
    "        \n",
    "        f5 = 1./np.sqrt(self.fc5.weight.data.size()[0])\n",
    "        self.fc5.weight.data.uniform_(-f5, f5)\n",
    "        self.fc5.bias.data.uniform_(-f5, f5)\n",
    "        \n",
    "        f6 = 1./np.sqrt(self.fc6.weight.data.size()[0])\n",
    "        self.fc6.weight.data.uniform_(-f6, f6)\n",
    "        self.fc6.bias.data.uniform_(-f6, f6)\n",
    "\n",
    "        f7 = 0.003\n",
    "        self.q.weight.data.uniform_(-f7, f7)\n",
    "        self.q.bias.data.uniform_(-f7, f7)\n",
    "\n",
    "        f8 = 1./np.sqrt(self.action_value.weight.data.size()[0])\n",
    "        self.action_value.weight.data.uniform_(-f8, f8)\n",
    "        self.action_value.bias.data.uniform_(-f8, f8)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.beta, weight_decay=0.01)\n",
    "        self.device = T.device('cpu') \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        #print(\"==> Critic action: \", action)\n",
    "        layer = self.fc1(state)\n",
    "        layer = self.bn1(layer)\n",
    "        layer = F.relu(layer)\n",
    "        \n",
    "        layer = self.fc2(layer)\n",
    "        layer = self.bn2(layer)\n",
    "        layer = F.relu(layer)\n",
    "        \n",
    "        layer = self.fc3(layer)\n",
    "        layer = self.bn3(layer)\n",
    "        layer = F.relu(layer)\n",
    "        \n",
    "        layer = self.fc4(layer)\n",
    "        layer = self.bn4(layer)\n",
    "        layer = F.relu(layer)\n",
    "        \n",
    "        layer = self.fc5(layer)\n",
    "        layer = self.bn5(layer)\n",
    "        layer = F.relu(layer)\n",
    "        \n",
    "        layer = self.fc6(layer)\n",
    "        layer = self.bn6(layer)\n",
    "        layer = F.relu(layer)\n",
    "\n",
    "        \n",
    "        action_value       = self.action_value(action)\n",
    "        state_action_value = F.relu(T.add(layer, action_value))\n",
    "        state_action_value = self.q(state_action_value)\n",
    "        #print(\"state_action_value : \", state_action_value )\n",
    "        return state_action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor Network\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, n_state, fc_unit, n_actions, name):\n",
    "        \n",
    "        super(ActorNetwork, self).__init__()\n",
    "        #print(\"Actor Network\")\n",
    "        self.alpha     = alpha\n",
    "        self.n_actions = n_actions\n",
    "        self.n_state   = n_state\n",
    "        self.fc_unit   = fc_unit\n",
    "        #self.drop      = nn.Dropout(p=drop_prob)\n",
    "        self.name      = name\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.n_state, self.fc_unit)\n",
    "        self.fc2 = nn.Linear(self.fc_unit, self.fc_unit)\n",
    "        self.fc3 = nn.Linear(self.fc_unit, self.fc_unit) #hidden 3\n",
    "        self.fc4 = nn.Linear(self.fc_unit, self.fc_unit) #hidden 4\n",
    "        self.fc5 = nn.Linear(self.fc_unit, self.fc_unit) #hidden 5      \n",
    "        self.fc6 = nn.Linear(self.fc_unit, self.fc_unit) #hidden 6\n",
    "        \n",
    "        #Layer Normalization\n",
    "        #self.bn1 = nn.LayerNorm(self.fc_unit)\n",
    "        #print(\"bc1: \", self.bn1)\n",
    "        #self.bn2 = nn.LayerNorm(self.fc_unit)\n",
    "        #print(\"bc2: \", self.bn2)\n",
    "        #self.bn3 = nn.LayerNorm(self.fc_unit)\n",
    "        #self.bn4 = nn.LayerNorm(self.fc_unit)\n",
    "        #self.bn5 = nn.LayerNorm(self.fc_unit)\n",
    "        #self.bn6 = nn.LayerNorm(self.fc_unit)\n",
    "        \n",
    "        #Batch Normalization\n",
    "        self.bn1 = nn.BatchNorm1d(self.fc_unit)\n",
    "        self.bn2 = nn.BatchNorm1d(self.fc_unit)\n",
    "        self.bn3 = nn.BatchNorm1d(self.fc_unit)\n",
    "        self.bn4 = nn.BatchNorm1d(self.fc_unit)\n",
    "        self.bn5 = nn.BatchNorm1d(self.fc_unit)\n",
    "        self.bn6 = nn.BatchNorm1d(self.fc_unit)\n",
    "        \n",
    "        self.mu  = nn.Linear(self.fc_unit, self.n_actions)\n",
    "        #print(\"mu\", self.mu)\n",
    "        \n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        self.fc1.weight.data.uniform_(-f1, f1)\n",
    "        self.fc1.bias.data.uniform_(-f1, f1)\n",
    "\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        self.fc2.weight.data.uniform_(-f2, f2)\n",
    "        self.fc2.bias.data.uniform_(-f2, f2)\n",
    "        \n",
    "        f3 = 1./np.sqrt(self.fc3.weight.data.size()[0])\n",
    "        self.fc3.weight.data.uniform_(-f3, f3)\n",
    "        self.fc3.bias.data.uniform_(-f3, f3)\n",
    "        \n",
    "        f4 = 1./np.sqrt(self.fc4.weight.data.size()[0])\n",
    "        self.fc4.weight.data.uniform_(-f4, f4)\n",
    "        self.fc4.bias.data.uniform_(-f4, f4)\n",
    "        \n",
    "        f5 = 1./np.sqrt(self.fc5.weight.data.size()[0])\n",
    "        self.fc5.weight.data.uniform_(-f5, f5)\n",
    "        self.fc5.bias.data.uniform_(-f5, f5)\n",
    "        \n",
    "        f6 = 1./np.sqrt(self.fc6.weight.data.size()[0])\n",
    "        self.fc6.weight.data.uniform_(-f6, f6)\n",
    "        self.fc6.bias.data.uniform_(-f6, f6)\n",
    "\n",
    "        f7 = 0.003\n",
    "        self.mu.weight.data.uniform_(-f7, f7)\n",
    "        self.mu.bias.data.uniform_(-f7, f7)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.alpha)\n",
    "        self.device = T.device('cpu') \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        #print(\"Actor forward state: \", state)\n",
    "        \n",
    "        layer = self.fc1(state)\n",
    "        layer = F.relu(self.bn1(layer))\n",
    "        \n",
    "        layer = self.fc2(layer)\n",
    "        layer = F.relu(self.bn2(layer))\n",
    "        \n",
    "        layer = self.fc3(layer)\n",
    "        layer = F.relu(self.bn3(layer))\n",
    "        \n",
    "        layer = self.fc4(layer)\n",
    "        layer = F.relu(self.bn4(layer))\n",
    "        \n",
    "        layer = self.fc5(layer)\n",
    "        layer = F.relu(self.bn5(layer))\n",
    "\n",
    "        layer = self.fc6(layer)\n",
    "        layer = F.relu(self.bn6(layer))\n",
    "        \n",
    "        #leaky Relu\n",
    "        #leaky_act = nn.LeakyReLU(0.1)\n",
    "        #L_layer = leaky_act(self.mu(layer))\n",
    "        #print(\"Leaky Relu: \", L_layer)\n",
    "        \n",
    "        #relu\n",
    "        #R_layer = F.relu(self.mu(layer))\n",
    "        #print(\"Relu: \", R_layer)\n",
    "        \n",
    "        #linear\n",
    "        layer = T.sigmoid(self.mu(layer))\n",
    "        #print(\"sigmoid: \", layer)\n",
    "        return layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Primary Network ##\n",
    "\n",
    "class PR_DDQN(nn.Module):\n",
    "    # nn.Module for constructing more hidden layers.\n",
    "    def __init__(self, lr, pr_n_actions, pr_n_state, fc_unit, drop_prob): #name, chkpt_dir\n",
    "        \n",
    "        #calls __inti__ method of nn.module class\n",
    "        super(PR_DDQN, self).__init__()\n",
    "        print(\"PR_DDQN\")\n",
    "        self.pr_n_state   = pr_n_state\n",
    "        self.fc_unit      = fc_unit\n",
    "        self.pr_n_actions = pr_n_actions\n",
    "        self.drop         = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        # Creating Fully Connected Layers between Input and Hidden layer of 128 neurons\n",
    "        self.fc1 = nn.Linear(*self.pr_n_state,self.fc_unit) #input - hidden 1\n",
    "        self.fc2 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 2 \n",
    "        #self.fc3 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 3\n",
    "        #self.fc4 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 4\n",
    "        #self.fc5 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 5      \n",
    "        #self.fc6 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 6\n",
    "        #self.fc7 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 7      \n",
    "        #self.fc8 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 8 \n",
    "        #self.fc9 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 9      \n",
    "        #self.fc10 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 10\n",
    "        # Creating Fully Connected Layers between Hidden layer of 128 neurons and Outout layer.\n",
    "        self.fc_op = nn.Linear(self.fc_unit, self.pr_n_actions) #output\n",
    "        # Using Adam's Optimiser\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)  \n",
    "        # Using Mean Square Error for calculating the Loss\n",
    "        self.loss = nn.MSELoss() \n",
    "        #checking of the GPU is availible or not.\n",
    "        self.device = T.device('cpu') \n",
    "        # sending the entire network to the device\n",
    "        self.to(self.device) \n",
    "    \n",
    "    # Feedforward\n",
    "    def forward(self, state): #activation\n",
    "        #print(\"forward state\",state)\n",
    "        layer = F.relu(self.drop(self.fc1(state))) # 1st hidden layer neuron activation\n",
    "        #print(\"forward layer1:\",layer1)\n",
    "        layer = F.relu(self.drop(self.fc2(layer)))\n",
    "        #layer = F.relu(self.drop(self.fc3(layer)))\n",
    "        #layer = F.relu(self.drop(self.fc4(layer)))\n",
    "        #layer = F.relu(self.drop(self.fc5(layer)))\n",
    "        #layer = F.relu(self.drop(self.fc6(layer)))\n",
    "        #layer = F.relu(self.drop(self.fc7(layer)))\n",
    "        #layer = F.relu(self.drop(self.fc8(layer)))\n",
    "        #layer = F.relu(self.drop(self.fc9(layer)))\n",
    "        #layer = F.relu(self.drop(self.fc10(layer)))\n",
    "        actions = self.fc_op(layer)# no activation function currently  #output later resul\n",
    "        #print(\"forward actions\", actions)\n",
    "        #print(\"\")\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Primary Network ##\n",
    "\n",
    "class LinearDDQN(nn.Module):\n",
    "    # nn.Module for constructing more hidden layers.\n",
    "    def __init__(self, lr, n_actions, n_state, fc_unit, drop_prob): #name, chkpt_dir\n",
    "        \n",
    "        #calls __inti__ method of nn.module class\n",
    "        super(LinearDDQN, self).__init__()\n",
    "        self.n_state   = n_state\n",
    "        self.fc_unit   = fc_unit\n",
    "        self.n_actions = n_actions\n",
    "        self.drop      = nn.Dropout(p=drop_prob)\n",
    "        # Creating Fully Connected Layers between Input and Hidden layer of 128 neurons\n",
    "        self.fc1 = nn.Linear(*self.n_state,self.fc_unit) #input - hidden 1\n",
    "        self.fc2 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 2 \n",
    "        self.fc3 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 3\n",
    "        self.fc4 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 4\n",
    "        self.fc5 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 5      \n",
    "        self.fc6 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 6\n",
    "        #self.fc7 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 7      \n",
    "        #self.fc8 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 8 \n",
    "        #self.fc9 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 9      \n",
    "        #self.fc10 = nn.Linear(self.fc_unit,self.fc_unit) #hidden 10\n",
    "        # Creating Fully Connected Layers between Hidden layer of 128 neurons and Outout layer.\n",
    "        self.fc_op = nn.Linear(self.fc_unit, self.n_actions) #output\n",
    "        # Using Adam's Optimiser\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr) \n",
    "        # Using Mean Square Error for calculating the Loss\n",
    "        self.loss = nn.MSELoss() \n",
    "        #checking of the GPU is availible or not.\n",
    "        #print(self.device)\n",
    "        self.device = T.device('cpu') \n",
    "        #print(self.device)  \n",
    "        # sending the entire network to the device\n",
    "        self.to(self.device) \n",
    "    \n",
    "    # Feedforward\n",
    "    def forward(self, state): #activation\n",
    "        #print(\"forward state\",state)\n",
    "        layer = F.relu(self.drop(self.fc1(state))) # 1st hidden layer neuron activation\n",
    "        #print(\"forward layer1:\",layer1)\n",
    "        layer = F.relu(self.drop(self.fc2(layer)))   \n",
    "        layer = F.relu(self.drop(self.fc3(layer)))\n",
    "        layer = F.relu(self.drop(self.fc4(layer)))\n",
    "        layer = F.relu(self.drop(self.fc5(layer)))\n",
    "        layer = F.relu(self.drop(self.fc6(layer)))\n",
    "        #layer = F.relu(self.drop(self.fc7(layer)))\n",
    "        #layer = F.relu(self.drop(self.fc8(layer)))\n",
    "        #layer = F.relu(self.drop(self.fc9(layer)))\n",
    "        #layer = F.relu(self.drop(self.fc10(layer)))\n",
    "        actions = self.fc_op(layer) # no activation function currently  #output later resul\n",
    "        #print(\"forward actions\", actions)\n",
    "        #print(\"\")\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise():\n",
    "    def __init__(self, mu, sigma=0.15, theta=0.2, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "        #print(\"==> NOISE\")\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Agent():\n",
    "    def __init__(self, sch_n_state, sch_n_actions, alpha, beta, tau,\n",
    "                 sfc_cnt, mem_size, batch_size, gamma, sch_model,\n",
    "                 sch_state_memory, sch_action_memory, sch_reward_memory,\n",
    "                 sch_next_state_memory, sch_terminal_memory, Neurons_per_HiddenLayer, \n",
    "                 algo = None, env_name = None,\n",
    "                 chkpt_dir ='tmp/dqn'):\n",
    "        \n",
    "        #added on 02112021\n",
    "        self.sch_model = sch_model\n",
    "        print(\"DDPG agent\")\n",
    "        self.sch_n_state   = sch_n_state #Input Neurons\n",
    "        self.sch_n_actions = sch_n_actions\n",
    "        self.alpha        = alpha #alpha\n",
    "        self.beta         = beta \n",
    "        self.sfc_cnt      = sfc_cnt\n",
    "        self.gamma        = gamma  # Discounting Factor\n",
    "        self.mem_size     = mem_size # maximum memory size\n",
    "        self.batch_size   = batch_size\n",
    "        self.algo         = algo\n",
    "        self.env_name     = env_name\n",
    "        self.chkpt_dir    = chkpt_dir \n",
    "        self.mem_cntr     = 0 #vnf_cnt\n",
    "        self.iter_cnt     = 0 #vnf_cnt\n",
    "        #print(\"Memory count\",self.mem_cntr)\n",
    "        #print(\"Iteration count\",self.iter_cnt)\n",
    "        self.tau          = tau   \n",
    "        #print(\"tau: \", tau)\n",
    "                 \n",
    "        # Initializing the different types of memory\n",
    "        self.sch_state_memory      = sch_state_memory\n",
    "        self.sch_action_memory     = sch_action_memory\n",
    "        self.sch_reward_memory     = sch_reward_memory\n",
    "        self.sch_next_state_memory = sch_next_state_memory\n",
    "        self.sch_terminal_memory   = sch_terminal_memory\n",
    "\n",
    "        #Noise\n",
    "        self.noise = OUActionNoise(mu=np.zeros(self.sch_n_actions))\n",
    "        #print(\"self.noise: \", self.noise)\n",
    "        \n",
    "        #Actor-Critic \n",
    "        self.actor         = ActorNetwork(alpha     = self.alpha, \n",
    "                                          n_state   = self.sch_n_state, \n",
    "                                          fc_unit   = Neurons_per_HiddenLayer,\n",
    "                                          n_actions = self.sch_n_actions, \n",
    "                                          name      = 'actor')\n",
    "        \n",
    "        self.critic        = CriticNetwork(beta      = self.beta, \n",
    "                                           n_state   = self.sch_n_state,  \n",
    "                                           fc_unit   = Neurons_per_HiddenLayer,\n",
    "                                           n_actions = self.sch_n_actions, \n",
    "                                           name      = 'critic')\n",
    "\n",
    "        self.target_actor  = ActorNetwork(alpha     = self.alpha,\n",
    "                                          n_state   = self.sch_n_state, \n",
    "                                          fc_unit   = Neurons_per_HiddenLayer,\n",
    "                                          n_actions = self.sch_n_actions,\n",
    "                                          name      = 'target_actor')\n",
    "\n",
    "        self.target_critic = CriticNetwork(beta      = self.beta,  \n",
    "                                           n_state   = self.sch_n_state,\n",
    "                                           fc_unit   = Neurons_per_HiddenLayer,\n",
    "                                           n_actions = self.sch_n_actions,\n",
    "                                           name      = 'target_critic')\n",
    "        \n",
    "        self.update_network_parameters(tau=1)\n",
    "     \n",
    "    \n",
    "    # Storing the transition in the agent's memory\n",
    "    def store_transition(self, sch_state, sch_action, sch_reward, sch_next_state, sch_done, sfc_cnt):\n",
    "        #print(\"==> DDPG storing\")\n",
    "        self.sfc_cnt = sfc_cnt\n",
    "        #print(\"sfc_cnt: \", sfc_cnt)\n",
    "        #print(\"pr_action: \", pr_action)\n",
    "        self.mem_cntr = sfc_cnt # when to update the weights of the target network with the weights of the eval network\n",
    "        #finding for the Position of unoccupied memory, \n",
    "        #if the position value is greater than the memory size, then the overwrittien on the memories will occur\n",
    "        index = self.mem_cntr % self.mem_size  #% = position of the unoccupied memory\n",
    "        \n",
    "        self.sch_state_memory[index]  = sch_state\n",
    "        self.sch_action_memory[index] = sch_action\n",
    "        self.sch_reward_memory[index] = sch_reward\n",
    "        self.sch_next_state_memory[index] = sch_next_state\n",
    "        self.sch_terminal_memory[index] = sch_done\n",
    "        #self.mem_cntr += 1\n",
    "        \n",
    "        #print(\"==> Index\", index, sch_state, sch_action)\n",
    "        #print(\"self.sch_state_memory:\", self.sch_state_memory)\n",
    "        #print(\"self.sch_action_memory:\", self.sch_action_memory)\n",
    "        #print(\"self.sch_reward_memory:\", self.sch_reward_memory)\n",
    "        #print(\"self.sch_terminal_memory:\", self.sch_terminal_memory)\n",
    "\n",
    "    def choose_action(self, sch_observation, sch_n_state): \n",
    "        #print(\"==> DDPG action selection\")\n",
    "        #print(sch_observation, sch_n_state)\n",
    "        self.actor.eval()\n",
    "        state = T.tensor([sch_observation], dtype = T.float).to(self.actor.device)\n",
    "        #print(\"Choose_action : state: \", state)\n",
    "        mu = self.actor.forward(state).to(self.actor.device)\n",
    "        #print(\"mu: \", mu)\n",
    "        mu_prime = mu + T.tensor(self.noise(), dtype = T.float).to(self.actor.device)\n",
    "        #print(\"mu_prime: \",mu_prime)\n",
    "        self.actor.train()\n",
    "        #print(\"pr_level: \", mu_prime.cpu().detach().numpy()[0])\n",
    "        sch_level = abs(mu_prime.cpu().detach().numpy()[0,0])\n",
    "        #print(\"sch_level: \", sch_level)\n",
    "        #print(\"\")\n",
    "        return sch_level\n",
    "    \n",
    "\n",
    "    def learn(self):\n",
    "        #print(\"==> DDPG learning\")\n",
    "        \n",
    "        self.iter_cnt = self.sfc_cnt\n",
    "        \n",
    "        #print(\"self.iter_cnt: \", self.iter_cnt, \" self.mem_cntr: \", self.mem_cntr )\n",
    "        if self.mem_cntr < self.batch_size: \n",
    "            return\n",
    "\n",
    "        max_mem           = min(self.mem_cntr, self.mem_size) \n",
    "        batch             = np.random.choice(max_mem, self.batch_size)              \n",
    "        batch_index       = np.arange(self.batch_size, dtype=np.int32)\n",
    "        #print(\"batch: \", batch)\n",
    "        \n",
    "        sch_state_batch       = T.tensor(self.sch_state_memory[batch]).to(self.actor.device)\n",
    "        sch_action_batch      = T.tensor(self.sch_action_memory[batch]).to(self.actor.device)\n",
    "        sch_reward_batch      = T.tensor(self.sch_reward_memory[batch]).to(self.actor.device)    \n",
    "        sch_next_states_batch = T.tensor(self.sch_next_state_memory[batch]).to(self.actor.device)\n",
    "        sch_dones_batch       = T.tensor(self.sch_terminal_memory[batch]).to(self.actor.device)\n",
    "        \n",
    "        #print(\"==> sch_state_batch: \", sch_state_batch)\n",
    "        #print(\"==> sch_action_batch: \", sch_action_batch)\n",
    "        #print(\"==> sch_next_states_batch:\", sch_next_states_batch)\n",
    "        \n",
    "        target_actions = self.target_actor.forward(sch_next_states_batch)\n",
    "        #print(\"target_actions:\", target_actions)\n",
    "        \n",
    "        critic_value_  = self.target_critic.forward(sch_next_states_batch, target_actions)\n",
    "        critic_value   = self.critic.forward(sch_state_batch, sch_action_batch)\n",
    "        \n",
    "        critic_value_[sch_dones_batch] = 0.0\n",
    "        critic_value_ = critic_value_.view(-1)\n",
    "        \n",
    "        target = sch_reward_batch + self.gamma*critic_value_\n",
    "        target = target.view(self.batch_size, 1)\n",
    "        \n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = F.mse_loss(target, critic_value)\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "        \n",
    "        self.actor.optimizer.zero_grad()\n",
    "        actor_loss = -self.critic.forward(sch_state_batch, self.actor.forward(sch_state_batch))\n",
    "        actor_loss = T.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "        self.update_network_parameters()\n",
    "        \n",
    "    \n",
    "    def update_network_parameters(self, tau = None):\n",
    "        #print(\"inside the network update\")\n",
    "        if tau is None: \n",
    "            tau = self.tau\n",
    "        #print(\"tau\", tau)\n",
    "\n",
    "        actor_params         = self.actor.named_parameters()\n",
    "        critic_params        = self.critic.named_parameters()\n",
    "        target_actor_params  = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "        \n",
    "        critic_state_dict        = dict(critic_params)\n",
    "        actor_state_dict         = dict(actor_params)\n",
    "        target_critic_state_dict = dict(target_critic_params)\n",
    "        target_actor_state_dict  = dict(target_actor_params)\n",
    "        #print(actor_state_dict, critic_state_dict )\n",
    "        \n",
    "        \n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                                (1-tau)*target_critic_state_dict[name].clone()\n",
    "\n",
    "        for name in actor_state_dict:\n",
    "             actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                                 (1-tau)*target_actor_state_dict[name].clone()\n",
    "                \n",
    "\n",
    "        self.target_critic.load_state_dict(critic_state_dict, strict=False)\n",
    "        self.target_actor.load_state_dict(actor_state_dict, strict=False)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## u ## updated on 2nd oct 2020\n",
    "## Creating an agent and construting all th required elements for implementing it.\n",
    "class LinearDDQNAgent():\n",
    "    def __init__(self, n_state, n_actions, epsilon, lr,\n",
    "                 vnf_cnt, mem_size, batch_size, state_memory, action_memory, reward_memory,\n",
    "                 next_state_memory, terminal_memory,Neurons_per_HiddenLayer, drop_prob,\n",
    "                 gamma, eps_dec, eps_min,replace, model,\n",
    "                 algo = None, env_name = None,\n",
    "                 chkpt_dir ='tmp/dqn'):\n",
    "                \n",
    "        print(\"DDQN Linear agent\")\n",
    "        \n",
    "        #added on 02112021\n",
    "        self.model = model\n",
    "        #print(self.model)\n",
    "        \n",
    "        self.n_state  = n_state #Input Neurons\n",
    "        self.n_actions = n_actions #Output Neurons = Num of HVS ina network\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr #Learning Rate\n",
    "        self.vnf_cnt = vnf_cnt\n",
    "        self.gamma = gamma  # Discounting Factor\n",
    "        self.eps_dec = eps_dec # Epsilon rate decay\n",
    "        self.eps_min = eps_min \n",
    "        self.mem_size = mem_size # maximum memory size\n",
    "        self.batch_size = batch_size\n",
    "        self.replace_target_cnt = replace\n",
    "        self.algo =algo\n",
    "        self.env_name = env_name\n",
    "        self.chkpt_dir = chkpt_dir \n",
    "        self.mem_cntr = 0 #vnf_cnt\n",
    "        self.iter_cnt = 0 #vnf_cnt\n",
    "        #print(\"Memory count\",self.mem_cntr)\n",
    "        #print(\"Iteration count\",self.iter_cnt)\n",
    "        \n",
    "        # Primary network\n",
    "        self.Q_eval = LinearDDQN(lr, n_actions=n_actions, n_state=n_state, \n",
    "                                fc_unit = Neurons_per_HiddenLayer, drop_prob =drop_prob)\n",
    "     \n",
    "        #print(\"self Q:\",self.Q_eval)\n",
    "        \n",
    "        #Target Network (backpropagation or gradient decend is not performed on this network)\n",
    "        self.Q_target = LinearDDQN(lr, n_actions=n_actions, n_state=n_state, \n",
    "                                fc_unit = Neurons_per_HiddenLayer, drop_prob=drop_prob)\n",
    "            \n",
    "        #print(\"self Q target:\",self.Q_target)\n",
    "        \n",
    "        # Initializing the different types of memory\n",
    "        self.state_memory = state_memory\n",
    "        #print(\"state_memory\", state_memory)\n",
    "        #print(\"self.state_memory\", self.state_memory)\n",
    "        self.action_memory = action_memory\n",
    "        self.reward_memory = reward_memory\n",
    "        self.next_state_memory = next_state_memory\n",
    "        #print(\"self.next_state_memory\", self.next_state_memory)\n",
    "        self.terminal_memory = terminal_memory\n",
    "\n",
    "     \n",
    "    # Storing the transition in the agent's memory\n",
    "    def store_transition(self, state, action, reward, next_state, done, vnf_cnt):\n",
    "        self.vnf_cnt = vnf_cnt\n",
    "        self.mem_cntr = vnf_cnt # when to update the weights of the target network with the weights of the eval network\n",
    "        #finding for the Position of unoccupied memory, \n",
    "        #if the position value is greater than the memory size, then the overwrittien on the memories will occur\n",
    "        index = self.mem_cntr % self.mem_size  #% = position of the unoccupied memory\n",
    "        \n",
    "        #print(\"self.mem_cntr\",self.mem_cntr)\n",
    "        #print(\"self.mem_size\",self.mem_size)\n",
    "        #print(\"Index\", index)\n",
    "        \n",
    "        self.state_memory[index]  = state\n",
    "        #print(\"State\", state)\n",
    "        #print(\"State value inside the replay buffer for given index\", self.state_memory[index])\n",
    "        #print(\"State value inside the replay buffer\", self.state_memory)\n",
    "        self.action_memory[index] = action\n",
    "        #print(\"action value inside the replay buffer\", self.action_memory[index])\n",
    "        self.reward_memory[index] = reward\n",
    "        #print(\"reward value inside the replay buffer\", self.reward_memory[index])\n",
    "        self.next_state_memory[index] = next_state\n",
    "        #print(\"next_state value inside the replay buffer\", self.next_state_memory[index])\n",
    "        self.terminal_memory[index] = done\n",
    "        #print(\"terminal_memory value inside the replay buffer\", self.terminal_memory[index])\n",
    "        \n",
    "        #once all is done, then the position is incremented\n",
    "        self.mem_cntr += 1\n",
    "    \n",
    "    ## Greedy method for action selection\n",
    "    def choose_action(self, observation, norm_vals_sorted_action, vals_sorted_action, action_space): # for the current state = observation\n",
    "        self.action_space = action_space \n",
    "        self.norm_vals_sorted_action = norm_vals_sorted_action\n",
    "        #print(\"Norm action space\",self.norm_vals_sorted_action)\n",
    "        self.vals_sorted_action = vals_sorted_action\n",
    "        #exploration_rate_threshold = random.uniform(0,1)\n",
    "        #print(\"observation\",observation)\n",
    "        limit = np.random.random()\n",
    "        #print(\"limit\", limit)\n",
    "        if limit > self.epsilon:\n",
    "            \n",
    "            # Expliotation\n",
    "            state   = T.tensor([observation], dtype=T.float).to(self.Q_eval.device)# make sure that the state is in tensor\n",
    "            #print(\"state\", state)\n",
    "            actions = self.Q_eval.forward(state) # retriving the actions produced by the DQN\n",
    "            \n",
    "            #print(\"Expliotation: Actions\",actions)\n",
    "            action  = T.argmax(actions).item() # achieving the max action element for that particular state\n",
    "            #print(\"action: \", action)\n",
    "        else:\n",
    "            #exploration\n",
    "            #print(\"Norm action space\",self.norm_vals_sorted_action)\n",
    "            #action  = np.random.choice(self.action_space, p = self.norm_vals_sorted_action) #if the exploration rate is less than epsilon value, then random choice of action is done.\n",
    "            \n",
    "            #print(\"norm\", self.norm_vals_sorted_action)\n",
    "            action  = rand.choices(self.action_space) \n",
    "            #action  = rand.choices(self.action_space, weights = self.vals_sorted_action) #if the exploration rate is less than epsilon value, then random choice of action is done.\n",
    "            #print(self.vals_sorted_action)\n",
    "            #print(\"trail action\",action)\n",
    "            action = action[0]\n",
    "            #print(\"Exploration: Action\", action)\n",
    "            \n",
    "        #print(\"selected action:\",action)\n",
    "        return action\n",
    "    \n",
    "     \n",
    "    ## How Agent will learn from the experiences. The learning function will take current state, action, reward and next state.\n",
    "    \n",
    "    def learn(self):\n",
    "        \n",
    "        #print(\"==> Inside learn\")\n",
    "        # When to start the learning? \n",
    "        \n",
    "        # Waiting until the agent fills a batch size of memories and then returns if it has not\n",
    "        #print(\"self.mem_cntr:\",self.mem_cntr)\n",
    "        #print(\"self.batch_size:\",self.batch_size)\n",
    "        #print(\"++> self.mem_cntr\",self.mem_cntr)\n",
    "        self.iter_cnt = self.vnf_cnt\n",
    "        \n",
    "        if self.mem_cntr < self.batch_size: # checking the batch size\n",
    "            return\n",
    "        \n",
    "        # Zero the gradient on the optimizer.        \n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        #print(self.Q_eval.optimizer.zero_grad)\n",
    "        \n",
    "        # Updating the Target network after every C iterations(transistions)\n",
    "        if self.replace_target_cnt is not None and self.iter_cnt % self.replace_target_cnt == 0:\n",
    "            self.Q_target.load_state_dict(self.Q_eval.state_dict())\n",
    "            #print(\"replacing \")\n",
    "        \n",
    "        # figuring out the how much memory is filled up in the buffer.\n",
    "        # checking \n",
    "        max_mem = min(self.mem_cntr, self.mem_size) # figuring out the last pposition of the stored memory.\n",
    "        #print(\"max_mem\",max_mem)\n",
    "        \n",
    "        #uniformly sampling the trasitions from the buffer\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace =False) \n",
    "        #print(\"batch\",batch)\n",
    "        # replace is false so that there is no repeat in the transitions\n",
    "        \n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        #print(\"batch_index\",batch_index)\n",
    "        \n",
    "        state_batch  = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        #print(\"state_batch\",state_batch)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        #print(\"action_batch\",action_batch)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)    \n",
    "        #print(\"reward_batch\",reward_batch)\n",
    "        next_states_batch = T.tensor(self.next_state_memory[batch]).to(self.Q_eval.device)\n",
    "        #print(\"next_states_batch\",next_states_batch)\n",
    "        dones_batch      = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        #print(\"dones_batch\", dones_batch)\n",
    "\n",
    "        \n",
    "        #editing 24th feb 2021\n",
    "        # Calculating the Q estimate\n",
    "        # This is the value what the network estimated for that state\n",
    "        q_pred = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        #print(\"Predcit\",q_pred)\n",
    "        \n",
    "        q_next = self.Q_target.forward(next_states_batch)\n",
    "        \n",
    "        # Target Q value\n",
    "        q_eval = self.Q_eval.forward(next_states_batch)\n",
    "        #print(\"Q next\",q_eval)\n",
    "        \n",
    "        max_actions = T.argmax(q_eval, dim=1)\n",
    "        \n",
    "        q_next[dones_batch]= 0.0\n",
    "        \n",
    "        q_target = reward_batch + self.gamma*q_next[batch_index,max_actions]\n",
    "        #print(\"Target\", q_target)\n",
    "        \n",
    "        # calcualting the loss between the what actually the network should have predicted and what it has done\n",
    "        # Loss is the MSE\n",
    "        loss = self.Q_eval.loss(q_target, q_pred).to(self.Q_eval.device)\n",
    "        #print(loss)\n",
    "        #self.service_loss.append(loss)\n",
    "        #print(self.service_loss)\n",
    "        \n",
    "        \n",
    "        ## Defining the Exploration rate decay\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "                        if self.epsilon > self.eps_min else self.eps_min\n",
    "        #print(\"Epsilon Dec\",self.epsilon)\n",
    "        \n",
    "        \n",
    "        #Backpropagation\n",
    "        #print(\"Backpropagation\")\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()  # step our optimizer \n",
    "        #print(self.Q_eval.optimizer.step)\n",
    "        self.iter_cnt += 1\n",
    "        #print('==> self.iter_cnt',self.iter_cnt)\n",
    "        #print(self.Q_eval.optimizer.step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding on 02112021\n",
    "\n",
    "class LinearDQNAgent():\n",
    "    def __init__(self, n_state, n_actions, epsilon, lr,\n",
    "                 vnf_cnt, Neurons_per_HiddenLayer, drop_prob,\n",
    "                 gamma, eps_dec, eps_min, model,\n",
    "                 algo = None, env_name = None,\n",
    "                 chkpt_dir ='tmp/dqn'):\n",
    "        \n",
    "        print(\"Linear DQN agent\")\n",
    "        self.model = model\n",
    "        #print(self.model)\n",
    "        #print(lr)\n",
    "        \n",
    "        self.n_state  = n_state #Input Neurons\n",
    "        self.n_actions = n_actions #Output Neurons = Num of HVS ina network\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr #Learning Rate\n",
    "        self.vnf_cnt = vnf_cnt\n",
    "        self.gamma = gamma  # Discounting Factor\n",
    "        self.eps_dec = eps_dec # Epsilon rate decay\n",
    "        self.eps_min = eps_min \n",
    "        self.algo =algo\n",
    "        self.env_name = env_name\n",
    "        self.chkpt_dir = chkpt_dir\n",
    "        self.iter_cnt = 0 #vnf_cnt\n",
    "        \n",
    "        #Primary network\n",
    "        #print(\"before Q_eval\")\n",
    "        self.Q_eval = LinearDDQN(lr, n_actions=n_actions, n_state=n_state, fc_unit = Neurons_per_HiddenLayer,\n",
    "                                    drop_prob =drop_prob)\n",
    "        #print(\"self.Q_eval: \",self.Q_eval)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, next_state, done, vnf_cnt):\n",
    "        self.state = state\n",
    "        #print(\"self.state\", self.state)\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.done = False #done \n",
    "        self.vnf_cnt = vnf_cnt\n",
    "        \n",
    "        \n",
    "        ## Greedy method for action selection\n",
    "    def choose_action(self, observation, norm_vals_sorted_action, vals_sorted_action, action_space): # for the current state = observation\n",
    "        self.action_space = action_space \n",
    "        self.norm_vals_sorted_action = norm_vals_sorted_action\n",
    "        #print(\"Norm action space\",self.norm_vals_sorted_action)\n",
    "        self.vals_sorted_action = vals_sorted_action\n",
    "        #exploration_rate_threshold = random.uniform(0,1)\n",
    "        #print(\"observation\",observation)\n",
    "        limit = np.random.random()\n",
    "        #print(\"limit\", limit)\n",
    "        if limit > self.epsilon:\n",
    "            \n",
    "            # Expliotation\n",
    "            state   = T.tensor([observation], dtype=T.float).to(self.Q_eval.device)# make sure that the state is in tensor\n",
    "            #print(\"state\", state)\n",
    "            actions = self.Q_eval.forward(state) # retriving the actions produced by the DQN\n",
    "            \n",
    "            #print(\"Expliotation: Actions\",actions)\n",
    "            action  = T.argmax(actions).item() # achieving the max action element for that particular state\n",
    "            #print(\"action: \", action)\n",
    "        else:\n",
    "            #exploration\n",
    "            #print(\"Norm action space\",self.norm_vals_sorted_action)\n",
    "            #action  = np.random.choice(self.action_space, p = self.norm_vals_sorted_action) #if the exploration rate is less than epsilon value, then random choice of action is done.\n",
    "            #print(\"norm\", self.norm_vals_sorted_action)\n",
    "            action  = rand.choices(self.action_space) \n",
    "            #action  = rand.choices(self.action_space, weights = self.vals_sorted_action) #if the exploration rate is less than epsilon value, then random choice of action is done.\n",
    "            #print(self.vals_sorted_action)\n",
    "            #print(\"trail action\",action)\n",
    "            action = action[0]\n",
    "            #print(\"Exploration: Action\", action)\n",
    "            \n",
    "        #print(\"selected action:\",action)\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        # Zero the gradient on the optimizer.        \n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        #self.done = False\n",
    "        #print(\"DQN\")\n",
    "        #print(\"==> self.state\", self.state)\n",
    "        #Converting to tensor form\n",
    "        states = T.tensor(self.state, dtype=T.float).to(self.Q_eval.device) \n",
    "        #print(\"Learn State\",states)\n",
    "        actions = T.tensor(self.action).to(self.Q_eval.device) \n",
    "        #print(\"Learn Action\",actions)\n",
    "        rewards = T.tensor([self.reward]).to(self.Q_eval.device) \n",
    "        #print(\"Learn Reward \",rewards)\n",
    "        next_states = T.tensor(self.next_state, dtype=T.float).to(self.Q_eval.device) \n",
    "        #print(\"Learn Next state\",next_states)\n",
    "        dones =  T.tensor(self.done).to(self.Q_eval.device)\n",
    "\n",
    "        #print(\"DQN\")\n",
    "        #DQN estimation\n",
    "        q_pred = self.Q_eval.forward(states)[actions]\n",
    "        #print(\"Predcit\", q_pred)\n",
    "\n",
    "        #changing here on 26th oct 2021\n",
    "        q_next = self.Q_eval.forward(next_states).max()\n",
    "        #print(\"q_next\", q_next)\n",
    "        #q_next[dones]= 0.0\n",
    "        q_target = rewards + self.gamma*q_next\n",
    "        #print(\"==> Target\", q_target)\n",
    "        \n",
    "        \n",
    "        # calcualting the loss between the what actually the network should have predicted and what it has done\n",
    "        # Loss is the MSE\n",
    "        loss = self.Q_eval.loss(q_target, q_pred).to(self.Q_eval.device)\n",
    "        #print(loss)\n",
    "        #self.service_loss.append(loss)\n",
    "        #print(self.service_loss)\n",
    "        \n",
    "        ## Defining the Exploration rate decay\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "                        if self.epsilon > self.eps_min else self.eps_min\n",
    "        #print(\"Epsilon Dec\",self.epsilon)\n",
    "        \n",
    "        \n",
    "        #Backpropagation\n",
    "        #print(\"Backpropagation\")\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()  # step our optimizer \n",
    "        #print(self.Q_eval.optimizer.step)\n",
    "        self.iter_cnt += 1\n",
    "        #print('==> self.iter_cnt',self.iter_cnt)\n",
    "        #print(self.Q_eval.optimizer.step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding on 01032022\n",
    "class HeuAgent():\n",
    "    def __init__(self, n_state, n_actions):\n",
    "        \n",
    "        print(\"Heuristic model\")\n",
    "        #print(\"State\", n_state)\n",
    "        #print(\"Action\", n_actions)\n",
    "        \n",
    "        self.n_state = n_state\n",
    "        self.n_actions = n_actions\n",
    "    \n",
    "    def choose_action(self, observation, norm_vals_sorted_action, vals_sorted_action, action_space):\n",
    "        #print(\"++++++ Inside Right Choose Action class++++++\")\n",
    "        self.observation = observation\n",
    "        #print(\"self.observation\",self.observation)\n",
    "        \n",
    "        self.norm_vals_sorted_action = norm_vals_sorted_action\n",
    "        #print(\"self.norm_vals_sorted_action\", self.norm_vals_sorted_action)\n",
    "        \n",
    "        self.vals_sorted_action = vals_sorted_action\n",
    "        #print(\"self.vals_sorted_action\", self.vals_sorted_action)\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        #print(\"self.action_space\", self.action_space)\n",
    "        \n",
    "        action = np.random.choice(self.action_space, p = self.norm_vals_sorted_action) #if the exploration rate is less than epsilon value, then random choice of action is done.\n",
    "        #print(\"Normalized: Action\", action)\n",
    "        \n",
    "        #action = rand.choices(self.action_space, weights = self.vals_sorted_action)\n",
    "        #action = action[0]\n",
    "        #print(\"Sorted value: Action\", action)\n",
    "         \n",
    "        # without any infromation about the resource avaialability    \n",
    "        #action = random.choice(self.action_space)\n",
    "        #action = action[0]\n",
    "        return action\n",
    "    \n",
    "     \n",
    "    def store_transition(self, state, action, reward, next_state, done, vnf_cnt):\n",
    "        pass\n",
    "    \n",
    "    def learn(self):\n",
    "        self.epsilon = 1\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "Concept:\n",
    "THe transition, i.e., the for an observed state an action value is estimated by the network. According to the state-action pair the environment provide the rewards as a feedback to the agent along with the next state. These values state, action, reward and next state are stored as memory. From this memory the transitions are given to the traget network for estimating the loss function. That is analysisng the performce of the policy network (Primamry network). Thus I am contructing the classes called ReplayBuffer or ReplayMemory to store the transitions. \n",
    "\n",
    "I have constructed two types of constructor for experiment. \n",
    "\n",
    "In class ReplayBuffer, stored the values of space seperately. That is the state, actions, rewards, and nxt state are stored in a seperate array and using the index values these are formed as Transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PER: Sumtree\n",
    "The sum-tree data structure used here is very similar in spirit to the array representation of a binary heap. However, instead of the usual heap property, the value of a parent node is the sum of its children. Leaf nodes store the transition priorities and the internal nodes are intermediate sums, with the parent node containing the sum over all priorities, p_total. This provides a efficient way of calculating the cumulative sum of priorities, allowing O(log N) updates and sampling.\n",
    "\n",
    "Additional useful links:\n",
    "Good tutorial about SumTree data structure:  https://adventuresinmachinelearning.com/sumtree-introduction-python/\n",
    "How to represent full binary tree as array: https://stackoverflow.com/questions/8256222/binary-tree-represented-using-array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dummy catalog, stating this info has been recoved or is available to the VNF descriptor. Thus the descriptor will know the extact name or in this case ID of the micro vnfs. \n",
    "\n",
    "Since the allocation of the ids are performed randomly. For example: for SFC:2, the vnf0 is decommposed into 6 microvnfs with various resouces demand such as \n",
    "microvnf    Resource      ID\n",
    "0             1           23 <==\n",
    "1             2           17\n",
    "2             3           15\n",
    "3             1           20\n",
    "4             1            1\n",
    "5             1           41\n",
    "\n",
    "microvnf    Resource      ID\n",
    "0             1           27\n",
    "1             2           48\n",
    "2             2           23 <==\n",
    "3             2           35\n",
    "4             2           11\n",
    "\n",
    "same id cant have different resouce requirment, thus we have categorized the resouces according to the microvnf ID; Declared the dependency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class microVNFFG():\n",
    "    def __init__(self,cpu_res, nodal_avail):\n",
    "        \n",
    "        self.cpu_res = cpu_res\n",
    "        self.nodal_avail = nodal_avail\n",
    "        \n",
    "    ## Resource initialization for the micro vnfs\n",
    "    def micro_resource_init(cpu_res, nodal_avail, granular_index):\n",
    "        #print(\"granular_index\", granular_index)\n",
    "        #print(\"cpu_res\",cpu_res,\"nodal_avail\",nodal_avail)\n",
    "        granular_idx = granular_index \n",
    "        i = cpu_res\n",
    "        exclude = [i]\n",
    "        while i > 0:\n",
    "            #print(\"while i \", i)\n",
    "            if i <= 2:\n",
    "                granular_index = i\n",
    "                #print(\"i <= 2\")\n",
    "                #print(\"granular_index\", granular_index)\n",
    "            else:\n",
    "                granular_index = round(cpu_res*nodal_avail)\n",
    "                #print(\"else\")\n",
    "                #print(\"==.>granular_index\", granular_index)\n",
    "                if granular_index == 0:\n",
    "                    if granular_idx == 1:\n",
    "                        granular_index = granular_idx\n",
    "                        #print(\"==granular_index\", granular_index)\n",
    "                        \n",
    "                    else:\n",
    "                        #print(\"empty microvnf init: break\")\n",
    "                        break\n",
    "            #print(\"granular_index\", granular_index)\n",
    "            n = rand.randint(1,granular_index)\n",
    "            #print(n)\n",
    "            \n",
    "            if n == 1:\n",
    "                n = n\n",
    "                #print(\"n\")\n",
    "            else:\n",
    "                while n in exclude:\n",
    "                    #print(\"stuck here\")\n",
    "                    n = rand.randint(1, i)           \n",
    "            yield n\n",
    "            i -= n\n",
    "        \n",
    "    \n",
    "    # Generating the IDs for the decomposed microvnfs, according to the requested resource with the help of the catalog. \n",
    "\n",
    "    def microvnfid_init(micro_vnf, micro_vnf_req, unique_id, catalog):\n",
    "        # constraint 1: setting the upper and lower limit \n",
    "        #print(\"catalog: \",catalog, \"micro_vnf: \", micro_vnf, \"micro_vnf_req: \", micro_vnf_req)\n",
    "        max_id = catalog[micro_vnf_req] # setting the higher bound for the id generation\n",
    "        #print(\"max_id: \", max_id)\n",
    "        \n",
    "        if micro_vnf_req == 1:\n",
    "            min_id = 0 #setting the lower bound for the id generation\n",
    "        else:\n",
    "            min_id = catalog[micro_vnf_req-1]+1\n",
    "        #print(\"min_id\", min_id)\n",
    "        id_gen = rand.randint(min_id,max_id)\n",
    "        #print(\"id_gen: \",id_gen)\n",
    "\n",
    "        # constaint 2: to not repeat any micro vnf id.  unique id represents the ids of a particular decomposed microvnf \n",
    "        while id_gen in unique_id:\n",
    "            id_gen = rand.randint(min_id,max_id)\n",
    "            #print(\"in while break here\")\n",
    "        return id_gen   \n",
    "    \n",
    "    def microvnf_descriptor(max_vnf_resource):\n",
    "        catalog = {}\n",
    "        count = max_vnf_resource\n",
    "        for resource in range(1, max_vnf_resource):\n",
    "            catalog[resource] = count\n",
    "            count += 5\n",
    "        return catalog\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(microVNFFG.micro_resource_init(cpu_res = 5, nodal_avail = 0.8, granular_index =10)))\n",
    "#cpu_res, nodal_avail = 20, 0.8\n",
    "#granular_index = 4\n",
    "#max_vnf_resource  = 10\n",
    "#unique_id =[]\n",
    "#qq = microVNFFG(cpu_res,nodal_avail)\n",
    "#catalog = microVNFFG.microvnf_descriptor(max_vnf_resource)\n",
    "#print(\"catalog\",catalog)\n",
    "#microVNFFG.microvnfid_init(micro_vnf = 1, micro_vnf_req = 2, unique_id = unique_id, catalog = catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class microvnf_construction():\n",
    "    def __init__(self,max_vnf_resource, episode, sfc, vnf, cpu_req, n, val, pr_level, sfc_rel, traffic_pred):\n",
    "        \n",
    "        self.max_vnf_resource = max_vnf_resource\n",
    "        self.episode = episode\n",
    "        self.sfc = sfc\n",
    "        self.vnf = vnf\n",
    "        self.cpu_req = cpu_req\n",
    "        self.n = n\n",
    "        self.val = val\n",
    "        self.pr_level = pr_level\n",
    "        self.sfc_rel = sfc_rel\n",
    "        self.traffic_pred = traffic_pred \n",
    "        \n",
    "        \n",
    "    # microvnf construction\n",
    "    def microvnf(self, microvnf_per_vnf, current_cpu_req, micro_vnf_res, decompose_candidate):\n",
    "        #print(\"micro vnf const site\", self.val)\n",
    "        seed_array = [1] #[1,3,4]\n",
    "        rand_seed = rand.choice(seed_array)\n",
    "        #print(rand_seed)\n",
    "        eps_value = 0.1\n",
    "        #print(\"microvnf_per_vnf inside the def\",microvnf_per_vnf)\n",
    "        #print(\"current_cpu_req\",current_cpu_req)\n",
    "        \n",
    "        p = (1 + eps_value)*(math.log(microvnf_per_vnf)/microvnf_per_vnf)\n",
    "\n",
    "        er_graph_seed_microvnf = nx.erdos_renyi_graph(microvnf_per_vnf,p,seed=rand_seed, \n",
    "                                                      directed =True) \n",
    "        #generating the graph\n",
    "        #nx.draw(er_graph_seed_microvnf, with_labels=True) \n",
    "        #plt.show()\n",
    "        \n",
    "        #print(\"==>\",microvnf_per_vnf,current_cpu_req, micro_vnf_res, decompose_candidate)\n",
    "\n",
    "        unique_id = []\n",
    "        for micro_vnf in er_graph_seed_microvnf.nodes():\n",
    "            #print(\"Micro VNF: \", micro_vnf)\n",
    "            id_gen = microVNFFG.microvnfid_init(micro_vnf,\n",
    "                                                micro_vnf_res[micro_vnf], \n",
    "                                                unique_id, \n",
    "                                                microVNFFG.microvnf_descriptor(self.max_vnf_resource))\n",
    "            unique_id.append(id_gen)\n",
    "\n",
    "            if micro_vnf == 0:\n",
    "                microVNF_position = 'head'\n",
    "            elif micro_vnf == (microvnf_per_vnf - 1):\n",
    "                microVNF_position = 'tail'\n",
    "            else:\n",
    "                microVNF_position = 'middle'\n",
    "            \n",
    "\n",
    "            er_graph_seed_microvnf.add_node(micro_vnf,\n",
    "                                               episode = self.episode,\n",
    "                                               sfc = self.sfc,\n",
    "                                               vnf = self.vnf,\n",
    "                                               vnf_req = current_cpu_req,\n",
    "                                               pr_level = self.pr_level,\n",
    "                                               traffic_pred = self.traffic_pred, \n",
    "                                               #sfc_rel = self.sfc_rel,\n",
    "                                               numMicroVNF = microvnf_per_vnf,\n",
    "                                               microVNF = micro_vnf,\n",
    "                                               microVNF_ID = id_gen,\n",
    "                                               micro_cpu_req = micro_vnf_res[micro_vnf],\n",
    "                                               microVNF_pos = microVNF_position,\n",
    "                                               decompose_candidate = decompose_candidate\n",
    "                                              )\n",
    "        #print(\"Unique Id:\", unique_id)\n",
    "        self.microvnf_node_resources= list(er_graph_seed_microvnf.nodes(data=True)) \n",
    "        #print(self.microvnf_node_resources)\n",
    "\n",
    "\n",
    "        # Micro virtual link\n",
    "        microlink_count = 0\n",
    "        for micro_link in er_graph_seed_microvnf.edges():\n",
    "            #INITIALIZATION RESOURCE\n",
    "            # Allocating the bandwidth resouce.\n",
    "            #print(micro_link)\n",
    "            er_graph_seed_microvnf.add_edge(micro_link[0],micro_link[1],\n",
    "                                            src_vnf  = micro_link[0],\n",
    "                                            dest_vnf = micro_link[1],\n",
    "                                            episode = self.episode,\n",
    "                                            sfc = self.sfc,\n",
    "                                            vnf = self.vnf,\n",
    "                                            numVNF = self.n,\n",
    "                                            microlink_num = microlink_count,\n",
    "                                            bw  = rand.randrange(1, 2),#Mbps\n",
    "                                            latency = rand.randrange(1,2) #ms\n",
    "                                            )\n",
    "            microlink_count += 1\n",
    "            #print(\"Total Micro Link\", microlink_count)\n",
    "        # Combining the nodes and edges per VNF-FG\n",
    "        self.microvnf_link_resources = list(er_graph_seed_microvnf.edges(data=True))\n",
    "        #print(self.microvnf_link_resources)\n",
    "\n",
    "        return self.microvnf_node_resources, self.microvnf_link_resources, unique_id\n",
    "    \n",
    "    # micro vnf\n",
    "    def microvnf_agent(self, episode, sfc, vnf, decomp, n_actions, lr, epsilon, mem_size, batch_size, vnf_cnt, gamma, \n",
    "                       eps_dec, eps_min, traffic_pred,traffic_load,\n",
    "                       replace, state_memory, action_memory,reward_memory, next_state_memory, terminal_memory, \n",
    "                       Neurons_per_HiddenLayer, drop_prob, chkpt_dir, algo, env_name, count_VNF, done, \n",
    "                       amount_local_reward, reward_per_current_vnfstates, microvnf_accpeted_action, accpeted_action,\n",
    "                       updated_path_perVNF, reward_per_current_states, nodal_avail, sum_resouces, state_space_size,\n",
    "                       threshold_delay, rewardlink_per_current_states, vnf_link_resources, temp_accepted_action, \n",
    "                       t_microvnf_accpeted_action, prob_val, temp_val, original_actionspace,  prev_maction, \n",
    "                       min_pr,max_pr, mvf_t_max, mvf_t_min, mvf_old_t_current, new_reward_per_current_states):\n",
    "        \n",
    "        #print(\"inside mvnf agent: accpeted_action\", accpeted_action)\n",
    "        #print(\"reward_per_current_states: \", reward_per_current_states)\n",
    "        #print(\"inside mvnf agent: micro accpeted_action\", microvnf_accpeted_action)\n",
    "        count_microvnf, count_updated_path_pervnf = 0, 0\n",
    "        my_dict = {}\n",
    "        micro_sorted_action = {} \n",
    "        micro_keys_sorted_action = []\n",
    "        reward_mvnfs = []\n",
    "        dict_node_avail = {}\n",
    "\n",
    "        for node, available_res in env.network_nodes:\n",
    "            if available_res['emb_vnf'] !=0:\n",
    "                #print(\"inside the env.network\")\n",
    "                #print(\">> Avaliable node\",node)\n",
    "                my_dict[node] = available_res['emb_vnf']\n",
    "                micro_sorted_action = dict(sorted(my_dict.items(), key=lambda item: item[1], reverse = True))\n",
    "        \n",
    "        if micro_sorted_action == None or micro_sorted_action == {}:\n",
    "            #print(\"xxxxxx empty sorted action xxxxxx\")\n",
    "            micro_keys_sorted_action, micro_action_space, micro_up_actionspace = [], [], []\n",
    "            micro_vals_sorted_action, micro_norm_vals_sorted_action =[], []\n",
    "            micro_sorted_action = None\n",
    "            \n",
    "        else: \n",
    "            micro_keys_sorted_action = micro_sorted_action.keys()\n",
    "            micro_action_space = list(map(int,micro_keys_sorted_action))\n",
    "            #print(\"micro_action_space\",micro_action_space)   \n",
    "            micro_up_actionspace = micro_action_space \n",
    "            micro_vals_sorted_action = micro_sorted_action.values()\n",
    "            micro_norm_vals_sorted_action = [float(i)/sum(micro_vals_sorted_action) for i in micro_vals_sorted_action]\n",
    "\n",
    "        for micro_vnf, microvnf_res in decomp[0]: \n",
    "            #print(\"==> micro_vnf\", micro_vnf)\n",
    "            #print(\"==> new_reward_per_current_states\", new_reward_per_current_states)\n",
    "            #print(\"reward_mvnfs\", reward_mvnfs)\n",
    "            mvnf_start = time.perf_counter_ns()\n",
    "            \n",
    "            if micro_sorted_action is None:\n",
    "                #print(\"xxxxxx break xxxxxx\")\n",
    "                break\n",
    "            else:\n",
    "                #print(\"===== micro_vnf_res\", microvnf_res, microvnf_res['microVNF_ID'])\n",
    "                microstate_space_size = microvnf_res['numMicroVNF']\n",
    "                micro_vnf_res         = microvnf_res['micro_cpu_req']\n",
    "                #traffic_pred          = microvnf_res['classifier_class']\n",
    "                          \n",
    "                #print(\"micro_vnf\", micro_vnf, \" micro_vnf_res\",micro_vnf_res)\n",
    "                #print(\"pr_level: \", pr_level)\n",
    "                micro_observation     = [episode, sfc, vnf, vnf_res_req, pr_level, micro_vnf, micro_vnf_res, traffic_pred]\n",
    "                state_dim             = [len(micro_observation)]\n",
    "                #print(\"micro state_dim: \", state_dim)\n",
    "                #print(\"micro_vnf\", micro_vnf)\n",
    "                status_existing_action = False\n",
    "                mVNF_ID = microvnf_res['microVNF_ID']\n",
    "                micro_reward_action_not = 0\n",
    "                \n",
    "                if temp_accepted_action:\n",
    "                    #print(\"=====> before:temp_accepted_action\", temp_accepted_action)\n",
    "                    existing_action = None\n",
    "                    for key, val in temp_accepted_action.items():\n",
    "                        #print(\"key\", key, \" val\", val)\n",
    "                        for mvnfid in val:\n",
    "                            #print(\"mvnfid\", mvnfid)\n",
    "                            #print(\"mVNF_ID\", mVNF_ID)\n",
    "                            if mVNF_ID is mvnfid:\n",
    "                                #print(\"micro_vnf\", micro_vnf)\n",
    "                                #print(\"=====> temp_accepted_action\", temp_accepted_action)\n",
    "                                status_existing_action = True\n",
    "                                existing_action = key\n",
    "                                #print(\"mVNFID:\", mvnfid, \"new action: \", existing_action)\n",
    "                            else: \n",
    "                                pass\n",
    "                \n",
    "                action_space = micro_up_actionspace\n",
    "                vals_sorted_action = list(micro_vals_sorted_action)\n",
    "                #print(\"\")\n",
    "                #print(\"before action agent is active: Mvnf action: \",prev_maction)\n",
    "                #print(\"original_actionspace\", original_actionspace)\n",
    "                #print(\"Inside the mvnf: prob_val\", prob_val)\n",
    "                #print(\"Inside the mvnf: temp_val\", temp_val)\n",
    "                \n",
    "                #print(\"micro_norm_vals_sorted_action\", micro_norm_vals_sorted_action)\n",
    "                #print(\"vals_sorted_action\", vals_sorted_action)\n",
    "                if status_existing_action is False:\n",
    "                    action = agent.choose_action(micro_observation, \n",
    "                                                 prob_val,\n",
    "                                                 temp_val, \n",
    "                                                 original_actionspace)\n",
    "                \n",
    "                    #print(\"Mvnf action: \",action)\n",
    "                    prev_maction = action\n",
    "\n",
    "                   # print(\"micro_up_actionspace\",micro_up_actionspace)\n",
    "                    if not action in micro_up_actionspace:\n",
    "                        #print(\"NOT in list\")\n",
    "                        micro_reward_action_not = -1000\n",
    "                    else:\n",
    "                        micro_reward_action_not = 0\n",
    "\n",
    "                    #print(\"micro_sorted_action: \",micro_sorted_action)\n",
    "                    if str(action) in micro_sorted_action.keys():\n",
    "                        current_res = micro_sorted_action[str(action)]\n",
    "                        #print(\"current_res\", current_res)\n",
    "                        updated_res = current_res - micro_vnf_res\n",
    "                        #print(\"updated_res\", updated_res)\n",
    "\n",
    "                        if updated_res > 0:\n",
    "                            updated_res = updated_res\n",
    "                        else:\n",
    "                            updated_res = 0\n",
    "\n",
    "                        micro_sorted_action.update({str(action): updated_res})\n",
    "                        #print(\"updated micro_sorted_action\", micro_sorted_action)\n",
    "                    else:\n",
    "                        current_res = 0\n",
    "                        #print(\"current_res\", current_res)\n",
    "                        #print(\"updated micro_sorted_action\", micro_sorted_action)\n",
    "                        pass\n",
    "\n",
    "                else: \n",
    "                    action = existing_action\n",
    "                \n",
    "                mvnf_end = time.perf_counter_ns()\n",
    "                #print(\"mvnf_end: \", mvnf_end)\n",
    "                mvnf_exutime = mvnf_end - mvnf_start\n",
    "                #print(\"mvnf_runtime\", mvnf_exutime)\n",
    "                \n",
    "                #print(\"Mvnf action: \",action)\n",
    "                max_val = max(micro_sorted_action.values())\n",
    "                vnf_cnt += 1\n",
    "\n",
    "                #print(\"Curent micro action: \", \"Micro vnf: \", micro_vnf, \"Micro req res: \",micro_vnf_res,  \"Action: \",action, )\n",
    "                #print(\"micro_sorted_action\", micro_sorted_action,\"str(action)\", str(action))\n",
    "\n",
    "                state  = micro_vnf\n",
    "                #constraints\n",
    "                micro_subjto = microvnf_constraints(action, micro_vnf_res, amount_local_reward, microvnf_accpeted_action)\n",
    "\n",
    "                if status_existing_action is False:\n",
    "                    \n",
    "                    #Constraint 1: checking for the available resouce for VNF: Resource checking\n",
    "                    local_rewardmicroVNF = micro_subjto.constraint_1()\n",
    "                    #print(\"local_rewardmicroVNF\", local_rewardmicroVNF)\n",
    "                    #print(\"reward_per_current_vnfstates\", reward_per_current_vnfstates)\n",
    "                    \n",
    "                else: \n",
    "                    t_action = [None, None]\n",
    "                    local_rewardmicroVNF = amount_local_reward\n",
    "                    #print(\"====> action\", action)\n",
    "                    t_action[0] = 'existing'\n",
    "                    t_action[1] = action\n",
    "                    #print(\"================== t_action: \", t_action)\n",
    "                    microvnf_accpeted_action.append(t_action)\n",
    "                \n",
    "                reward_per_current_vnfstates.append(local_rewardmicroVNF)  \n",
    "                #print(\"after: microvnf_accpeted_action\", microvnf_accpeted_action)\n",
    "                #print(\"==== _action\", action, \"--\", \"local_rewardmicroVNF\", local_rewardmicroVNF)\n",
    "                \n",
    "                \n",
    "                #Modifying the Reward function added on 13th April 2022\n",
    "                #print(\"reward_mvnfs\", reward_mvnfs)\n",
    "                #Part 1: Reward based on the quality of Nodes selected\n",
    "                for node, available_res in env.network_nodes:   \n",
    "                    dict_node_avail[node] = (available_res['emb_vnf'] / tot_vnf_per_core)    \n",
    "                reward_Node_based = local_rewardmicroVNF*dict_node_avail[str(action)]\n",
    "                #print(\"reward_Node_based\", reward_Node_based)\n",
    "                reward_mvnfs.append(reward_Node_based)\n",
    "                \n",
    "                #Part 2: Reward based on the Priority service\n",
    "                reward_pr    = local_rewardmicroVNF*pr_level\n",
    "                reward_mvnfs.append(reward_pr)\n",
    "                \n",
    "                # Part 3: Reward based on the sfc_rel\n",
    "                reward_rel = local_rewardmicroVNF*sfc_rel\n",
    "                reward_vnfs.append(reward_rel)\n",
    "                \n",
    "                # Part 4: Reward based on the traffic_pred\n",
    "                if traffic_load == None:\n",
    "                    traffic_load = 0\n",
    "                else:\n",
    "                    traffic_load = traffic_load\n",
    "\n",
    "                if local_rewardmicroVNF == 1000:\n",
    "                    reward_traffic_pred = local_rewardmicroVNF*(traffic_pred*traffic_load)\n",
    "                    #print(\"reward_traffic_pred:\", reward_traffic_pred, traffic_pred, local_rewardVNF)\n",
    "                else: \n",
    "                    if traffic_pred == -1:\n",
    "                        reward_traffic_pred = -(local_rewardmicroVNF*(traffic_pred*traffic_load))\n",
    "                    else:\n",
    "                        reward_traffic_pred = (local_rewardmicroVNF*(traffic_pred*traffic_load))\n",
    "                #print(\"reward_traffic_pred:\", reward_traffic_pred, traffic_pred, local_rewardVNF)\n",
    "                reward_vnfs.append(reward_traffic_pred)\n",
    "                \n",
    "                #Part 4: Reward based on scheduling delay\n",
    "                mvf_t_current = mvnf_exutime #rand.randint(1, 50)  #vnf_runtime #\n",
    "                #print(\"vnf current time\", t_current)\n",
    "               \n",
    "                \n",
    "                #updating the t max\n",
    "                if mvf_t_max is None or mvf_t_current >= mvf_t_max: mvf_t_max = mvf_t_current\n",
    "                \n",
    "                #updating the t min\n",
    "                if (mvf_t_current < mvf_t_max and mvf_t_min == 0) or (mvf_t_current <= mvf_t_min): mvf_t_min = mvf_t_current\n",
    "                elif (mvf_t_current >= mvf_old_t_current and mvf_t_min == 0):mvf_t_min = mvf_old_t_current\n",
    "                norm_time = 1 - Normalization(mvf_t_current, mvf_t_max, mvf_t_min)\n",
    "                #print(\"t_current: \", mvf_t_current,\"-- t_max: \", mvf_t_max, \"-- t_min: \",mvf_t_min, \"-- norm_time: \", norm_time)\n",
    "                time_reward = local_rewardmicroVNF*norm_time\n",
    "                reward_mvnfs.append(time_reward)\n",
    "                mvf_old_t_current = mvf_t_current\n",
    "                #print(\"reward_mvnfs\", reward_mvnfs)\n",
    "                \n",
    "                # Total Rewards:\n",
    "                total_reward_mvnf = round(sum(reward_mvnfs), 4)\n",
    "                #print(\"total_reward_mvnf: \", total_reward_mvnf)\n",
    "                #new_reward_per_current_states.append(total_reward_vnf)\n",
    "                #print(\"new_reward_per_current_states: \", new_reward_per_current_states)\n",
    "                #print(\"vnf\", vnf, \"-->\", new_reward_per_current_states[vnf], \"reward: \", total_reward_mvnf)\n",
    "                new_reward_per_current_states[vnf] = total_reward_mvnf\n",
    "                #print(\"vnf\", vnf, \"-->\", new_reward_per_current_states[vnf], \"reward: \", total_reward_mvnf)\n",
    "\n",
    "                # 07032022\n",
    "                if local_rewardmicroVNF == amount_local_reward:\n",
    "                    pref_node_val = 10 #amount_local_reward #amount_local_reward #1\n",
    "                else:\n",
    "                    pref_node_val = 0\n",
    "                #print(\"ACTION: \", action)\n",
    "                #print(\"prev_maction\", prev_maction)\n",
    "                \n",
    "                temp_val[action] = temp_val[action] + pref_node_val\n",
    "                prob_val = [float(i)/sum(temp_val) for i in temp_val]\n",
    "                #print(\"===> Inside the mvnf: prob_val\", prob_val)\n",
    "                #print(\"===> Inside the mvnf: temp_val\", temp_val)\n",
    "                #print(\"---\")\n",
    "\n",
    "                #Constraint 2 and 3: : checking for the available resouce for Links and estab. the path\n",
    "                micro_subjto.constraint_2(microstate_space_size, decomp[1])\n",
    "                rewardlink_per_current_vnfstates = micro_subjto.local_reward_delay\n",
    "                microlink_space_size = len(rewardlink_per_current_vnfstates)\n",
    "                #print(\"rewardlink_per_current_vnfstates\", rewardlink_per_current_vnfstates)\n",
    "                #print(\"updated_path_perVNF\", updated_path_perVNF)\n",
    "\n",
    "                # Reward Function\n",
    "                global_reward =  0              \n",
    "                ## Global reward for VNF placement\n",
    "                #print(\"microstate_space_size\", microstate_space_size)\n",
    "                #print(\"reward_per_current_vnfstates\", reward_per_current_vnfstates)\n",
    "                #print(\"microlink_space_size\", microlink_space_size)\n",
    "                #print(\"rewardlink_per_current_vnfstates\", rewardlink_per_current_vnfstates)\n",
    "\n",
    "                if (sum(reward_per_current_vnfstates) == amount_local_reward*microstate_space_size) and \\\n",
    "                    (sum(rewardlink_per_current_vnfstates) == amount_local_reward*microlink_space_size):\n",
    "                    \n",
    "                    #print(\"==> REWARD <==\")\n",
    "                    #done = True\n",
    "                    reward_all = 1000*10\n",
    "                    rewars_delay = sum(rewardDelay_per_current_states)\n",
    "                    global_reward = reward_all + rewars_delay\n",
    "                    accpeted_action.append(microvnf_accpeted_action)\n",
    "                    score = 1\n",
    "                    #print(\"inside mvnf REWARD: accpeted_action\", accpeted_action)\n",
    "                    #embedding edit this one: change the flow instead of embedding checking should be done here and embedding will be \n",
    "                    #performed during the main vnf coding side\n",
    "\n",
    "                    microvnf_node_resources = decomp[0]\n",
    "                    #env.embedding_microvnf(microvnf_accpeted_action,\n",
    "                    #                       microvnf_node_resources, \n",
    "                    #                       count_microvnf)\n",
    "                    #count_microvnf = env.count_microvnf\n",
    "                    #print(\"count_microvnf\", count_microvnf)\n",
    "                    count_microvnf = microstate_space_size\n",
    "                    #print(\"count_microvnf\",count_microvnf)\n",
    "                    \n",
    "                    \n",
    "                    microvnf_link_resources = decomp[1]\n",
    "                    #env.embedding_microvl(updated_path_perVNF, \n",
    "                    #                      microvnf_link_resources, \n",
    "                    #                      count_updated_path_pervnf)\n",
    "                    #count_updated_path_pervnf = env.count_updated_path_pervnf\n",
    "                    #print(\"count_updated_path_pervnf\", count_updated_path_pervnf)\n",
    "                    count_updated_path_pervnf = microlink_space_size\n",
    "                    #print(\"count_updated_path_pervnf\",count_updated_path_pervnf)\n",
    "\n",
    "                micro_local_reward = local_rewardmicroVNF #immediate rewards\n",
    "                micro_rewards = total_reward_mvnf + global_reward + micro_reward_action_not \n",
    "                #print(\"MICRO REWARDS\", micro_rewards)\n",
    "\n",
    "                #Next state\n",
    "                next_microstate = micro_vnf+1\n",
    "\n",
    "                if next_microstate >= microstate_space_size:\n",
    "                    next_microstate = 0\n",
    "                    next_observation = [episode, sfc, vnf+1, vnf_res_req, pr_level, \\\n",
    "                                        next_microstate, micro_vnf_res, traffic_pred]\n",
    "                else: \n",
    "                    next_microstate = next_microstate\n",
    "                    next_observation = [episode, sfc, vnf, vnf_res_req, pr_level, \\\n",
    "                                        next_microstate, micro_vnf_res, traffic_pred]\n",
    "\n",
    "                state = micro_observation\n",
    "                next_state = next_observation\n",
    "\n",
    "                # storing the transistions\n",
    "                #print(\"state\", state)\n",
    "                agent.store_transition(state, action, micro_rewards, next_state, done, vnf_cnt)\n",
    "\n",
    "                #learning:  providing state, action, reward and next state to the NN for learning.\n",
    "                agent.learn()\n",
    "                epsilon = agent.epsilon\n",
    "\n",
    "                #print(\"count vnf\",count_VNF)\n",
    "\n",
    "                if microstate_space_size == count_microvnf and microlink_space_size == count_updated_path_pervnf:\n",
    "                    #print(\"======>>Micro vnf embedded\")\n",
    "                    #print(\"count vnf\",count_VNF)\n",
    "                    #print(\"reward_per_current_states 1st\", reward_per_current_states)\n",
    "                    done_microvnf = True \n",
    "                    reward_microvnf = 1000\n",
    "                    local_rewardVNF = 1000\n",
    "                    #count_VNF =len(reward_per_current_states)\n",
    "                    reward_per_current_states.append(reward_microvnf)   \n",
    "                    #print(\"reward_per_current_states\", reward_per_current_states)\n",
    "                    count_VNF += 1\n",
    "                    #print(\"count_VNF\", count_VNF)\n",
    "\n",
    "                    #if the last vnf is decomposed then:\n",
    "                    check_pt = microvnf_node_resources[0][1]['decompose_candidate']\n",
    "                    #print(\"check point\", check_pt)\n",
    "                    #print(\"count_VNF\", count_VNF)\n",
    "                    if state_space_size == count_VNF and  check_pt !='Penalty Decompose':\n",
    "                        #print(\"======>>>> Last vnf\")\n",
    "                        done = True\n",
    "                        #print(\"decomp\", microvnf_node_resources[0][1]['decompose_candidate'])\n",
    "\n",
    "                        subjected_to = vnf_constraints(amount_local_reward, vnf_res_req,\n",
    "                                                       nodal_avail, max_vnf_resource, \n",
    "                                                       episode, sfc, vnf, vnf_per_service,\n",
    "                                                       sum_resouces, accpeted_action)\n",
    "\n",
    "                        vnf_link_resources = vnf_link_resources\n",
    "                        env_networkgraph   = env.network_graph\n",
    "                        env_networkedges   = env.network_edges\n",
    "                        subjected_to.constraint_3(state_space_size, vnf_link_resources,\n",
    "                                                 env_networkgraph, env_networkedges,\n",
    "                                                 threshold_delay)\n",
    "                        rewardlink_per_current_states = subjected_to.local_reward_link\n",
    "\n",
    "        #print(\" inside mvnf: accpeted_action\", accpeted_action)\n",
    "        return count_VNF,reward_per_current_states, rewardlink_per_current_states, accpeted_action, updated_path_perVNF, vnf_cnt, microvnf_accpeted_action, mvf_t_max, mvf_t_min, mvf_old_t_current, new_reward_per_current_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decomp  = [[(0, {'episode': 0, 'sfc': 0, 'vnf': 1, 'vnf_req': 8, 'numMicroVNF': 3, 'microVNF': 0, 'microVNF_ID': 56, 'micro_cpu_req': 6, 'microVNF_pos': 'head', 'decompose_candidate': 'VNF Decompose'}), \n",
    "#(1, {'episode': 0, 'sfc': 0, 'vnf': 1, 'vnf_req': 8, 'numMicroVNF': 3, 'microVNF': 1, 'microVNF_ID': 4, 'micro_cpu_req': 1, 'microVNF_pos': 'middle', 'decompose_candidate': 'VNF Decompose'}), (2, {'episode': 0, 'sfc': 0, 'vnf': 1, 'vnf_req': 8, 'numMicroVNF': 3, 'microVNF': 2, 'microVNF_ID': 5, 'micro_cpu_req': 1, 'microVNF_pos': 'tail', 'decompose_candidate': 'VNF Decompose'})], \n",
    "#[(0, 1, {'src_vnf': 0, 'dest_vnf': 1, 'episode': 0, 'sfc': 0, 'numVNF': 3, 'microlink_num': 0, 'bw': 6, 'latency': 1}), \n",
    "#(1, 2, {'src_vnf': 1, 'dest_vnf': 2, 'episode': 0, 'sfc': 0, 'numVNF': 3, 'microlink_num': 1, 'bw': 7, 'latency': 1})]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#microvnf_const = microvnf_construction(max_vnf_resource = 10, episode = 0, sfc = 0, vnf = 1, cpu_req = 8, n = 3, val=0)\n",
    "#microvnf_const.microvnf_agent(episode =0, sfc=0, vnf=1, decomp =decomp, num_micro_vnf = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decomp_identifier():\n",
    "    \n",
    "    def __init__(self,cpu_req, nodal_avail, max_vnf_resource, episode, sfc, vnf, vnf_per_service):\n",
    "        \n",
    "        \n",
    "        self.cpu_req = cpu_req\n",
    "        self.nodal_avail = nodal_avail\n",
    "        #self.micro_vnf_res =list(microVNFFG.micro_resource_init(self.cpu_req, self.nodal_avail)) \n",
    "        #self.num_micro_vnf = len(self.micro_vnf_res) #cal the nun of micro vnfs\n",
    "        self.max_vnf_resource = max_vnf_resource\n",
    "        self.episode = episode\n",
    "        self.sfc = sfc\n",
    "        self.vnf = vnf\n",
    "        self.vnf_per_service = vnf_per_service\n",
    "\n",
    "        \n",
    "    def microvnf_res_init(self, cpu_req, nodal_avail, granular_index):\n",
    "        self.microvnf_const = microvnf_construction(max_vnf_resource = self.max_vnf_resource, \n",
    "                                                    episode = self.episode, \n",
    "                                                    sfc = self.sfc, \n",
    "                                                    vnf = self.vnf, \n",
    "                                                    cpu_req = self.cpu_req, \n",
    "                                                    n = self.vnf_per_service,\n",
    "                                                    val = 100, \n",
    "                                                    pr_level = pr_level,\n",
    "                                                    sfc_rel = sfc_rel, \n",
    "                                                    traffic_pred = traffic_pred)\n",
    "        #print(\"self.microvnf_const: \",self.microvnf_const)\n",
    "        #print(\"resource identifier: microvnf_res_init\")\n",
    "        self.micro_vnf_res =list(microVNFFG.micro_resource_init(self.cpu_req, self.nodal_avail, self.granular_index)) \n",
    "        #print(\"self.micro_vnf_res\", self.micro_vnf_res)\n",
    "        self.num_micro_vnf = len(self.micro_vnf_res) #cal the nun of micro vnfs\n",
    "        #print(\"self.num_micro_vnf\", self.num_micro_vnf)\n",
    "        \n",
    "        if self.num_micro_vnf == 0 or self.num_micro_vnf == []:\n",
    "            self.num_micro_vnf = []\n",
    "            self.microvnf_status  = False\n",
    "        else:\n",
    "            self.num_micro_vnf = self.num_micro_vnf\n",
    "            self.microvnf_status  = True\n",
    "        \n",
    "        return self.num_micro_vnf, self.micro_vnf_res, self.microvnf_status\n",
    "    \n",
    "    \n",
    "    def potential_vnf(self,threshold_decompose, done_potential_vnf):\n",
    "        #print(\"VNF: \", self.vnf)\n",
    "        self.done_potential_vnf = done_potential_vnf\n",
    "        self.granular_index = round(self.cpu_req*self.nodal_avail)\n",
    "        self.microvnf_info = self.microvnf_res_init(self.cpu_req, self.nodal_avail, self.granular_index)\n",
    "        \n",
    "        \n",
    "        if self.num_micro_vnf:# and not self.num_micro_vnf:\n",
    "            #print(\"VNF Decomposition\")\n",
    "            \n",
    "            self.decompose_candidate = \"VNF Decompose\"\n",
    "            #print(\"SFC: \", self.sfc, \"VNF: \", self.vnf, \"self.decompose_candidate: \", self.decompose_candidate)\n",
    "            self.done_potential_vnf = True\n",
    "            self.decomp = self.microvnf_const.microvnf(self.microvnf_info[0], \n",
    "                                                       self.cpu_req,\n",
    "                                                       self.microvnf_info[1], \n",
    "                                                       self.decompose_candidate) #construct the micro-VNF-FG  \n",
    "            \n",
    "            #print(\"self.decomp\", self.decomp)\n",
    "            self.unique_id = self.decomp[2]\n",
    "            \n",
    "        else:\n",
    "            #print(\"in potential else\")\n",
    "            self.decomp = None\n",
    "            self.decompose_candidate = None\n",
    "            self.num_micro_vnf = None\n",
    "            self.micro_vnf_res = None\n",
    "            self.unique_id = None\n",
    "            \n",
    "        return self.num_micro_vnf, self.micro_vnf_res, self.decompose_candidate, self.unique_id\n",
    "    \n",
    "    def nodal_availability(self,done_nodal):\n",
    "        self.done_nodal = done_nodal\n",
    "        self.granular_index = self.cpu_req*self.nodal_avail\n",
    "        self.microvnf_info = self.microvnf_res_init(self.cpu_req, self.nodal_avail, self.granular_index)\n",
    "        #print(\"microvnf_info\", self.microvnf_info)\n",
    "        \n",
    "        if self.num_micro_vnf and self.microvnf_info[2] == True:\n",
    "            self.done_nodal = True\n",
    "            self.decompose_candidate = \"Nodal Decompose\"\n",
    "            #print(\"SFC: \", self.sfc, \"VNF: \", self.vnf, \"self.decompose_candidate: \", self.decompose_candidate)\n",
    "            #print(\"num_micro_vnf\", self.num_micro_vnf)\n",
    "            self.decomp = self.microvnf_const.microvnf(self.microvnf_info[0], \n",
    "                                                       self.cpu_req,\n",
    "                                                       self.microvnf_info[1], \n",
    "                                                       self.decompose_candidate)\n",
    "            self.unique_id = self.decomp[2]\n",
    "            #print(\"==> self.done_nodal\",  self.done_nodal)\n",
    "        else:\n",
    "            #print(\"in nodal else\")\n",
    "            self.decomp = None\n",
    "            self.decompose_candidate = None\n",
    "            self.num_micro_vnf = None\n",
    "            self.micro_vnf_res = None\n",
    "            self.unique_id = None\n",
    "            #print(self.decomp, self.decompose_candidate)\n",
    "            \n",
    "        return self.num_micro_vnf, self.micro_vnf_res, self.decompose_candidate, self.unique_id\n",
    "    \n",
    "    \n",
    "    def unsuccessful_vnf(self,sum_resouces, granular_index, done_unsuccessful_vnf):\n",
    "        self.granular_index = granular_index\n",
    "        self.done_unsuccessful_vnf = done_unsuccessful_vnf\n",
    "        self.microvnf_info = self.microvnf_res_init(self.cpu_req, self.nodal_avail, self.granular_index)\n",
    "        #print(\"self.microvnf_info \", self.microvnf_info )\n",
    "        #print(\"self.num_micro_vnf\", self.num_micro_vnf)\n",
    "        #self.num_micro_vnf = []\n",
    "        \n",
    "        if self.num_micro_vnf and sum_resouces >= self.cpu_req: \n",
    "            self.done_unsuccessful_vnf = True\n",
    "            #print(\"self.done_unsuccessful_vnf\", self.done_unsuccessful_vnf)\n",
    "            self.decompose_candidate = \"Penalty Decompose\"\n",
    "            #print(\"SFC: \", self.sfc, \"VNF: \", self.vnf, \"self.decompose_candidate: \", self.decompose_candidate)\n",
    "            self.decomp = self.microvnf_const.microvnf(self.microvnf_info[0], \n",
    "                                                       self.cpu_req, \n",
    "                                                       self.microvnf_info[1], \n",
    "                                                       self.decompose_candidate) \n",
    "            self.unique_id = self.decomp[2]\n",
    "            #print(\"decomp\", self.decomp)\n",
    "            \n",
    "        else:\n",
    "            #print(\"in penalty else\")\n",
    "            self.decomp = None\n",
    "            self.decompose_candidate = None\n",
    "            self.num_micro_vnf = None\n",
    "            self.micro_vnf_res = None\n",
    "            self.unique_id = None\n",
    "            \n",
    "        return self.num_micro_vnf, self.micro_vnf_res, self.decompose_candidate, self.unique_id, self.done_unsuccessful_vnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#episode = 1\n",
    "#sfc = 3\n",
    "\n",
    "#vnf = 2\n",
    "#decompose_candidate = (2, [1, 1], 'Nodal Decompose', [2, 1]) \n",
    "#microvnf_done_status = True\n",
    "#qq= Trail(num_episode = 2, num_sfc =5, vNetwork_service_er =[] , vNetwork_service_details_er =[], max_vnf_resource =10,\n",
    "#                 vnf_per_service =3, vnf_node_resources =[], vnf_link_resources =[])\n",
    "#qq.update_vnf(episode, sfc, vnf, decompose_candidate, microvnf_done_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class substrate_network():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.topology_name = 'Netrail'#'BtEurope'\n",
    "        self.network_graph = nx.read_graphml('../Topology_Dataset/Netrail.graphml')\n",
    "        self.fig = plt.figure() \n",
    "        #print (nx.info(self.network_graph))\n",
    "        #nx.draw(self.network_graph,with_labels=True)\n",
    "        #plt.show()\n",
    "        #fig.savefig('Generated_Plots/core/for_100ms/BtEurope.png')\n",
    "        \n",
    "        self.network_len_nodes = len(self.network_graph.nodes)\n",
    "        self.range_nodes = str(self.network_graph.nodes)\n",
    "        self.tot_links= self.network_graph.edges()\n",
    "        self.len_tot_links = len(self.tot_links)\n",
    "        \n",
    "        \n",
    "    def substrate_node_init(self, removed_nodes, tot_vnf_per_core): \n",
    "        #print(\"initialization of network resouce\")\n",
    "        for node in self.network_graph.nodes():\n",
    "            if int(node) in removed_nodes:              \n",
    "                self.network_graph.add_node(node, emb_vnf = 0) #(no of core * no of VNFper core)\n",
    "            else:\n",
    "                self.network_graph.add_node(node, emb_vnf = tot_vnf_per_core) #(no of core * no of VNFper core)\n",
    "        #print(list(self.network_graph.nodes(data=True)))\n",
    "        self.network_nodes = list(self.network_graph.nodes(data=True))\n",
    "        return self.network_nodes\n",
    "    \n",
    "    \n",
    "    def substrate_link_init(self):\n",
    "        count = 0\n",
    "        for link in self.network_graph.edges():\n",
    "            self.network_graph.add_edge(link[0],link[1],\n",
    "                                        id = count,\n",
    "                                        bw = int(rand.uniform(1000, 10000)), # random.choice(link_capacity),\n",
    "                                        weight = 1,\n",
    "                                        initial = int(link[0]),\n",
    "                                        final = int(link[1]),\n",
    "                                        emb = None,\n",
    "                                        latency = round(rand.uniform(0,10), 2)#ms\n",
    "                                       )\n",
    "            count +=1\n",
    "        #print(list(self.network_graph.edges(data = True)))\n",
    "        self.network_edges = list(self.network_graph.edges(data=True))\n",
    "        return self.network_edges\n",
    "    \n",
    "    \n",
    "    def embedding_vnf(self, actions_per_iteration, vnf_node_resources, count_vnf, global_decomp, emb_mVNFID, \n",
    "                      switch_reconst):\n",
    "        self.actions_per_iteration = actions_per_iteration\n",
    "        self.vnf_node_resources = vnf_node_resources\n",
    "        self.count_vnf = count_vnf\n",
    "        self.global_decomp = global_decomp\n",
    "        #print(\"self.actions_per_iteration\", self.actions_per_iteration)\n",
    "        #print(\"inside emb\")\n",
    "        \n",
    "        if not self.actions_per_iteration:\n",
    "            pass \n",
    "        else: \n",
    "            self.count_vnf = 0\n",
    "            count_action = 0\n",
    "            #print(\"emb\")\n",
    "            #print(\"self.count_vnf\", self.count_vnf)\n",
    "            for key, val in self.vnf_node_resources:\n",
    "                #count_action = self.count_vnf\n",
    "                #print(\"self.actions_per_iteration\",self.actions_per_iteration)\n",
    "                #print(\"count_action:\",count_action)\n",
    "                if count_action >= len(self.actions_per_iteration):\n",
    "                    break\n",
    "                else:\n",
    "                    count_decomp = 0\n",
    "                    for action in self.actions_per_iteration:\n",
    "                        emb_vnf = False\n",
    "                        #print(\"action\", action)\n",
    "                        #print(\"self.count_vnf\", self.count_vnf)\n",
    "                        #print(\"count_action\", count_action)\n",
    "                        \n",
    "                        if type(action) is list:\n",
    "                            #print(action)\n",
    "                            microvnf_accpeted_action = action\n",
    "                            microvnf_node_resources =  self.global_decomp[count_decomp][0]\n",
    "                            count_microvnf = 0\n",
    "                            count_microvnf = self.embedding_microvnf(microvnf_accpeted_action,\n",
    "                                                                     microvnf_node_resources, \n",
    "                                                                     count_microvnf, \n",
    "                                                                     emb_mVNFID, \n",
    "                                                                     switch_reconst)\n",
    "                            \n",
    "                            count_action +=1 \n",
    "                            #print(\"count_microvnf:\", count_microvnf[0])\n",
    "                            #print(\"count_emb_mvnf\", count_microvnf[1])\n",
    "                            count_decomp += 1\n",
    "                            \n",
    "                            if len(action) == count_microvnf[1]:\n",
    "                                emb_vnf = True\n",
    "\n",
    "                        else:\n",
    "                            for node, available_res in self.network_nodes:\n",
    "                                if available_res['id'] == self.actions_per_iteration[self.count_vnf]:\n",
    "                                    count_action +=1 \n",
    "                                    #print(\"count_action:\",count_action)\n",
    "                                    CPU_old = available_res['emb_vnf']\n",
    "                                    #print(\"CPU_old: \", CPU_old)\n",
    "                                    \n",
    "                                    requested_vnf_resource = val['cpu_req']\n",
    "                                    #print(\"requested_vnf_resource:\", requested_vnf_resource)\n",
    "                                    \n",
    "                                    CPU_remian = CPU_old - requested_vnf_resource\n",
    "                                    #print(\"CPU_remian: \", CPU_remian)\n",
    "                                    \n",
    "                                    if CPU_old >= requested_vnf_resource:\n",
    "                                        emb_vnf = True\n",
    "                                        #print(\"here\")\n",
    "                                        #print(\"emf vnf: \", self.actions_per_iteration[self.count_vnf])\n",
    "                                        #print('CPU_old:', CPU_old,' requested_vnf_resource:', requested_vnf_resource,' CPU_remian', CPU_remian)\n",
    "                                        if CPU_remian <= 0:\n",
    "                                            #embedding\n",
    "                                            self.network_graph.add_node(node, emb_vnf = 0)\n",
    "                                        else: \n",
    "                                            self.network_graph.add_node(node, emb_vnf = CPU_remian)\n",
    "                                        \n",
    "\n",
    "                                    else:\n",
    "                                        #print(\"in else when no avail res\")\n",
    "                                        break\n",
    "                                else: \n",
    "                                    pass\n",
    "                        \n",
    "                        if emb_vnf is True:\n",
    "                            self.count_vnf += 1\n",
    "                        if count_action >= len(self.actions_per_iteration):\n",
    "                            break\n",
    "            #print(\"self.count_vnf\", self.count_vnf)\n",
    "            #print(\"count_action\", count_action)\n",
    "            return self.count_vnf\n",
    "\n",
    "        \n",
    "    def embedding_vl(self, updated_path_perSFC, vnf_link_resources, count_updated_path_perSFC, global_decomp, \n",
    "                     global_updated_path_perVNF):\n",
    "        self.count_updated_path_perSFC = count_updated_path_perSFC\n",
    "        self.global_decomp = global_decomp\n",
    "        self.global_updated_path_perVNF = global_updated_path_perVNF\n",
    "\n",
    "        \n",
    "        if self.global_decomp:\n",
    "            count_decomp = 0\n",
    "            for key in self.global_updated_path_perVNF:\n",
    "                updated_path_perVNF = key\n",
    "                microvnf_link_resources = self.global_decomp[count_decomp][1]\n",
    "                count_updated_path_pervnf = 0\n",
    "                count_updated_path_pervnf = self.embedding_microvl(updated_path_perVNF,\n",
    "                                                                   microvnf_link_resources,\n",
    "                                                                   count_updated_path_pervnf)\n",
    "                count_decomp += 1\n",
    "\n",
    "        len_updated_path = len(updated_path_perSFC)\n",
    "        if not updated_path_perSFC:\n",
    "            self.count_updated_path_perSFC = 0\n",
    "            pass\n",
    "\n",
    "        elif len_updated_path != link_space_size:\n",
    "            self.count_updated_path_perSFC = 0\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            count_sfc_link = 0\n",
    "            self.count_updated_path_perSFC = 0\n",
    "            len_updated_path = len(updated_path_perSFC)\n",
    "\n",
    "            if self.count_updated_path_perSFC == len_updated_path:\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                for requested_bw in vnf_link_resources:\n",
    "                    #print(\"Link\",count_sfc_link, requested_bw)\n",
    "                    link_perVNF = updated_path_perSFC[self.count_updated_path_perSFC]\n",
    "                    #print(\"link_perVNF: \",link_perVNF, \" count_updated_path_perSFC: \",self.count_updated_path_perSFC)\n",
    "                    source_node =[]\n",
    "                    target_node =[]\n",
    "                    count_dis = 0\n",
    "\n",
    "                    if not link_perVNF:\n",
    "                        pass\n",
    "                    else:\n",
    "                        len_dis = len(link_perVNF)\n",
    "\n",
    "                    for key in link_perVNF:\n",
    "                        if (count_dis == len_dis-1):\n",
    "                            #break\n",
    "                            pass\n",
    "                        else: \n",
    "                            source_node = link_perVNF[count_dis]\n",
    "                            target_node = link_perVNF[count_dis+1]\n",
    "\n",
    "                            #Embbeding coding \n",
    "                            for available_bw in self.network_edges:                        \n",
    "                            #Checking for the direct links \n",
    "                                if (available_bw[2]['initial'] == int(source_node) and available_bw[2]['final'] == int(target_node)) or (available_bw[2]['initial'] == int(target_node) and available_bw[2]['final'] == int(source_node)): \n",
    "\n",
    "                                    sfc_avail_bw = available_bw[2]['bw']\n",
    "                                    vnf_req_bw = requested_bw[2]['bw']\n",
    "                                    direct_remain_bw = available_bw[2]['bw'] - requested_bw[2]['bw']\n",
    "\n",
    "                                    if sfc_avail_bw >= vnf_req_bw:\n",
    "                                        #print(\"emb link\", link_perVNF)\n",
    "                                        self.network_graph.add_edge(available_bw[0],available_bw[1],\n",
    "                                                               bw = direct_remain_bw,\n",
    "                                                               emb= 1)\n",
    "                                else:\n",
    "                                    #print(\"not emb link\")\n",
    "                                    pass\n",
    "                        count_dis +=1\n",
    "                    self.count_updated_path_perSFC +=1  \n",
    "                    count_sfc_link +=1\n",
    "        \n",
    "        return self.count_updated_path_perSFC\n",
    "\n",
    "    def embedding_microvnf(self, microvnf_accpeted_action, microvnf_node_resources, count_microvnf, emb_mVNFID, switch_reconst):\n",
    "        self.microvnf_accpeted_action = microvnf_accpeted_action\n",
    "        self.microvnf_node_resources = microvnf_node_resources\n",
    "        self.count_microvnf = count_microvnf\n",
    "        self.emb_mVNFID = emb_mVNFID\n",
    "        #print(\"emb_mVNFID\", emb_mVNFID)\n",
    "        #print(\"inside emb\")\n",
    "        if not self.microvnf_accpeted_action:\n",
    "            pass \n",
    "        else: \n",
    "            #print(\"self.microvnf_accpeted_action\", self.microvnf_accpeted_action)\n",
    "            self.count_microvnf = 0\n",
    "            self.count_emb_mvnf = 0\n",
    "            #print(\"emb\", self.count_microvnf)\n",
    "            for key, val in self.microvnf_node_resources:\n",
    "                count_action = self.count_microvnf\n",
    "                for action in self.microvnf_accpeted_action:\n",
    "                    microemb_vnf = False\n",
    "                    if self.count_microvnf == count_action:\n",
    "                        for node, available_res in self.network_nodes:\n",
    "                            #print(\"self.count_microvnf\", self.count_microvnf)\n",
    "                            if available_res['id'] == microvnf_accpeted_action[self.count_microvnf]:\n",
    "                                CPU_old = available_res['emb_vnf']\n",
    "                                requested_vnf_resource = val['micro_cpu_req']\n",
    "                                #print(\"Avail Resource\", CPU_old, \"Reqested Resource\",requested_vnf_resource)\n",
    "                                #print(\"emb microvnf\",microvnf_accpeted_action[self.count_microvnf],\"microVNF_ID: \",val['microVNF_ID'])\n",
    "                                \n",
    "                                if switch_reconst is True:\n",
    "                                    #print(\"switch_reconst\", switch_reconst)\n",
    "                                    #print(self.emb_mVNFID)\n",
    "                                    if val['microVNF_ID'] in self.emb_mVNFID:\n",
    "                                        CPU_remian = CPU_old\n",
    "                                        self.count_emb_mvnf +=1\n",
    "                                        #print(\" ==> alreay present: val['microVNF_ID']\", val['microVNF_ID'])\n",
    "                                    else:  \n",
    "                                        if CPU_old >= requested_vnf_resource:\n",
    "                                            CPU_remian = CPU_old - requested_vnf_resource\n",
    "                                            self.emb_mVNFID.append(val['microVNF_ID']) \n",
    "                                            self.count_emb_mvnf +=1\n",
    "                                            #print(\"val['microVNF_ID']\", val['microVNF_ID'])\n",
    "                                        else:\n",
    "                                            #print(\"breAK\")\n",
    "                                            break\n",
    "                                else:\n",
    "\n",
    "                                    CPU_remian = CPU_old - requested_vnf_resource\n",
    "                                #print(\"==> count_emb_mvnf\", self.count_emb_mvnf)\n",
    "                                #print(\"CPU remain: \", CPU_remian)\n",
    "                                if CPU_remian <= 0:\n",
    "                                    #embedding\n",
    "                                    self.network_graph.add_node(node, emb_vnf = 0)\n",
    "                                else: \n",
    "                                    self.network_graph.add_node(node, emb_vnf = CPU_remian)\n",
    "                                count_action +=1 \n",
    "                                #print(count_action)\n",
    "\n",
    "                            else:\n",
    "                                #print(\"not emb microvnf\")\n",
    "                                pass\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                self.count_microvnf +=1\n",
    "            #print(\"inside count_action:\",count_action)\n",
    "            #print(\"inside self.count_microvnf\", self.count_microvnf)\n",
    "            return self.count_microvnf, self.count_emb_mvnf\n",
    "        \n",
    "\n",
    "    def embedding_microvl(self, updated_path_perVNF, microvnf_link_resources, count_updated_path_pervnf):\n",
    "        self.count_updated_path_pervnf = count_updated_path_pervnf\n",
    "        #print(\"count_updated_path_pervnf: \", count_updated_path_pervnf)\n",
    "        #print(\"updated_path_perVNF\", updated_path_perVNF)\n",
    "        #print(\"microvnf_link_resources\", microvnf_link_resources)\n",
    "        \n",
    "        \n",
    "        len_updated_path = len(updated_path_perVNF)\n",
    "        #print(\"len_updated_path\", len_updated_path)\n",
    "        \n",
    "        if not updated_path_perVNF:\n",
    "            self.count_updated_path_pervnf = 0\n",
    "            pass\n",
    "        elif len_updated_path != len(microvnf_link_resources):\n",
    "            self.count_updated_path_pervnf = 0\n",
    "            pass\n",
    "        else:\n",
    "            count_sfc_link = 0\n",
    "            self.count_updated_path_pervnf = 0\n",
    "            len_updated_path = len(updated_path_perVNF)\n",
    "\n",
    "            if self.count_updated_path_pervnf == len_updated_path:\n",
    "                #print(\"break\")\n",
    "                pass\n",
    "                \n",
    "            else:\n",
    "                for requested_bw in microvnf_link_resources:\n",
    "                    #print(\"Link\",count_sfc_link, requested_bw)\n",
    "                    link_perVNF = updated_path_perVNF[self.count_updated_path_pervnf]\n",
    "                    #print(\"link_perVNF\",link_perVNF, self.count_updated_path_pervnf)\n",
    "                    source_node =[]\n",
    "                    target_node =[]\n",
    "                    count_dis = 0\n",
    "\n",
    "                    if not link_perVNF:\n",
    "                        pass\n",
    "                    else:\n",
    "                        len_dis = len(link_perVNF)\n",
    "                        #print(\"len_dis\", len_dis)\n",
    "\n",
    "                    for key in link_perVNF:\n",
    "                        if (count_dis == len_dis-1):\n",
    "                            #break\n",
    "                            pass\n",
    "                        else: \n",
    "                            source_node = link_perVNF[count_dis]\n",
    "                            target_node = link_perVNF[count_dis+1]\n",
    "                            \n",
    "                            #print(\"source_node\", source_node)\n",
    "                            #print(\"target_node\", target_node)\n",
    "                            \n",
    "                            #Embbeding coding \n",
    "                            for available_bw in self.network_edges:                        \n",
    "                            #Checking for the direct links \n",
    "                                if (available_bw[2]['initial'] == int(source_node) and available_bw[2]['final'] == int(target_node)) or (available_bw[2]['initial'] == int(target_node) and available_bw[2]['final'] == int(source_node)): \n",
    "                                    sfc_avail_bw = available_bw[2]['bw']\n",
    "                                    vnf_req_bw = requested_bw[2]['bw']\n",
    "                                    direct_remain_bw = available_bw[2]['bw'] - requested_bw[2]['bw']\n",
    "                                    #print(\"==> micro link emb\")\n",
    "                                    if sfc_avail_bw >= vnf_req_bw:\n",
    "                                        self.network_graph.add_edge(available_bw[0],available_bw[1],\n",
    "                                                               bw = direct_remain_bw,\n",
    "                                                               emb= 1)\n",
    "                                else:\n",
    "                                    #print(\"not emb link\")\n",
    "                                    pass\n",
    "                        count_dis +=1\n",
    "                        #print(\"count_dis\", count_dis)\n",
    "                    self.count_updated_path_pervnf +=1               \n",
    "                    count_sfc_link +=1\n",
    "        #print(\"count_updated_path_pervnf: \", self.count_updated_path_pervnf)\n",
    "                    \n",
    "        return self.count_updated_path_pervnf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vnf_constraints():\n",
    "    def __init__ (self, amount_local_reward, vnf_res_req, nodal_avail, max_vnf_resource, episode, sfc, vnf, \n",
    "                  vnf_per_service, sum_resouces, accpeted_action, switch_decomp):\n",
    "        \n",
    "        self.amount_local_reward = amount_local_reward\n",
    "        self.vnf_res_req = vnf_res_req\n",
    "        self.nodal_avail = nodal_avail\n",
    "        self.max_vnf_resource = max_vnf_resource\n",
    "        self.episode = episode\n",
    "        self.sfc = sfc\n",
    "        self.vnf = vnf\n",
    "        self.vnf_per_service = vnf_per_service\n",
    "        self.sum_resouces = sum_resouces\n",
    "        self.accpeted_action = accpeted_action\n",
    "        self.decomp = []\n",
    "        #\n",
    "        \n",
    "    def constraint_1(self, action):\n",
    "        self.action = action\n",
    "        #\n",
    "        decompose_candidate = []\n",
    "        #print(\"constraint_1\")\n",
    "        for key, available_res in env.network_nodes:\n",
    "            #print(\"key\",key)\n",
    "            if available_res['id'] == self.action:\n",
    "                #print(\"self.vnf_res_req\", self.vnf_res_req)\n",
    "                if available_res['emb_vnf'] >= self.vnf_res_req: \n",
    "                    #print(\"available_res['emb_vnf']\", available_res['emb_vnf'])\n",
    "                    local_rewardVNF = self.amount_local_reward\n",
    "                    #print(\"Reward\", local_rewardVNF)\n",
    "                    self.accpeted_action.append(self.action)\n",
    "                    #print(\"const 1: self.accpeted_action\", self.accpeted_action)\n",
    "                else:\n",
    "                    if switch_decomp is False: \n",
    "                        self.decomp = None\n",
    "                        local_rewardVNF = -(self.amount_local_reward)\n",
    "                        #print(\"insufficent resouce in vnf\", local_rewardVNF)\n",
    "                    else:\n",
    "                        #print(\"\")\n",
    "                        #print(\"decomposition inside constraint_1\")\n",
    "                        #print(\"action\", self.action, available_res['emb_vnf'])\n",
    "                        #print(\"insufficent resouce in vnf\")\n",
    "                        topology_available_resource = available_res['emb_vnf']\n",
    "                        local_rewardVNF = -(self.amount_local_reward)\n",
    "                        done_unsuccessful_vnf = False\n",
    "                        granular_index = self.vnf_res_req * self.nodal_avail\n",
    "\n",
    "                        if granular_index <= 1:\n",
    "                            granular_index = 1\n",
    "                        else: \n",
    "                            granular_index = granular_index\n",
    "\n",
    "                        #print(\"granular_index\", granular_index)\n",
    "\n",
    "                        decompose_criteria = decomp_identifier(self.vnf_res_req, self.nodal_avail,self.max_vnf_resource, \n",
    "                                                             self.episode, self.sfc, self.vnf, self.vnf_per_service) \n",
    "\n",
    "                        decompose_criteria.unsuccessful_vnf(self.sum_resouces, granular_index, done_unsuccessful_vnf) \n",
    "                        decompose_candidate = [decompose_criteria.num_micro_vnf, \n",
    "                                               decompose_criteria.micro_vnf_res, \n",
    "                                               decompose_criteria.decompose_candidate\n",
    "                                              ]\n",
    "                        #print(decompose_candidate)\n",
    "                        if decompose_criteria.decompose_candidate is None:\n",
    "                            pass #print(done_unsuccessful_vnf)\n",
    "                        else: \n",
    "                            done_unsuccessful_vnf = True\n",
    "                            #print(done_unsuccessful_vnf)\n",
    "                        #print('done_unsuccessful_vnf', done_unsuccessful_vnf)\n",
    "                        #print(decompose_candidate.done_unsuccessful_vnf)\n",
    "                        global_services.update_vnf(self.episode, \n",
    "                                                   self.sfc, \n",
    "                                                   self.vnf, \n",
    "                                                   decompose_candidate, \n",
    "                                                   done_unsuccessful_vnf)\n",
    "\n",
    "                        self.decomp = decompose_criteria.decomp\n",
    "                    \n",
    "                    #print(\"vnf_const.er_graph_seed.add_node\", vnf_const.er_graph_seed.add_node)\n",
    "                    #print(\"local_rewardVNF: \",local_rewardVNF)\n",
    "        #print('done_unsuccessful_vnf: ', done_unsuccessful_vnf)\n",
    "        return local_rewardVNF\n",
    "    \n",
    "    def constraint_2(self,up_actionspace, sorted_action):\n",
    "        #print(\"constraint_2\")\n",
    "        self.up_actionspace = up_actionspace\n",
    "        self.sorted_action = sorted_action\n",
    "        #print(\"self.up_actionspace\", self.up_actionspace)\n",
    "        #print(\"self.sorted_action\", self.sorted_action)\n",
    "        #print(\"self.accpeted_action\", self.accpeted_action)\n",
    "        \n",
    "        for a_key in self.accpeted_action:\n",
    "            if a_key in self.up_actionspace:\n",
    "                self.up_actionspace.remove(a_key)\n",
    "                del self.sorted_action[str(a_key)]\n",
    "                #print(\"a_key\", a_key)\n",
    "        return #self.up_actionspace, self.sorted_action \n",
    "    \n",
    "\n",
    "    def constraint_3(self, state_space_size, vnf_link_resources, env_networkgraph, env_networkedges, \n",
    "                     threshold_delay, updated_path_perSFC):\n",
    "        #print(\"constraint_3\")\n",
    "        self.state_space_size   = state_space_size\n",
    "        self.vnf_link_resources = vnf_link_resources\n",
    "        self.env_networkgraph   = env_networkgraph    \n",
    "        self.env_networkedges   = env_networkedges\n",
    "        self.threshold_delay    = threshold_delay\n",
    "        self.local_reward_link  = []\n",
    "        self.local_reward_delay = 0\n",
    "        self.decomp_nodes       = []\n",
    "        self.updated_path_perSFC = updated_path_perSFC\n",
    "\n",
    "        if len(self.accpeted_action) == self.state_space_size:\n",
    "            \n",
    "            count_sfc_link = 1\n",
    "            for requested_bw in self.vnf_link_resources:\n",
    "                #print(\"link\", count_sfc_link, requested_bw)\n",
    "                \n",
    "                #Finding the Path\n",
    "                Start_node = self.accpeted_action[requested_bw[0]]\n",
    "                End_node   = self.accpeted_action[requested_bw[1]]\n",
    "                \n",
    "                #print(\"==\")\n",
    "                if type(Start_node) == list:\n",
    "                    #print(\"Start Node\", Start_node)\n",
    "                    t_cnt = 0\n",
    "                    for t_key in Start_node:\n",
    "                        if type(t_key) is list:\n",
    "                            #print(\"t_key\",t_key)\n",
    "                            Start_node[t_cnt] = t_key[1]\n",
    "                            #print(\"Start_node[t_cnt]\", Start_node[t_cnt])\n",
    "                        t_cnt += 1 \n",
    "                    self.decomp_nodes = Start_node\n",
    "                    len_startnode = len(Start_node)\n",
    "                    Start_node = self.decomp_nodes[len_startnode-1]\n",
    "                #print(\"Start Node\", Start_node)\n",
    "                \n",
    "                if type(End_node) == list:\n",
    "                    #print(\"End_node\", End_node)\n",
    "                    t_cnt = 0\n",
    "                    for t_key in End_node:\n",
    "                        if type(t_key) is list:\n",
    "                            End_node[t_cnt] = t_key[1]\n",
    "                        t_cnt += 1\n",
    "                    self.decomp_nodes = End_node\n",
    "                    End_node = self.decomp_nodes[0]\n",
    "                #print(\"End_node\", End_node)\n",
    "\n",
    "                    \n",
    "                #print(\"inside const 3: accpeted_action\", self.accpeted_action, count_sfc_link)    \n",
    "                if Start_node == End_node:\n",
    "                    self.local_reward_delay = 1000\n",
    "                    intralink = ['intra link']\n",
    "                    self.updated_path_perSFC.append(intralink)\n",
    "                    count_sfc_link +=1\n",
    "                else:\n",
    "                    \n",
    "                    #print(Start_node,End_node)\n",
    "                    #print(\"..\")\n",
    "                    path =  nx.all_simple_paths(self.env_networkgraph,str(Start_node),str(End_node))\n",
    "                    count_path = 0\n",
    "                    \n",
    "                    #self.local_reward_link  = [] \n",
    "                    tot_score, delay_per_link =[], []\n",
    "                    len_delay_per_link,len_each_path =[], []\n",
    "                    score = 0\n",
    "\n",
    "                    for each_path in path:\n",
    "                        #print(\"each_path\", each_path)\n",
    "                        delay_per_link =[]\n",
    "                        updated_path_perVNF = each_path\n",
    "\n",
    "                        if score == 0:\n",
    "                            len_dis = len(each_path)\n",
    "                            #print(\"len_dis\",len_dis)\n",
    "                            count_dis = 0\n",
    "                            source_node, target_node =[], []\n",
    "\n",
    "                            for key in each_path:\n",
    "                                #print(\"key in each path\", key)\n",
    "                                if count_dis == len_dis-1:\n",
    "                                    #print(\"break because it has reached the end of the path\")\n",
    "                                    break\n",
    "                                else:\n",
    "                                    source_node = each_path[count_dis]\n",
    "                                    target_node = each_path[count_dis+1]\n",
    "                                    #print(\"source and target: \", source_node, target_node)\n",
    "\n",
    "                                for available_bw in env_networkedges:                        \n",
    "                                    if (available_bw[2]['initial'] == int(source_node) and available_bw[2]['final'] == int(target_node)) or (available_bw[2]['initial'] == int(target_node) and available_bw[2]['final'] == int(source_node)): \n",
    "                                        #Checking for the available resources\n",
    "                                        sfc_avail_bw    = available_bw[2]['bw']\n",
    "                                        vnf_req_bw      = requested_bw[2]['bw']\n",
    "                                        sfc_avail_delay = available_bw[2]['latency']\n",
    "                                        vnf_req_delay   = requested_bw[2]['latency']\n",
    "\n",
    "                                        if (sfc_avail_bw >= vnf_req_bw):# and (sfc_avail_delay >= vnf_req_delay):\n",
    "                                            local_reward_link = self.amount_local_reward\n",
    "                                            score = 1\n",
    "                                            delay_per_link.append(sfc_avail_delay) \n",
    "                                            #print(\" link Reward \", source_node,target_node) \n",
    "                                        else: \n",
    "                                            local_reward_link = -10000\n",
    "                                            pen_score = 2\n",
    "                                            score = pen_score\n",
    "                                            #print(\"link penalty\", source_node,target_node) \n",
    "                                            pass\n",
    "\n",
    "                                count_dis += 1\n",
    "\n",
    "                            tot_delay_perPath  = sum(delay_per_link)\n",
    "                            len_delay_per_link = len(delay_per_link)\n",
    "                            len_each_path      = len(each_path)-1\n",
    "                            #print(\"count_sfc_link\",count_sfc_link)\n",
    "                            #print(\"len_delay_per_link\", len_delay_per_link, delay_per_link)\n",
    "                            #print(\"len_each_path\", len_each_path, each_path)\n",
    "                            #print(\"self.threshold_delay\", self.threshold_delay)\n",
    "                            #print(\"tot_delay_perPath\", tot_delay_perPath)\n",
    "\n",
    "\n",
    "                            if len_delay_per_link == len_each_path: #end of the path\n",
    "                                \n",
    "                                if tot_delay_perPath <= self.threshold_delay:\n",
    "                                    self.updated_path_perSFC.append(each_path)\n",
    "                                    self.local_reward_delay = self.amount_local_reward\n",
    "                                    #print(\"==> Reward in latency\", each_path)\n",
    "                                    #print(\" link Reward \",self.local_reward_delay)\n",
    "                                else: \n",
    "                                    score = 0\n",
    "                                    #print(\"link not found vnf\")\n",
    "                                    self.local_reward_delay = -10000\n",
    "                            else:\n",
    "                                score = 0\n",
    "                                #print(\"link not found vnf\", each_path, tot_delay_perPath)\n",
    "                                self.local_reward_delay = -10000\n",
    "\n",
    "                        else:\n",
    "                            pass\n",
    "                            #break\n",
    "\n",
    "                        count_path +=1\n",
    "                    count_sfc_link += 1 \n",
    "                self.local_reward_link.append(self.local_reward_delay)\n",
    "                #print(\"local delay reward link: \",self.local_reward_delay)\n",
    "                #print(\"count_sfc_link\",count_sfc_link)\n",
    "                #print(\"updated_path_perSFC\", self.updated_path_perSFC)\n",
    "            #print(\"end of cont 3: accpeted_action\", self.accpeted_action)\n",
    "            #print(\"temp_accpeted_action\", temp_accpeted_action)\n",
    "        #print(\"+++++++++++++ trail_ac\", trail_ac)\n",
    "        return \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class microvnf_constraints():\n",
    "    def __init__ (self, action, micro_vnf_res, amount_local_reward, microvnf_accpeted_action):\n",
    "        #print(\"import all the reqired inputs\")\n",
    "        self.action = action\n",
    "        self.micro_vnf_res = micro_vnf_res\n",
    "        self.amount_local_reward = amount_local_reward\n",
    "        self.microvnf_accpeted_action = microvnf_accpeted_action\n",
    "        #print(\"microvnf_accpeted_action\", self.microvnf_accpeted_action)\n",
    "        \n",
    "    def constraint_1(self):\n",
    "        #print(\"Constraint 1\")\n",
    "        for key, available_res in env.network_nodes:\n",
    "            if available_res['id'] == self.action:\n",
    "                if available_res['emb_vnf'] >= self.micro_vnf_res: \n",
    "                    local_rewardmicroVNF = self.amount_local_reward\n",
    "                    #print(\"Reward\", local_rewardmicroVNF)\n",
    "                    self.microvnf_accpeted_action.append(self.action)\n",
    "                else:\n",
    "                    #print(\"insufficent resouce in micro\")\n",
    "                    #print(self.micro_vnf_res)\n",
    "                    local_rewardmicroVNF = -1000\n",
    "        \n",
    "        return local_rewardmicroVNF\n",
    "    \n",
    "    \n",
    "    def constraint_2(self, microstate_space_size, microvnf_link_resources):\n",
    "\n",
    "        self.microstate_space_size = microstate_space_size\n",
    "        #print(self.microstate_space_size)\n",
    "        self.microvnf_link_resources = microvnf_link_resources\n",
    "        \n",
    "        self.threshold_delay    = 50 #30\n",
    "        self.local_reward_link  = []\n",
    "        self.local_reward_delay = []\n",
    "        local_reward_delay = 0\n",
    "        \n",
    "        #print(\"microvnf_accpeted_action\", microvnf_accpeted_action)\n",
    "        if len(self.microvnf_accpeted_action) == self.microstate_space_size:\n",
    "            count_microsfc_link = 1\n",
    "            #print(\"Constraint 2\")\n",
    "            #print(self.microvnf_accpeted_action)\n",
    "            for head_link, end_link, requested_bw in self.microvnf_link_resources:\n",
    "                \n",
    "                \n",
    "                #Finding the Path\n",
    "                Start_node = self.microvnf_accpeted_action[head_link]\n",
    "                End_node   = self.microvnf_accpeted_action[end_link]\n",
    "                #print(self.microvnf_link_resources)\n",
    "                #print(type(Start_node), type(End_node))\n",
    "                \n",
    "                if type(Start_node) == list:\n",
    "                    Start_node = Start_node[1]\n",
    "                    \n",
    "                if type(End_node) is list:\n",
    "                    End_node = End_node[1]\n",
    "                    \n",
    "                if Start_node == End_node:\n",
    "                    local_reward_delay = 1000\n",
    "                    intralink = ['intra link']\n",
    "                    updated_path_perVNF.append(intralink)\n",
    "                    #sprint(\"updated_path_perVNF\", updated_path_perVNF)\n",
    "                    #print(\"local_reward_delay\", local_reward_delay)\n",
    "                    count_microsfc_link +=1\n",
    "                else: \n",
    "                \n",
    "                    path = nx.all_simple_paths(env.network_graph,\n",
    "                                               str(Start_node),\n",
    "                                               str(End_node))\n",
    "                    #print(\"Path\", path)\n",
    "                    \n",
    "                    #shortest_path = nx.all_shortest_paths(env.network_graph, \n",
    "                    #                                      str(Start_node), \n",
    "                    #                                     str(End_node),\n",
    "                    #                                      weight=None, \n",
    "                    #                                      method='dijkstra')\n",
    "                    \n",
    "                    count_path = 0\n",
    "                    tot_score, delay_per_link =[], []\n",
    "                    len_delay_per_link,len_each_path =[], []\n",
    "                    score = 0\n",
    "\n",
    "                    for each_path in path:\n",
    "                        #print(\"each path\", each_path)\n",
    "                        delay_per_link =[]\n",
    "                        updated_path_permVNF = each_path\n",
    "\n",
    "                        if score == 0:\n",
    "                            len_dis = len(each_path)\n",
    "                            count_dis = 0\n",
    "                            source_node, target_node =[], []\n",
    "\n",
    "                            for key in each_path:\n",
    "                                if count_dis == len_dis-1:\n",
    "                                    break\n",
    "                                else:\n",
    "                                    source_node = each_path[count_dis]\n",
    "                                    target_node = each_path[count_dis+1]\n",
    "                                    #print(source_node, target_node)\n",
    "\n",
    "                                for initial_link, final_link, available_bw in env.network_edges:   \n",
    "                                    if (initial_link == source_node and final_link == target_node) or (initial_link == target_node and final_link == source_node): \n",
    "                                        #Checking for the available resources\n",
    "                                        sfc_avail_bw    = available_bw['bw']\n",
    "                                        vnf_req_bw      = requested_bw['bw']\n",
    "                                        sfc_avail_delay = available_bw['latency']\n",
    "                                        vnf_req_delay   = requested_bw['latency']\n",
    "\n",
    "\n",
    "                                        if (sfc_avail_bw >= vnf_req_bw): \n",
    "                                            local_reward_link = self.amount_local_reward\n",
    "                                            score = 1\n",
    "                                            delay_per_link.append(sfc_avail_delay) \n",
    "                                            #print(\" link Reward \",local_reward_link)\n",
    "                                        else: \n",
    "                                            local_reward_link = -10000\n",
    "                                            pen_score = 2\n",
    "                                            score = pen_score\n",
    "                                            #print(\"link penalty\") \n",
    "                                            pass\n",
    "\n",
    "                                count_dis += 1\n",
    "\n",
    "                            tot_delay_perPath  = sum(delay_per_link)\n",
    "                            #print(each_path, tot_delay_perPath)\n",
    "                            len_delay_per_link = len(delay_per_link)\n",
    "                            len_each_path      = len(each_path)-1\n",
    "\n",
    "                            self.local_reward_link.append(local_reward_link)\n",
    "                            #print(\"local reward link: \",self.local_reward_link)\n",
    "\n",
    "                            if len_delay_per_link == len_each_path: #end of the path\n",
    "                                if tot_delay_perPath <= self.threshold_delay:\n",
    "                                    updated_path_perVNF.append(each_path)\n",
    "                                    #print(\"Each path\", each_path, tot_delay_perPath)\n",
    "                                    local_reward_delay = self.amount_local_reward\n",
    "                                    #print(\"Reward in latency\", score)\n",
    "                                    #print(\" link Reward \",local_reward_delay)\n",
    "                                    pass\n",
    "                                else: \n",
    "                                    score = 0\n",
    "                                    #print(\"link not found microvnf\")\n",
    "                                    local_reward_delay = -10000\n",
    "                            else:\n",
    "                                score = 0\n",
    "                                local_reward_delay = -10000\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "\n",
    "                        count_path +=1\n",
    "                    count_microsfc_link += 1   \n",
    "                \n",
    "                self.local_reward_delay.append(local_reward_delay)\n",
    "                #print(self.local_reward_delay)\n",
    "\n",
    "                \n",
    "        #print(\"local reward delay: \", self.local_reward_delay)\n",
    "        return \n",
    "                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward_Function():\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self.localreward = 1000\n",
    "        \n",
    "    def reward_vnf(self, global_reward, rewardDelay_per_current_states, rewardlink_per_current_states, \n",
    "                   actions_per_iteration,\n",
    "                   accpeted_action, vnf_node_resources, count_vnf, updated_path_perSFC, vnf_link_resources, \n",
    "                   count_updated_path_perSFC, count_accpeted, global_decomp, emb_mVNFID, switch_reconst, \n",
    "                   global_updated_path_perVNF):\n",
    "        \n",
    "        self.global_reward = global_reward\n",
    "        self.rewardDelay_per_current_states = rewardDelay_per_current_states\n",
    "        self.rewardlink_per_current_states = rewardlink_per_current_states\n",
    "        self.actions_per_iteration = actions_per_iteration\n",
    "        self.accpeted_action = accpeted_action\n",
    "        self.vnf_node_resources = vnf_node_resources\n",
    "        self.count_vnf = count_vnf\n",
    "        self.updated_path_perSFC = updated_path_perSFC\n",
    "        self.vnf_link_resources = vnf_link_resources\n",
    "        self.count_updated_path_perSFC = count_updated_path_perSFC\n",
    "        self.count_accpeted = count_accpeted\n",
    "        self.global_decomp = global_decomp\n",
    "        self.emb_mVNFID = emb_mVNFID\n",
    "        self.global_updated_path_perVNF = global_updated_path_perVNF\n",
    "        #print(\"inside reward funtion:  accpeted_action\",accpeted_action)\n",
    "\n",
    "        done = True\n",
    "        reward_all = 1000*10\n",
    "        rewars_delay = sum(self.rewardDelay_per_current_states)\n",
    "        reward_link = sum(self.rewardlink_per_current_states)\n",
    "        self.global_reward = reward_all + rewars_delay + reward_link \n",
    "        self.actions_per_iteration = self.accpeted_action\n",
    "        score = 1\n",
    "        self.count_accpeted +=1\n",
    "        #print(\"emb\", self.actions_per_iteration)\n",
    "        #---Embedding Starts here---#\n",
    "        env.embedding_vnf(self.actions_per_iteration, self.vnf_node_resources, self.count_vnf, self.global_decomp, \n",
    "                          self.emb_mVNFID, switch_reconst)\n",
    "        self.count_vnf = env.count_vnf\n",
    "\n",
    "        ##---- Link Placement ----##\n",
    "        #print(\"self.updated_path_perSFC\", self.updated_path_perSFC)\n",
    "        #print(\"self.vnf_link_resources\", self.vnf_link_resources)\n",
    "        #print(\"self.count_updated_path_perSFC\", self.count_updated_path_perSFC)\n",
    "    \n",
    "        env.embedding_vl(self.updated_path_perSFC, self.vnf_link_resources, self.count_updated_path_perSFC, self.global_decomp, \n",
    "                        self.global_updated_path_perVNF)   \n",
    "        self.count_updated_path_perSFC = env.count_updated_path_perSFC\n",
    "        \n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside the MLP model\n"
     ]
    }
   ],
   "source": [
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    print(\"inside the MLP model\")\n",
    "    # define model elements\n",
    "    def __init__(self, tr_n_state, tr_n_actions, hidden_neuron):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(tr_n_state[0], hidden_neuron)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(hidden_neuron, hidden_neuron)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(hidden_neuron, tr_n_actions)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    " \n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        #print(\"x forward: \", X)\n",
    "        #X = T.tensor([X], dtype = T.float)\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def traffic_prepare_train_data(tr_input_train_mem, tr_target_train_mem, tr_input_train_data, tr_target_train_data,\n",
    "                                X, Y, traf_mem_cnt, traf_mem_size, traf_batch_size):\n",
    "        data = [X, Y]\n",
    "        #print(data)\n",
    "        #storing the dataset\n",
    "        index = traf_mem_cnt % traf_mem_size\n",
    "        tr_input_train_mem[index] = X\n",
    "        tr_target_train_mem[index] = Y[0]\n",
    "        traf_mem_cnt += 1\n",
    "        #print(tr_mem_cnt, tr_batch_size)\n",
    "\n",
    "        if traf_mem_cnt < traf_batch_size:       \n",
    "            batch = np.random.choice(traf_mem_cnt, traf_mem_cnt, replace =False)\n",
    "            tr_input_train_data = T.tensor(tr_input_train_mem[batch], dtype=T.float)\n",
    "            tr_target_train_data = T.tensor(tr_target_train_mem[batch], dtype=T.float)\n",
    "        else:\n",
    "            max_mem = min(traf_mem_cnt, traf_mem_size)\n",
    "            batch = np.random.choice(max_mem, traf_batch_size, replace =False) \n",
    "            #print(\"batch: \", batch)\n",
    "            tr_input_train_data = T.tensor(tr_input_train_mem[batch], dtype=T.float)\n",
    "            tr_target_train_data = T.tensor(tr_target_train_mem[batch], dtype=T.float)\n",
    "\n",
    "        return tr_input_train_data, tr_target_train_data, traf_mem_cnt\n",
    "\n",
    "    # train the model\n",
    "    def train_model(input_train_data, target_train_data, pr_MLP_model):\n",
    "        # define the optimization\n",
    "        criterion = MSELoss()\n",
    "        optimizer = optim.Adam(pr_MLP_model.parameters(), lr=0.001) #, momentum=0.9)\n",
    "        optimizer.zero_grad()\n",
    "        yhat = pr_MLP_model(input_train_data)\n",
    "        loss = criterion(yhat, target_train_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(yhat)\n",
    "        return yhat[0], loss\n",
    "\n",
    "\n",
    "    # evaluate the model\n",
    "    def evaluate_model(input_test_data, target_test_data, pr_MLP_model):\n",
    "        predictions, actuals = list(), list()\n",
    "\n",
    "        # evaluate the model on the test set\n",
    "        yhat = pr_MLP_model(input_test_data)\n",
    "\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = target_test_data.detach().numpy()\n",
    "\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        yhat = yhat.reshape((len(yhat), 1))\n",
    "        #print(yhat, actual)\n",
    "\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "\n",
    "        predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "        # calculate accuracy\n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        return mse, yhat\n",
    "\n",
    "\n",
    "    # make a class prediction for one row of data\n",
    "    def predict(X, pr_MLP_model):\n",
    "        # convert row to data\n",
    "        X = T.tensor([X], dtype = T.float)  #T.tensor([X], dtype = T.float)\n",
    "        # make prediction\n",
    "        yhat = pr_MLP_model(X)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        #print(\"predict yhat: \", yhat)\n",
    "        return yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(input_train_mem, target_train_mem, input_train_data, target_train_data,\n",
    "                       X, Y, tr_mem_cnt, tr_mem_size, tr_batch_size):\n",
    "    data = [X, Y]\n",
    "    #print(data)\n",
    "    #storing the dataset\n",
    "    index = tr_mem_cnt % tr_mem_size\n",
    "    input_train_mem[index] = X\n",
    "    target_train_mem[index] = Y[0]\n",
    "    tr_mem_cnt += 1\n",
    "    #print(tr_mem_cnt, tr_batch_size)\n",
    "    \n",
    "    if tr_mem_cnt < tr_batch_size:       \n",
    "        batch = np.random.choice(tr_mem_cnt, tr_mem_cnt, replace =False)\n",
    "        input_train_data = T.tensor(input_train_mem[batch], dtype=T.float)\n",
    "        target_train_data = T.tensor(target_train_mem[batch], dtype=T.float)\n",
    "    else:\n",
    "        max_mem = min(tr_mem_cnt, tr_mem_size)\n",
    "        batch = np.random.choice(max_mem, tr_batch_size, replace =False) \n",
    "        #print(\"batch: \", batch)\n",
    "        input_train_data = T.tensor(input_train_mem[batch], dtype=T.float)\n",
    "        target_train_data = T.tensor(target_train_mem[batch], dtype=T.float)\n",
    "\n",
    "    return input_train_data, target_train_data, tr_mem_cnt\n",
    "\n",
    "\n",
    "def prepare_test_data(input_test_mem, target_test_mem, input_test_data, target_test_data, \n",
    "                      test_mem_cnt, test_mem_size, test_batch_size, X, Y):\n",
    "    data = [X, Y]\n",
    "    index = test_mem_cnt % test_mem_size\n",
    "    input_test_mem[index] = X\n",
    "    target_test_mem[index] = Y[0]\n",
    "\n",
    "    test_mem_cnt += 1\n",
    "\n",
    "    if test_mem_cnt < test_batch_size:    \n",
    "        batch = np.random.choice(test_mem_cnt, test_mem_cnt, replace =False)\n",
    "        input_test_data  = T.tensor(input_test_mem[batch], dtype=T.float)\n",
    "        target_test_data = T.tensor(target_test_mem[batch], dtype=T.float)\n",
    "    else:\n",
    "        max_mem = min(test_mem_cnt, test_mem_size)\n",
    "        batch = np.random.choice(max_mem, test_batch_size, replace =False) \n",
    "        input_test_data  = T.tensor(input_test_mem[batch], dtype=T.float)\n",
    "        target_test_data = T.tensor(target_test_mem[batch], dtype=T.float)\n",
    "\n",
    "    return input_test_data, target_test_data, test_mem_cnt   \n",
    "\n",
    "\n",
    "\n",
    "def wfq_sch(arr_sfc_de, h_pr_queue, m_pr_queue, l_pr_queue):\n",
    "    wt_H,wt_M, wt_L = 20,10,5\n",
    "    temp_sorted = []\n",
    "    cnt = 0\n",
    "    cnt_wt_H, cnt_wt_M, cnt_wt_L = 0, 0, 0\n",
    "    status = False\n",
    "\n",
    "    \n",
    "    while status != True:\n",
    "        \n",
    "        if cnt == (len(arr_sfc_de)):\n",
    "            #print(\"here\", cnt)\n",
    "            status = True\n",
    "        else: \n",
    "            # high pr queue\n",
    "            if not h_pr_queue:\n",
    "                pass\n",
    "            else: \n",
    "                temp_cnt_wt_H = 0\n",
    "                if len(h_pr_queue) >= wt_H:\n",
    "                        cond1 = wt_H\n",
    "                else:\n",
    "                    cond1 = len(h_pr_queue)\n",
    "                while temp_cnt_wt_H <= (cond1-1):\n",
    "                    var = h_pr_queue[0]\n",
    "                    temp_sorted.append(var)\n",
    "                    h_pr_queue.remove(var)\n",
    "                    cnt_wt_H +=1\n",
    "                    temp_cnt_wt_H += 1\n",
    "                    cnt += 1\n",
    "\n",
    "            # medium pr queue\n",
    "            if not m_pr_queue:\n",
    "                pass\n",
    "            else: \n",
    "                temp_cnt_wt_M = 0\n",
    "                if len(m_pr_queue) >= wt_M:\n",
    "                        cond1 = wt_M\n",
    "                else:\n",
    "                    cond1 = len(m_pr_queue)\n",
    "\n",
    "                while temp_cnt_wt_M <= (cond1-1):\n",
    "                    var = m_pr_queue[0]\n",
    "                    #print(var)\n",
    "                    temp_sorted.append(var)\n",
    "                    m_pr_queue.remove(var)\n",
    "                    cnt_wt_M +=1\n",
    "                    temp_cnt_wt_M += 1\n",
    "                    cnt += 1\n",
    "\n",
    "            # low pr queue\n",
    "            if not l_pr_queue:\n",
    "                pass\n",
    "            else: \n",
    "                temp_cnt_wt_L = 0\n",
    "                if len(l_pr_queue) >= wt_L:\n",
    "                        cond1 = wt_L\n",
    "                else:\n",
    "                    cond1 = len(l_pr_queue)\n",
    "                while temp_cnt_wt_L <= (cond1-1):\n",
    "                    var = l_pr_queue[0]\n",
    "                    temp_sorted.append(var)\n",
    "                    l_pr_queue.remove(var)\n",
    "                    cnt_wt_L +=1\n",
    "                    temp_cnt_wt_L += 1\n",
    "                    cnt += 1\n",
    "    \n",
    "    return temp_sorted\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h_pr_queue =  [1, 3, 5]\n",
    "#m_pr_queue =  [0, 2, 6, 4]\n",
    "#l_pr_queue =  [] #[1, 3, 5]\n",
    "#print(len(arr_sfc_de))\n",
    "\n",
    "#ww = wfq_sch(arr_sfc_de, h_pr_queue, m_pr_queue, l_pr_queue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arrival_sfc():\n",
    "    def  __init__ (self, num_sfc, arr_sfc_dist):\n",
    "        \n",
    "        self.num_sfc = num_sfc\n",
    "        self.arr_sfc_dist = arr_sfc_dist\n",
    "    \n",
    "    def sfc_poisson_dist(self):\n",
    "\n",
    "        #generate Poisson distribution with sample size 10000\n",
    "        for i in range(self.num_sfc): \n",
    "            sfc_poisson = poisson.rvs(mu=self.num_sfc, size=1)\n",
    "            print(\"sfc_poisson: \", sfc_poisson)\n",
    "            temp_arr_sfc_dist  = sum(self.arr_sfc_dist) + int(sfc_poisson)\n",
    "            print(\"->\",temp_arr_sfc_dist)\n",
    "            done_dist = False\n",
    "            if temp_arr_sfc_dist == self.num_sfc: \n",
    "                done_dist = True\n",
    "                self.arr_sfc_dist.append(int(sfc_poisson))\n",
    "                break\n",
    "\n",
    "            elif temp_arr_sfc_dist < self.num_sfc:\n",
    "                done_dist = False\n",
    "                self.arr_sfc_dist.append(int(sfc_poisson))\n",
    "            else:\n",
    "                done_dist = False\n",
    "                pass\n",
    "        print(\"arr_sfc_dist: \", arr_sfc_dist)\n",
    "        return self.arr_sfc_dist\n",
    "    \n",
    "    \n",
    "    def sfc_details(self,per_episode,arr_sfc_dist, sfc_arrival_detail,sfc_arrival_num):\n",
    "        \n",
    "        self.per_episode = per_episode\n",
    "        self.arr_sfc_dist = arr_sfc_dist\n",
    "        self.sfc_arrival_detail = sfc_arrival_detail\n",
    "        self.sfc_arrival_num = sfc_arrival_num\n",
    "        cnt = 0\n",
    "        for i in self.arr_sfc_dist:\n",
    "            temp, temp_detail = [], []\n",
    "            for ii in range(i):\n",
    "                temp.append(cnt)\n",
    "                temp_detail.append(self.per_episode[cnt])\n",
    "                cnt += 1\n",
    "            self.sfc_arrival_num.append(temp)\n",
    "            self.sfc_arrival_detail.append(temp_detail)      \n",
    "        #print(\"=>\", self.sfc_arrival_num) #self.sfc_arrival_detail, self.sfc_arrival_num)\n",
    "        return   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sfc_poisson:  [56]\n",
      "-> 56\n",
      "sfc_poisson:  [49]\n",
      "-> 49\n",
      "sfc_poisson:  [44]\n",
      "-> 93\n",
      "sfc_poisson:  [55]\n",
      "-> 104\n",
      "sfc_poisson:  [45]\n",
      "-> 94\n",
      "sfc_poisson:  [54]\n",
      "-> 103\n",
      "sfc_poisson:  [41]\n",
      "-> 90\n",
      "sfc_poisson:  [56]\n",
      "-> 105\n",
      "sfc_poisson:  [43]\n",
      "-> 92\n",
      "sfc_poisson:  [56]\n",
      "-> 105\n",
      "sfc_poisson:  [56]\n",
      "-> 105\n",
      "sfc_poisson:  [54]\n",
      "-> 103\n",
      "sfc_poisson:  [60]\n",
      "-> 109\n",
      "sfc_poisson:  [62]\n",
      "-> 111\n",
      "sfc_poisson:  [45]\n",
      "-> 94\n",
      "sfc_poisson:  [51]\n",
      "-> 100\n",
      "sfc_poisson:  [46]\n",
      "-> 95\n",
      "sfc_poisson:  [48]\n",
      "-> 97\n",
      "sfc_poisson:  [44]\n",
      "-> 93\n",
      "sfc_poisson:  [47]\n",
      "-> 96\n",
      "sfc_poisson:  [61]\n",
      "-> 110\n",
      "sfc_poisson:  [49]\n",
      "-> 98\n",
      "sfc_poisson:  [60]\n",
      "-> 109\n",
      "sfc_poisson:  [43]\n",
      "-> 92\n",
      "sfc_poisson:  [61]\n",
      "-> 110\n",
      "sfc_poisson:  [57]\n",
      "-> 106\n",
      "sfc_poisson:  [52]\n",
      "-> 101\n",
      "sfc_poisson:  [45]\n",
      "-> 94\n",
      "sfc_poisson:  [60]\n",
      "-> 109\n",
      "sfc_poisson:  [47]\n",
      "-> 96\n",
      "sfc_poisson:  [44]\n",
      "-> 93\n",
      "sfc_poisson:  [54]\n",
      "-> 103\n",
      "sfc_poisson:  [60]\n",
      "-> 109\n",
      "sfc_poisson:  [57]\n",
      "-> 106\n",
      "sfc_poisson:  [61]\n",
      "-> 110\n",
      "sfc_poisson:  [60]\n",
      "-> 109\n",
      "sfc_poisson:  [46]\n",
      "-> 95\n",
      "sfc_poisson:  [52]\n",
      "-> 101\n",
      "sfc_poisson:  [45]\n",
      "-> 94\n",
      "sfc_poisson:  [36]\n",
      "-> 85\n",
      "sfc_poisson:  [47]\n",
      "-> 96\n",
      "sfc_poisson:  [48]\n",
      "-> 97\n",
      "sfc_poisson:  [49]\n",
      "-> 98\n",
      "sfc_poisson:  [63]\n",
      "-> 112\n",
      "sfc_poisson:  [41]\n",
      "-> 90\n",
      "sfc_poisson:  [62]\n",
      "-> 111\n",
      "sfc_poisson:  [32]\n",
      "-> 81\n",
      "sfc_poisson:  [33]\n",
      "-> 82\n",
      "sfc_poisson:  [51]\n",
      "-> 100\n",
      "sfc_poisson:  [62]\n",
      "-> 111\n",
      "arr_sfc_dist:  [49]\n"
     ]
    }
   ],
   "source": [
    "arr_sfc_dist = []\n",
    "class_arr_sfc_dist = arrival_sfc(num_sfc, arr_sfc_dist)\n",
    "arr_sfc_dist = list(class_arr_sfc_dist.sfc_poisson_dist())\n",
    "sfc_arrival_detail, sfc_arrival_num = [], []\n",
    "#arr_sfc_detail = class_arr_sfc_dist.sfc_details(per_episode, arr_sfc_dist, sfc_arrival_detail, sfc_arrival_num)\n",
    "\n",
    "#print(sfc_arrival_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class traffic_clustering():\n",
    "    def  __init__ (self, traffic_num_sfc_requested, epi_temp_sfc_arrived, deployed_sfc_size, sfc_deployment_time, \n",
    "                           arr_traffic_packet_per_sfc, traffic_transition_dict, traffic_batch_size, episode):\n",
    "        \n",
    "        self.traffic_num_sfc_requested = traffic_num_sfc_requested\n",
    "        self.epi_temp_sfc_arrived = epi_temp_sfc_arrived\n",
    "        self.deployed_sfc_size = deployed_sfc_size\n",
    "        self.sfc_deployment_time = sfc_deployment_time\n",
    "        self.arr_traffic_packet_per_sfc = arr_traffic_packet_per_sfc\n",
    "        self.traffic_transition_dict = traffic_transition_dict\n",
    "        self.traffic_batch_size = traffic_batch_size\n",
    "        self.episode = episode\n",
    "        #print(\"--:Traffic Tansistions:--\", self.traffic_transition_dict)\n",
    "\n",
    "    def input_traffic_transistion_storage(self, x_traffic_mem, traffic_mem_size, input_train_traffic_mem_cnt):\n",
    "        self.x_traffic_mem = x_traffic_mem \n",
    "        self.input_train_traffic_mem_cnt = input_train_traffic_mem_cnt\n",
    "        self.traffic_mem_size = traffic_mem_size\n",
    "        \n",
    "        for i in self.epi_temp_sfc_arrived: \n",
    "            #storing the dataset\n",
    "            #print(i)\n",
    "            index = self.input_train_traffic_mem_cnt % self.traffic_mem_size\n",
    "            #print(index)\n",
    "            self.x_traffic_mem[index] = self.traffic_transition_dict[i]\n",
    "            self.input_train_traffic_mem_cnt += 1\n",
    "            \n",
    "            if self.input_train_traffic_mem_cnt < self.deployed_sfc_size:\n",
    "                batch = np.random.choice(self.input_train_traffic_mem_cnt, self.input_train_traffic_mem_cnt, replace =False)\n",
    "                input_train_data = T.tensor(x_traffic_mem[batch], dtype=T.float)\n",
    "            else:\n",
    "                max_mem = min(self.input_train_traffic_mem_cnt, self.traffic_mem_size)\n",
    "                batch = np.random.choice(max_mem, self.deployed_sfc_size, replace =False) \n",
    "                input_train_data = T.tensor(x_traffic_mem[batch], dtype=T.float)\n",
    "                \n",
    "        #print(input_train_data.tolist())                \n",
    "        return self.input_train_traffic_mem_cnt, input_train_data\n",
    "        \n",
    "    \n",
    "    def kmean(self, n_clusters, traffic_input_dataset, kmeans_runtime):\n",
    "        kmean_start = time.perf_counter()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.traffic_input_dataset = traffic_input_dataset.tolist()\n",
    "        kmeans = KMeans(self.n_clusters)\n",
    "        kmeans.fit(self.traffic_input_dataset)\n",
    "        kmean_labels = kmeans.labels_\n",
    "        kmean_end = time.perf_counter()\n",
    "        kmean_temp_Rtime = kmean_end - kmean_start\n",
    "        kmeans_runtime.append(kmean_temp_Rtime)\n",
    "        return kmean_labels.tolist()\n",
    "    \n",
    "    def hierarchical(self, n_clusters, traffic_input_dataset, Aggo_runtime):\n",
    "        Aggo_start = time.perf_counter()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.traffic_input_dataset = traffic_input_dataset.tolist()\n",
    "        hierarchical_cluster = AgglomerativeClustering(n_clusters=self.n_clusters, affinity='euclidean', linkage='ward')\n",
    "        hierarchical_labels = hierarchical_cluster.fit_predict(self.traffic_input_dataset)\n",
    "        Aggo_end = time.perf_counter()\n",
    "        Aggo_temp_Rtime = Aggo_end - Aggo_start\n",
    "        Aggo_runtime.append(Aggo_temp_Rtime)\n",
    "        return hierarchical_labels.tolist()\n",
    "    \n",
    "    def brc(self, n_clusters, traffic_input_dataset, birch_runtime):\n",
    "        birch_start = time.perf_counter()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.traffic_input_dataset = traffic_input_dataset.tolist()\n",
    "        brc = Birch(n_clusters = self.n_clusters)\n",
    "        brc_labels = brc.fit_predict(self.traffic_input_dataset)\n",
    "        birch_end = time.perf_counter()\n",
    "        birch_temp_Rtime = birch_end - birch_start\n",
    "        birch_runtime.append(birch_temp_Rtime)\n",
    "        return brc_labels.tolist()\n",
    "    \n",
    "    #def spectral(self, n_clusters, traffic_input_dataset):\n",
    "    #    self.n_clusters = n_clusters\n",
    "    #    self.traffic_input_dataset = traffic_input_dataset.tolist()\n",
    "    #    spectral = SpectralClustering(n_clusters=self.n_clusters, eigen_solver = 'amg') #{arpack, lobpcg, amg})\n",
    "    #    spectral_labels = spectral.fit_predict(self.traffic_input_dataset)\n",
    "    #    return spectral_labels.tolist()\n",
    "    \n",
    "    def GM(self, n_clusters, traffic_input_dataset, GM_runtime):\n",
    "        GM_start = time.perf_counter()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.traffic_input_dataset = traffic_input_dataset.tolist()\n",
    "        GM = GaussianMixture(n_components=self.n_clusters)\n",
    "        GM.fit(self.traffic_input_dataset)\n",
    "        GM_labels = GM.predict(self.traffic_input_dataset)\n",
    "        GM_end = time.perf_counter()\n",
    "        GM_temp_Rtime = GM_end - GM_start\n",
    "        GM_runtime.append(GM_temp_Rtime)\n",
    "        return GM_labels.tolist()\n",
    "    \n",
    "    def clusteringplot(self, epi_clustering_status, traffic_input_dataset, cluster_label_val, epi_temp_sfc_arrived):\n",
    "        self.epi_clustering_status = epi_clustering_status #labels\n",
    "        self.traffic_input_dataset = traffic_input_dataset.tolist() #x and y axis\n",
    "        self.cluster_label_val = cluster_label_val\n",
    "        self.epi_temp_sfc_arrived = epi_temp_sfc_arrived\n",
    "        \n",
    "        clustering_x_axis_sfc, clustering_y_axis_priority, clustering_y_axis_packets, clustering_y_axis_load = [], [], [], []\n",
    "        \n",
    "        cnt_ = 0\n",
    "        for i in self.traffic_input_dataset:\n",
    "            clustering_x_axis_sfc.append(self.epi_temp_sfc_arrived[cnt_])\n",
    "            clustering_y_axis_priority.append(i[0]*100)\n",
    "            clustering_y_axis_packets.append(i[2])\n",
    "            clustering_y_axis_load.append(i[3]*10)\n",
    "            cnt_ += 1\n",
    "\n",
    "        for i in epi_clustering_status:\n",
    "            #print(i)\n",
    "            temp_HD_sfc = []\n",
    "            cluster_title = epi_clustering_status[i][0]\n",
    "            labels = epi_clustering_status[i][1]\n",
    "            #print(labels)\n",
    "            \n",
    "            for j in range(len(labels)):\n",
    "                if labels[j] == 1:\n",
    "                    temp_HD_sfc.append(clustering_x_axis_sfc[j])\n",
    "                    \n",
    "            self.cluster_label_val[i] = [cluster_title, temp_HD_sfc]\n",
    "    \n",
    "            fig, ax = plt.subplots(1)\n",
    "            ax.scatter(clustering_x_axis_sfc, clustering_y_axis_load, s= clustering_y_axis_packets,c=labels,alpha = 0.7)\n",
    "            ax.legend()\n",
    "            ax.set_xlabel(\"Services\")\n",
    "            ax.set_ylabel(\"Traffic Load\")\n",
    "            #ax.set_title(cluster_title)\n",
    "            \n",
    "            #ax[1].scatter(clustering_x_axis_sfc, clustering_y_axis_packets, s= clustering_y_axis_priority, c=labels)\n",
    "            #ax[1].set_xlabel(\"Services\")\n",
    "            #ax[1].set_ylabel(\"Traffic Load\")\n",
    "            #ax[1].set_title(cluster_title)\n",
    "            \n",
    "            #plt.show()\n",
    "            title = \"Clustering Experiment\"\n",
    "            dt = datetime.now()\n",
    "            # getting the timestamp\n",
    "            ts = datetime.timestamp(dt)\n",
    "            # convert to datetime\n",
    "            date_time = datetime.fromtimestamp(ts)\n",
    "            # convert timestamp to string in dd-mm-yyyy HH:MM:SS\n",
    "            str_date_time = date_time.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "            #print(str_date_time)\n",
    "            \n",
    "            figure_title = str(title)+\"_\"+str(cluster_title)+\"_\"+str(len(labels))+\"_\"+str(self.episode)+\"_\"+date_time.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "            ##saving the figure\n",
    "            fig.savefig('Generated_plots/cluster/epi_2000_100sfc_agglo_DT/Plot_'+str(figure_title)+'.png')\n",
    "            fig.savefig('Generated_plots/cluster/epi_2000_100sfc_agglo_DT/Plot_'+str(figure_title)+'.pdf', bbox_inches='tight')\n",
    "            \n",
    "            #saving clustering data\n",
    "            cluster_data = [\"Time: \"+str_date_time+ \"Title: \" + str(title) + \"Cluster model: \" + str(cluster_title) +\\\n",
    "                            \"clustering_x_axis_sfc: \" + str(clustering_x_axis_sfc) +\\\n",
    "                            \"clustering_y_axis_packets: \" + str(clustering_y_axis_packets) +\\\n",
    "                            \"clustering_y_axis_load: \" + str(clustering_y_axis_load) +\\\n",
    "                            \"clustering_y_axis_priority: \" + str(clustering_y_axis_priority) +\\\n",
    "                            \"labels: \" + str(labels) +\\\n",
    "                            \"cluster_label_val: \" + str(self.cluster_label_val)]\n",
    "            \n",
    "            fname = str(title)+\"_\"+str(cluster_title)+\"_\"+str(len(labels))+\"_\"+str(self.episode)+\"_\"+str_date_time\n",
    "            print(fname)\n",
    "            f = open('Generated_plots/cluster/epi_2000_100sfc_agglo_DT/'+fname,\"a\")\n",
    "            f.write(str(cluster_data))\n",
    "            f.close()\n",
    "            print(\"XXXXXXXXXXXXXXXXX\")\n",
    "            print(\"\")\n",
    "\n",
    "        \n",
    "        return self.cluster_label_val\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classifer\n",
    "class traffic_classifer():\n",
    "    def  __init__ (self, traffic_input_dataset, traffic_output_dataset, episode, cluster_testing):\n",
    "        \n",
    "        if traffic_input_dataset == []:\n",
    "            self.traffic_input_dataset  = traffic_input_dataset\n",
    "        else:\n",
    "            self.traffic_input_dataset  = traffic_input_dataset.tolist()\n",
    "        \n",
    "        self.traffic_output_dataset = traffic_output_dataset\n",
    "        self.episode = episode\n",
    "        self.cluster_testing = cluster_testing\n",
    "        #print(\"self.traffic_input_dataset:\", self.traffic_input_dataset)\n",
    "        #print(\"self.traffic_output_dataset: \", self.traffic_output_dataset)\n",
    "        #print(\"Traffic Classifier\")\n",
    "\n",
    "    def classifier_dataset(self, train_traffic_classifier_input_dataset, train_traffic_classifier_output_dataset,\n",
    "                                 test_traffic_classifier_input_dataset,  test_traffic_classifier_output_dataset):\n",
    "        birch_cluster\n",
    "        self.train_traffic_classifier_input_dataset  = train_traffic_classifier_input_dataset\n",
    "        self.train_traffic_classifier_output_dataset = train_traffic_classifier_output_dataset\n",
    "        self.test_traffic_classifier_input_dataset   = test_traffic_classifier_input_dataset\n",
    "        self.test_traffic_classifier_output_dataset  = test_traffic_classifier_output_dataset\n",
    "        #print(\"Classifier\")\n",
    "\n",
    "        if self.cluster_testing == True:\n",
    "            training_index = int((len(self.traffic_input_dataset)*60)/100)\n",
    "            #input datateset\n",
    "            for i in range(len(self.traffic_input_dataset)):\n",
    "                if i <= (training_index-1):\n",
    "                    self.train_traffic_classifier_input_dataset.append(self.traffic_input_dataset[i])\n",
    "                    self.train_traffic_classifier_output_dataset.append(self.traffic_output_dataset[i])\n",
    "                else:\n",
    "                    self.test_traffic_classifier_input_dataset.append(self.traffic_input_dataset[i])\n",
    "                    self.test_traffic_classifier_output_dataset.append(self.traffic_output_dataset[i])                 \n",
    "        else:\n",
    "            for i in range(len(self.traffic_input_dataset)):\n",
    "                self.train_traffic_classifier_input_dataset.append(self.traffic_input_dataset[i])\n",
    "                self.train_traffic_classifier_output_dataset.append(self.traffic_output_dataset[i])\n",
    "\n",
    "        return self.train_traffic_classifier_input_dataset, self.train_traffic_classifier_output_dataset, self.test_traffic_classifier_input_dataset, self.test_traffic_classifier_output_dataset\n",
    "        \n",
    "    def classifier_models(self, classifier_models, classifier_datasets, predict_dataset, classifier_training_status, Switch_classifier_info):\n",
    "        self.classifier_models = classifier_models\n",
    "        self.train_traffic_classifier_input_dataset =  classifier_datasets[0]\n",
    "        self.train_traffic_classifier_output_dataset = classifier_datasets[1]\n",
    "        self.test_traffic_classifier_input_dataset = classifier_datasets[2]\n",
    "        self.test_traffic_classifier_output_dataset = classifier_datasets[3]\n",
    "        self.predict_dataset = predict_dataset\n",
    "        self.classifier_training_status = classifier_training_status\n",
    "        #print(\"train input: classifier_datasets\", self.train_traffic_classifier_input_dataset)\n",
    "        #print(\"train output: classifier_datasets\", self.train_traffic_classifier_output_dataset)\n",
    "        #print(\"test input: classifier_datasets\", self.test_traffic_classifier_input_dataset)\n",
    "        #print(\"test output: classifier_datasets\", self.test_traffic_classifier_output_dataset)\n",
    "        \n",
    "        \n",
    "        if self.episode == 0 and self.classifier_training_status == True:\n",
    "            if Switch_classifier_info == 'Decision Trees': \n",
    "                self.classifier_models['Decision Trees'] = DecisionTreeClassifier()\n",
    "            elif Switch_classifier_info == 'Logistic Regression': \n",
    "                self.classifier_models['Logistic Regression'] = LogisticRegression()\n",
    "            elif Switch_classifier_info == 'Support Vector Machines': \n",
    "                self.classifier_models['Support Vector Machines'] = LinearSVC()\n",
    "            elif Switch_classifier_info == 'Random Forest':            \n",
    "                self.classifier_models['Random Forest'] = RandomForestClassifier()\n",
    "            elif Switch_classifier_info == 'Naive Bayes': \n",
    "                self.classifier_models['Naive Bayes'] = GaussianNB()\n",
    "            elif Switch_classifier_info == 'K-Nearest Neighbor': \n",
    "                self.classifier_models['K-Nearest Neighbor'] = KNeighborsClassifier()\n",
    "            else: \n",
    "                print(\"no classifer mdel found\")\n",
    "                \n",
    "        accuracy, precision, recall = {}, {}, {}\n",
    "        Runtime_classifier = {}\n",
    "        \n",
    "        for key in self.classifier_models.keys():\n",
    "            #print(\"key:\", key)\n",
    "            if self.classifier_training_status == True:\n",
    "                # Fit the classifier\n",
    "                R_classifier_start = time.perf_counter()\n",
    "                count_zero = self.train_traffic_classifier_output_dataset.count(0)\n",
    "                count_one  = self.train_traffic_classifier_output_dataset.count(1)\n",
    "                if count_zero == 0 or count_one == 0:\n",
    "                    print(\"no learning as all the output sets are of same class\")\n",
    "                    pass\n",
    "                else:\n",
    "                    self.classifier_models[key].fit(self.train_traffic_classifier_input_dataset, \n",
    "                                                    self.train_traffic_classifier_output_dataset)\n",
    "                    R_classifier_end   = time.perf_counter()\n",
    "                    temp_class_runtime = (R_classifier_end - R_classifier_start)\n",
    "                    Runtime_classifier[key] = temp_class_runtime\n",
    "                    #print(\"Runtime_classifier: \",Runtime_classifier)\n",
    "                    #print(\"self.classifier_models: \", self.classifier_models)\n",
    "\n",
    "                    # Make predictions\n",
    "                    #print(\"self.test_traffic_classifier_input_dataset: \",self.test_traffic_classifier_input_dataset)\n",
    "                    predictions = self.classifier_models[key].predict(self.test_traffic_classifier_input_dataset)\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    accuracy[key] = accuracy_score(predictions, self.test_traffic_classifier_output_dataset)\n",
    "                    precision[key] = precision_score(predictions, self.test_traffic_classifier_output_dataset)\n",
    "                    recall[key] = recall_score(predictions, self.test_traffic_classifier_output_dataset)\n",
    "                    #online_predictions = self.classifier_models[key].predict([self.predict_dataset]) \n",
    "                    #print(\"online_predictions: \", online_predictions)\n",
    "                \n",
    "            else:\n",
    "                #trail prediction\n",
    "                #print(\"Key: \", key, \"PREDICTION\")\n",
    "                predictions = self.classifier_models[key].predict([self.predict_dataset])\n",
    "                #predictions = self.classifier_models[key].predict([self.predict_dataset])  \n",
    "                #online_predictions = self.classifier_models[key].predict([self.predict_dataset])  \n",
    "                \n",
    "        return accuracy, precision, recall, Runtime_classifier, predictions\n",
    "    \n",
    "    \n",
    "    #def pred_classifier(self, predict_dataset, classifier_models):\n",
    "    #    self.predict_dataset = predict_dataset\n",
    "    #    self.classifier_models = classifier_models\n",
    "    #    \n",
    "    #    for key in self.classifier_models.keys():\n",
    "           # online_predictions = self.classifier_models[key].predict([self.predict_dataset])  \n",
    "    #    return online_predictions\n",
    "    \n",
    "    \n",
    "    \n",
    "    def classifier_plot(self, classifier_measurements,num_sfc, baseline):\n",
    "            #overall_epi_pr_emb\n",
    "            col4 = 'lightcyan' \n",
    "            col5 =  '#CDB5CD' \n",
    "            col6 =  '#548B54' #'#7CCD7C'#'#4EEE94'\n",
    "            col7 =  '#FFE7BA'\n",
    "            col8 =  '#8B3A62' #'hotpink4'\n",
    "            col9 =  '#C67171' #'lightcoral'#'salmon'#'tomato'#'#8F8F8F'#'powderblue'\n",
    "\n",
    "            \n",
    "            xaxis_clustering_models = []\n",
    "            self.classifier_measurements = classifier_measurements\n",
    "            log_acc, svm_acc, dt_acc, rf_acc, nb_acc, knn_acc = [],[],[],[],[],[]\n",
    "            log_pre, svm_pre, dt_pre, rf_pre, nb_pre, knn_pre = [],[],[],[],[],[]\n",
    "            log_recall, svm_recall, dt_recall, rf_recall, nb_recall, knn_recall = [],[],[],[],[],[]\n",
    "            log_runtime, svm_runtime, dt_runtime, rf_runtime, nb_runtime, knn_runtime = [],[],[],[],[],[]\n",
    "            print(\"baseline: \", baseline)\n",
    "            for key in self.classifier_measurements:\n",
    "                cluster_model = key\n",
    "                print(\"cluster_model: \", cluster_model)\n",
    "                xaxis_clustering_models.append(key)\n",
    "                models_acc = self.classifier_measurements[key][0] \n",
    "                models_precision = self.classifier_measurements[key][1] \n",
    "                models_recall = self.classifier_measurements[key][2] \n",
    "                models_runtime = self.classifier_measurements[key][3] \n",
    "                \n",
    "                log_acc.append(models_acc['Logistic Regression'])\n",
    "                svm_acc.append(models_acc['Support Vector Machines'])\n",
    "                dt_acc.append(models_acc['Decision Trees'])\n",
    "                rf_acc.append(models_acc['Random Forest']) \n",
    "                nb_acc.append(models_acc['Naive Bayes'])\n",
    "                knn_acc.append(models_acc['K-Nearest Neighbor'])\n",
    "                \n",
    "                log_pre.append(models_precision['Logistic Regression'])\n",
    "                svm_pre.append(models_precision['Support Vector Machines'])\n",
    "                dt_pre.append(models_precision['Decision Trees'])\n",
    "                rf_pre.append(models_precision['Random Forest']) \n",
    "                nb_pre.append(models_precision['Naive Bayes'])\n",
    "                knn_pre.append(models_precision['K-Nearest Neighbor'])\n",
    "                \n",
    "                log_recall.append(models_recall['Logistic Regression'])\n",
    "                svm_recall.append(models_recall['Support Vector Machines'])\n",
    "                dt_recall.append(models_recall['Decision Trees'])\n",
    "                rf_recall.append(models_recall['Random Forest']) \n",
    "                nb_recall.append(models_recall['Naive Bayes'])\n",
    "                knn_recall.append(models_recall['K-Nearest Neighbor'])\n",
    "                \n",
    "                log_runtime.append(models_runtime['Logistic Regression'])\n",
    "                svm_runtime.append(models_runtime['Support Vector Machines'])\n",
    "                dt_runtime.append(models_runtime['Decision Trees'])\n",
    "                rf_runtime.append(models_runtime['Random Forest']) \n",
    "                nb_runtime.append(models_runtime['Naive Bayes'])\n",
    "                knn_runtime.append(models_runtime['K-Nearest Neighbor'])\n",
    "\n",
    "            xaxis_classifier_models = []\n",
    "            for key in models_acc:\n",
    "                xaxis_classifier_models.append(key)\n",
    "            #print(\"xaxis_classifier_models\" , xaxis_classifier_models)\n",
    "            print(\"xaxis_clustering_models: \", xaxis_clustering_models)\n",
    "        \n",
    "            title  = 'Performace of Clustering and Classifier Models'\n",
    "            xlabel = 'Clustering Models'\n",
    "            ylabel = 'Accuracy'\n",
    "            #print(title)\n",
    "            \n",
    "            # set width of bar\n",
    "            barWidth = 0.15\n",
    "            fig1 = plt.figure(figsize=(15,4))\n",
    "            \n",
    "            # set height of bar\n",
    "            x_axis = xaxis_clustering_models #xaxis_classifier_models\n",
    "            y_axis_baseline = []\n",
    "            for i in range(len(x_axis)):\n",
    "                y_axis_baseline.append(baseline)\n",
    "            \n",
    "            \n",
    "            ## Set position of bar on X axis\n",
    "            br1 = np.arange(len(xaxis_clustering_models))\n",
    "            br2 = [x + barWidth for x in br1]\n",
    "            br3 = [x + barWidth for x in br2]\n",
    "            br4 = [x + barWidth for x in br3]\n",
    "            br5 = [x + barWidth for x in br4]\n",
    "            br6 = [x + barWidth for x in br5]\n",
    "            \n",
    "            # Make the plot\n",
    "            plt.plot(br1, y_axis_baseline, color = 'cyan')\n",
    "            plt.bar(br1, log_acc, color = col4, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='LR',  hatch='/////')\n",
    "            \n",
    "            plt.plot(br2, y_axis_baseline, color = 'cyan')\n",
    "            plt.bar(br2, svm_acc, color =col5, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='SVM', hatch='----')\n",
    "            \n",
    "            plt.plot(br3, y_axis_baseline, color = 'cyan')\n",
    "            plt.bar(br3, dt_acc,  color =col6, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='DT', hatch='xxxx')\n",
    "            \n",
    "            plt.plot(br4, y_axis_baseline, color = 'cyan')\n",
    "            plt.bar(br4, rf_acc, color =col7, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='RF', hatch='\\\\\\\\')\n",
    "            \n",
    "            plt.plot(br5, y_axis_baseline, color = 'cyan')\n",
    "            plt.bar(br5, nb_acc, color =col8, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E',label ='NB', hatch='....')\n",
    "            \n",
    "            plt.plot(br6, y_axis_baseline, color = 'cyan')\n",
    "            plt.bar(br6, knn_acc, color =col9, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='KNN', hatch='+++')\n",
    "            \n",
    "            # Adding Xticks\n",
    "            temp =  ['K-means', 'Agglomerative', 'BIRCH', 'GM']\n",
    "            plt.xlabel(xlabel)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.xticks([r + barWidth for r in range(len(temp))], ['K-means', 'Agglomerative', 'BIRCH', 'GM']) \n",
    "            #plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            ##saving the figure\n",
    "            dt = datetime.now()\n",
    "            # getting the timestamp\n",
    "            ts = datetime.timestamp(dt)\n",
    "            # convert to datetime\n",
    "            date_time = datetime.fromtimestamp(ts)\n",
    "            # convert timestamp to string in dd-mm-yyyy HH:MM:SS\n",
    "            str_date_time = date_time.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "            #print(str_date_time)\n",
    "            \n",
    "            figure_title = str(title)+str(ylabel)+\"_\"+str(cluster_num_sfc)+\"_\"+str(self.episode)+\"_\"+str(str_date_time)\n",
    "            fig1.savefig('Generated_plots/classifier/epi_2000_100sfc_agglo_DT/Plot_'+str(figure_title)+'.png')\n",
    "            fig1.savefig('Generated_plots/classifier/epi_2000_100sfc_agglo_DT/Plot_'+str(figure_title)+'.pdf', bbox_inches='tight')\n",
    "            \n",
    "            \n",
    "            fig2 = plt.figure(figsize=(15,4))\n",
    "            ylabel = 'Precision'\n",
    "            # set height of bar\n",
    "            x_axis = xaxis_clustering_models #xaxis_classifier_models\n",
    "            ## Set position of bar on X axis\n",
    "            br1 = np.arange(len(xaxis_clustering_models))\n",
    "            br2 = [x + barWidth for x in br1]\n",
    "            br3 = [x + barWidth for x in br2]\n",
    "            br4 = [x + barWidth for x in br3]\n",
    "            br5 = [x + barWidth for x in br4]\n",
    "            br6 = [x + barWidth for x in br5]\n",
    "            \n",
    "            # Make the plot\n",
    "            \n",
    "            plt.bar(br1, log_pre, color = col4, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='LR',  hatch='/////')\n",
    "            plt.bar(br2, svm_pre, color =col5, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='SVM', hatch='----')\n",
    "            plt.bar(br3, dt_pre,  color =col6, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='DT', hatch='xxxx')\n",
    "            plt.bar(br4, rf_pre, color =col7, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='RF', hatch='\\\\\\\\')\n",
    "            plt.bar(br5, nb_pre, color =col8, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E',label ='NB', hatch='....')\n",
    "            plt.bar(br6, knn_pre, color =col9, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='KNN', hatch='+++')\n",
    "            \n",
    "            # Adding Xticks\n",
    "            temp =  ['K-means', 'Agglomerative', 'BIRCH', 'GM']\n",
    "            plt.xlabel(xlabel)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.xticks([r + barWidth for r in range(len(temp))], ['K-means', 'Agglomerative', 'BIRCH', 'GM']) \n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "            \n",
    "            ##saving the figure\n",
    "            ##saving the figure\n",
    "            dt = datetime.now()\n",
    "            # getting the timestamp\n",
    "            ts = datetime.timestamp(dt)\n",
    "            # convert to datetime\n",
    "            date_time = datetime.fromtimestamp(ts)\n",
    "            # convert timestamp to string in dd-mm-yyyy HH:MM:SS\n",
    "            str_date_time = date_time.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "            #print(str_date_time)\n",
    "            \n",
    "            figure_title = str(title)+str(ylabel)+\"_\"+str(cluster_num_sfc)+\"_\"+str(self.episode)+\"_\"+str(str_date_time)\n",
    "            fig2.savefig('Generated_plots/classifier/epi_2000_100sfc_agglo_DT/Plot_'+str(figure_title)+'.png')\n",
    "            fig2.savefig('Generated_plots/classifier/epi_2000_100sfc_agglo_DT/Plot_'+str(figure_title)+'.pdf', bbox_inches='tight')\n",
    "            \n",
    "            \n",
    "            \n",
    "            fig3 = plt.figure(figsize=(15,4))\n",
    "            ylabel = 'Recall'\n",
    "            # set height of bar\n",
    "            x_axis = xaxis_clustering_models #xaxis_classifier_models\n",
    "            \n",
    "            ## Set position of bar on X axis\n",
    "            br1 = np.arange(len(xaxis_clustering_models))\n",
    "            br2 = [x + barWidth for x in br1]\n",
    "            br3 = [x + barWidth for x in br2]\n",
    "            br4 = [x + barWidth for x in br3]\n",
    "            br5 = [x + barWidth for x in br4]\n",
    "            br6 = [x + barWidth for x in br5]\n",
    "            \n",
    "            # Make the plot\n",
    "            plt.bar(br1, log_recall, color = col4, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='LR',  hatch='/////')\n",
    "            plt.bar(br2, svm_recall, color =col5, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='SVM', hatch='----')\n",
    "            plt.bar(br3, dt_recall,  color =col6, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='DT', hatch='xxxx')\n",
    "            plt.bar(br4, rf_recall, color =col7, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='RF', hatch='\\\\\\\\')\n",
    "            plt.bar(br5, nb_recall, color =col8, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E',label ='NB', hatch='....')\n",
    "            plt.bar(br6, knn_recall, color =col9, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='KNN', hatch='+++')\n",
    "            \n",
    "            # Adding Xticks\n",
    "            temp =  ['K-means', 'Agglomerative', 'BIRCH', 'GM']\n",
    "            plt.xlabel(xlabel)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.xticks([r + barWidth for r in range(len(temp))], ['K-means', 'Agglomerative', 'BIRCH', 'GM']) \n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "            \n",
    "            ##saving the figure\n",
    "            dt = datetime.now()\n",
    "            # getting the timestamp\n",
    "            ts = datetime.timestamp(dt)\n",
    "            # convert to datetime\n",
    "            date_time = datetime.fromtimestamp(ts)\n",
    "            # convert timestamp to string in dd-mm-yyyy HH:MM:SS\n",
    "            str_date_time = date_time.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "            #print(str_date_time)\n",
    "            \n",
    "            figure_title = str(title)+str(ylabel)+\"_\"+str(cluster_num_sfc)+\"_\"+str(self.episode)+\"_\"+str(str_date_time)\n",
    "            fig3.savefig('Generated_plots/classifier/epi_2000_100sfc_agglo_DT/Plot_'+str(figure_title)+'.png')\n",
    "            fig3.savefig('Generated_plots/classifier/epi_2000_100sfc_agglo_DT/Plot_'+str(figure_title)+'.pdf', bbox_inches='tight')\n",
    "\n",
    "            ##### Runtime\n",
    "            fig4 = plt.figure(figsize=(15,4))\n",
    "            ylabel = 'Runtime'\n",
    "            # set height of bar\n",
    "            x_axis = xaxis_clustering_models #xaxis_classifier_models\n",
    "            barWidth = 0.15\n",
    "\n",
    "            ## Set position of bar on X axis\n",
    "            br1 = np.arange(len(xaxis_clustering_models))\n",
    "            br2 = [x + barWidth for x in br1]\n",
    "            br3 = [x + barWidth for x in br2]\n",
    "            br4 = [x + barWidth for x in br3]\n",
    "            br5 = [x + barWidth for x in br4]\n",
    "            br6 = [x + barWidth for x in br5]\n",
    "            \n",
    "            # Make the plot\n",
    "            \n",
    "            plt.bar(br1, log_runtime, color = col4, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='LR',  hatch='/////')\n",
    "            plt.bar(br2, svm_runtime, color =col5, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='SVM', hatch='----')\n",
    "            plt.bar(br3, dt_runtime,  color =col6, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='DT', hatch='xxxx')\n",
    "            plt.bar(br4, rf_runtime, color =col7, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='RF', hatch='\\\\\\\\')\n",
    "            plt.bar(br5, nb_runtime, color =col8, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E',label ='NB', hatch='....')\n",
    "            plt.bar(br6, knn_runtime, color =col9, width = barWidth,\n",
    "                    edgecolor ='#2E2E2E', label ='KNN', hatch='+++')\n",
    "            \n",
    "            # Adding Xticks\n",
    "            temp =  ['K-means', 'Agglomerative', 'BIRCH', 'GM']\n",
    "            \n",
    "            plt.xlabel(xlabel)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.xticks([r + barWidth for r in range(len(temp))], ['K-means', 'Agglomerative', 'BIRCH', 'GM']) \n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "            \n",
    "            ##saving the figure\n",
    "            dt = datetime.now()\n",
    "            # getting the timestamp\n",
    "            ts = datetime.timestamp(dt)\n",
    "            # convert to datetime\n",
    "            date_time = datetime.fromtimestamp(ts)\n",
    "            # convert timestamp to string in dd-mm-yyyy HH:MM:SS\n",
    "            str_date_time = date_time.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "            #print(str_date_time)\n",
    "            \n",
    "            figure_title = str(title)+str(ylabel)+\"_\"+str(cluster_num_sfc)+\"_\"+str(self.episode)+\"_\"+str(str_date_time)\n",
    "            fig4.savefig('Generated_plots/classifier/epi_0_100_100sfc/Plot_'+str(figure_title)+'.png')\n",
    "            fig4.savefig('Generated_plots/classifier/epi_0_100_100sfc/Plot_'+str(figure_title)+'.pdf', bbox_inches='tight')\n",
    "            \n",
    "            \n",
    "            #saving clustering data\n",
    "            cluster_data = [\"Title: \" + str(title) +\\\n",
    "                            \"Classifier_measurements \" + str(self.classifier_measurements) +\\\n",
    "                            \"xaxis_clustering_models: \" + str(xaxis_clustering_models) +\\\n",
    "                            \"xaxis_classifier_models: \" + str(xaxis_classifier_models) +\\\n",
    "                            \"num_sfc: \" + str(cluster_num_sfc)+\\\n",
    "                           \"baseline accuracy:\"+str(baseline)]\n",
    "            \n",
    "            fname = str(title)+\"_\"+str(cluster_num_sfc)+\"_\"+str(self.episode)+\"_\"+str(str_date_time)\n",
    "            print(fname)\n",
    "            f = open('Generated_plots/classifier/epi_2000_100sfc_agglo_DT/'+fname,\"a\")\n",
    "            f.write(str(cluster_data))\n",
    "            f.close()\n",
    "            print(\"XXXXXXXXXXXXXXXXX\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Priority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "arr_core_cpu = [12] #[2, 4, 8, 12]\n",
    "arr_num_vnf_per_core = [4] #, 32, 64] #\n",
    "\n",
    "# Nodal Outage\n",
    "arr_nodal_outage_percentage = [0] #[0, 10, 20, 30, 40, 50]\n",
    "\n",
    "tot_avail_res_network = (arr_core_cpu[0]*arr_num_vnf_per_core[0])*7\n",
    "print(tot_avail_res_network)\n",
    "avail_res_per_node = (arr_core_cpu[0]*arr_num_vnf_per_core[0])\n",
    "print(avail_res_per_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch \n",
    "switch_priorty = True  #False #\n",
    "switch_reconst = True # False # \n",
    "switch_decomp  = True #False # \n",
    "pr_model = 'ridge' #'MPL' #'MPL'#'DDPG' #'DDQN'\n",
    "ar_sch_model = ['DDPG'] #['DDPG', 'WFQ', 'Pr_val_based', 'FIFO'] #'WFQ'#  'Pr_val_based' #'FIFO' #\n",
    "switch_scheduling = True #False #\n",
    "switch_sfc_expiring_check = False #True #False\n",
    "switch_clustering = True\n",
    "Switch_cluster_info = ['brich']#['kmean', 'Aggo', 'brich', 'GM']\n",
    "Switch_classifier_info = 'Support Vector Machines' \n",
    "## 'Decision Trees', 'Logistic Regression', 'Support Vector Machines', 'Random Forest', 'Naive Bayes', 'K-Nearest Neighbor'\n",
    "        \n",
    "cluster_testing = True #False\n",
    "stop_classifier_training = 1\n",
    "classifier_training_status = True\n",
    "classifier_prediction_status = True\n",
    "length = 1\n",
    "min_samples = 10\n",
    "classifier_models ={}\n",
    "classifier_training_interval = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a different environment\n",
    "cluster_num_episode, cluster_num_sfc = 1, 50\n",
    "\n",
    "cluster_global_services =Trail(num_episode= cluster_num_episode, \n",
    "                                  num_sfc = cluster_num_sfc, \n",
    "                                  max_vnf_resource = max_vnf_resource, \n",
    "                                  vnf_per_service = vnf_per_service,\n",
    "                                  vNetwork_service_er =[], \n",
    "                                  vNetwork_service_details_er=[],\n",
    "                                  vnf_link_resources = [], \n",
    "                                  vnf_node_resources=[], \n",
    "                                  mean = mean, \n",
    "                                  var = var\n",
    "                                 )\n",
    "\n",
    "#print(cluster_global_services.vNetwork_service_details_er)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switch_clustering:  True\n"
     ]
    }
   ],
   "source": [
    "if switch_clustering is True:\n",
    "    print(\"switch_clustering: \", switch_clustering)\n",
    "    n_traffic_transition = [4]\n",
    "    tr_n_state = [3]\n",
    "    tr_n_actions = 1\n",
    "\n",
    "    traffic_mem_size = 500000\n",
    "    x_traffic_mem = np.zeros((traffic_mem_size, *n_traffic_transition), dtype = np.float32)\n",
    "    y_traffic_mem = np.zeros((traffic_mem_size), dtype = np.float32)\n",
    "\n",
    "    hidden_neuron = 20\n",
    "    tr_input_train_data, tr_target_train_data = [], [] \n",
    "    tr_input_test_data,  tr_target_test_data  = [], []\n",
    "    tr_MLP_model = MLP(tr_n_state = tr_n_state, \n",
    "                       tr_n_actions = tr_n_actions, \n",
    "                       hidden_neuron = hidden_neuron)\n",
    "\n",
    "    tr_input_train_mem     = np.zeros((mem_size, *tr_n_state), dtype = np.float)\n",
    "    tr_target_train_mem    = np.zeros(mem_size, dtype = np.float)\n",
    "    tr_input_test_mem      = np.zeros((mem_size, *tr_n_state), dtype = np.float)\n",
    "    tr_target_test_mem     = np.zeros(mem_size, dtype = np.float)\n",
    "    tr_done_status = \"Train\"\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFC:  0\n",
      "SFC:  1\n",
      "SFC:  2\n",
      "SFC:  3\n",
      "SFC:  4\n",
      "SFC:  5\n",
      "SFC:  6\n",
      "SFC:  7\n",
      "SFC:  8\n",
      "SFC:  9\n",
      "SFC:  10\n",
      "SFC:  11\n",
      "SFC:  12\n",
      "SFC:  13\n",
      "SFC:  14\n",
      "SFC:  15\n",
      "SFC:  16\n",
      "SFC:  17\n",
      "SFC:  18\n",
      "SFC:  19\n",
      "SFC:  20\n",
      "SFC:  21\n",
      "SFC:  22\n",
      "SFC:  23\n",
      "SFC:  24\n",
      "SFC:  25\n",
      "SFC:  26\n",
      "SFC:  27\n",
      "SFC:  28\n",
      "SFC:  29\n",
      "SFC:  30\n",
      "SFC:  31\n",
      "SFC:  32\n",
      "SFC:  33\n",
      "SFC:  34\n",
      "SFC:  35\n",
      "SFC:  36\n",
      "SFC:  37\n",
      "SFC:  38\n",
      "SFC:  39\n",
      "SFC:  40\n",
      "SFC:  41\n",
      "SFC:  42\n",
      "SFC:  43\n",
      "SFC:  44\n",
      "SFC:  45\n",
      "SFC:  46\n",
      "SFC:  47\n",
      "SFC:  48\n",
      "SFC:  49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = substrate_network()\n",
    "\n",
    "if switch_clustering is True:\n",
    "    #print(\"switch_clustering: \", switch_clustering)\n",
    "    n_traffic_transition = [4]\n",
    "    traffic_mem_size = 500000\n",
    "    x_traffic_mem = np.zeros((traffic_mem_size, *n_traffic_transition), dtype = np.float32)\n",
    "    y_traffic_mem = np.zeros((traffic_mem_size), dtype = np.float32)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "a = 0\n",
    "input_train_traffic_mem_cnt, test_traffic_mem_cnt = 0, 0\n",
    "for per_episode in cluster_global_services.vNetwork_service_details_er: \n",
    "    episode = a\n",
    "    #print(\"\")\n",
    "    #print(\"Episode: \", episode)\n",
    "    traffic_input_dataset = []\n",
    "    epi_classification_grp_acc = {}\n",
    "    traffic_sfc_arrival = []\n",
    "    traffic_transition_dict, cluster_label_val = {}, {}\n",
    "    arr_traffic_packet_per_epi = []\n",
    "    epi_temp_sfc_arrived = []\n",
    "    \n",
    "    kmeans_runtime, Aggo_runtime, birch_runtime, GM_runtime = [],[],[],[]\n",
    "    \n",
    "    if switch_clustering is True:\n",
    "        #traffic\n",
    "        epi_traffic_packets_dict, epi_traffic_load_dict = {}, {}\n",
    "        for i in range(cluster_num_sfc):\n",
    "            epi_traffic_packets_dict[i] = 0\n",
    "            epi_traffic_load_dict[i] = 0\n",
    "\n",
    "        #clustering\n",
    "        epi_clustering_status = {}\n",
    "        for i in range(len(Switch_cluster_info)):\n",
    "            #print(i, Switch_cluster_info[i])\n",
    "            epi_clustering_status[i] = [str(Switch_cluster_info[i]), None]\n",
    "            \n",
    "    \n",
    "    removed_nodes = [0]\n",
    "    core_cpu = 12\n",
    "    num_vnf_per_core = 4\n",
    "    tot_vnf_per_core = core_cpu*num_vnf_per_core\n",
    "    #----Resource Initialization for the network Network----#\n",
    "    env.substrate_node_init(removed_nodes, tot_vnf_per_core)\n",
    "    env.substrate_link_init()\n",
    "    if switch_priorty is True and pr_model == 'DDPG':\n",
    "        pr_agent.noise.reset()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #---- End of Initialization of Resource ----#\n",
    "    cnt_sfc= 0\n",
    "    classifier_measurements= {}\n",
    "    for per_sfc, per_link in per_episode:\n",
    "        #print(\"\")\n",
    "        sfc = per_sfc[0][1]['sfc']\n",
    "        min_samples = min_samples\n",
    "        sfc_priority_level = per_sfc[0][1]['env_priority']\n",
    "        sfc_int_flow = per_sfc[0][1]['int_flow_behaviour']\n",
    "        arr_traffic_packet_per_sfc = []\n",
    "        dict_traffic_packet_per_sfc ={}\n",
    "        \n",
    "        traffic_N =0 # None\n",
    "        traffic_load = 0 # None\n",
    "        temp_s = 0\n",
    "        temp_pr_ft = [sfc_priority_level, sfc_int_flow, traffic_N, traffic_load]\n",
    "        traffic_transition_dict[sfc] = temp_pr_ft\n",
    "        #print(\"traffic_transition_dict: \", traffic_transition_dict, len(traffic_transition_dict))\n",
    "        \n",
    "        # Traffic : update 11jan2023\n",
    "        test_list = np.arange(len(traffic_transition_dict))\n",
    "        if sfc >= classifier_training_interval:\n",
    "            k = rand.choice(test_list)\n",
    "            while k <= classifier_training_interval-5:\n",
    "                k = rand.choice(test_list)\n",
    "        else: \n",
    "            k = rand.choice(test_list)\n",
    "        arrving_sfc = rand.choices(test_list, k=k)\n",
    "        #print(\"arrving_sfc: \", arrving_sfc)\n",
    "        #epi_temp_sfc_arrived.append(arrving_sfc)\n",
    "        epi_temp_sfc_arrived = arrving_sfc\n",
    "       # print(\"epi_temp_sfc_arrived:\", epi_temp_sfc_arrived)\n",
    "       # \n",
    "        if switch_clustering is True:\n",
    "            print(\"SFC: \", sfc)\n",
    "            traffic_output_dataset = []\n",
    "            \n",
    "            #print(epi_temp_sfc_arrived)\n",
    "            deployed_sfc_size  = len(epi_temp_sfc_arrived)\n",
    "           # print(\"epi_temp_sfc_arrived : \", epi_temp_sfc_arrived)\n",
    "            #print(\"deployed_sfc_size: \", deployed_sfc_size)\n",
    "            sfc_deployment_time =1\n",
    "\n",
    "            if deployed_sfc_size <= 1:\n",
    "                #print(\"deployment is 0\")\n",
    "                pass\n",
    "            else: \n",
    "                mu, std, size = 100, 300, deployed_sfc_size\n",
    "                traffic_num_sfc_requested = abs(random.normal(mu, std, size))\n",
    "                #print(\"===>\", traffic_num_sfc_requested)\n",
    "\n",
    "                traffic_cnt = 0\n",
    "                for i in traffic_num_sfc_requested:\n",
    "                    #print(i)\n",
    "                    length = sfc_deployment_time\n",
    "                    hurst= 0.7\n",
    "                    num = abs(int(i))\n",
    "                    if  num == 0:\n",
    "                        traffic_packet_per_sfc = 0 #load\n",
    "                        arr_traffic_packet_per_sfc.append(traffic_packet_per_sfc)\n",
    "                        pass\n",
    "                    else: \n",
    "                        f = FBM(n  = int(num), hurst=hurst, length=length, method='daviesharte')\n",
    "                        fbm_sample = abs(f.fbm())\n",
    "                        fgn_sample = f.fgn()\n",
    "                        t_values   = f.times() #stationary traffic\n",
    "                        traffic_packet_per_sfc = sum(t_values) #load\n",
    "                        arr_traffic_packet_per_sfc.append(traffic_packet_per_sfc)\n",
    "\n",
    "                    \n",
    "                    deployed_sfc = epi_temp_sfc_arrived[traffic_cnt]\n",
    "                    traffic_transition_dict[epi_temp_sfc_arrived[traffic_cnt]][2] = traffic_num_sfc_requested[traffic_cnt]\n",
    "                    traffic_transition_dict[epi_temp_sfc_arrived[traffic_cnt]][3] = arr_traffic_packet_per_sfc[traffic_cnt]\n",
    "                    epi_traffic_packets_dict[epi_temp_sfc_arrived[traffic_cnt]] = epi_traffic_packets_dict[epi_temp_sfc_arrived[traffic_cnt]] +  traffic_num_sfc_requested[traffic_cnt]\n",
    "                    epi_traffic_load_dict[epi_temp_sfc_arrived[traffic_cnt]] = epi_traffic_load_dict[epi_temp_sfc_arrived[traffic_cnt]] + arr_traffic_packet_per_sfc[traffic_cnt]\n",
    "                    \n",
    "                    #print(\"epi_traffic_packets_dict:\", epi_traffic_packets_dict)\n",
    "                    #print(\"epi_traffic_packets_dict[epi_temp_sfc_arrived[traffic_cnt]]:\", epi_traffic_packets_dict[epi_temp_sfc_arrived[traffic_cnt]])\n",
    "                    \n",
    "                    #print(\"deployed_sfc\", deployed_sfc, traffic_num_sfc_requested[traffic_cnt], arr_traffic_packet_per_sfc[traffic_cnt])\n",
    "                    \n",
    "                    #updating in global level\n",
    "                    cluster_global_services.traffic_update(episode, deployed_sfc, vnf_per_service, \n",
    "                                                   epi_traffic_packets_dict, epi_traffic_load_dict)\n",
    "                    traffic_cnt += 1\n",
    "                    \n",
    "                    \n",
    "                    arr_traffic_packet_per_epi.append(arr_traffic_packet_per_sfc) #traffic_clustering\n",
    "                    \n",
    "                    \n",
    "                    class_traffic_clustering = traffic_clustering(traffic_num_sfc_requested,\n",
    "                                                                  epi_temp_sfc_arrived, \n",
    "                                                                  deployed_sfc_size,\n",
    "                                                                  sfc_deployment_time, \n",
    "                                                                  arr_traffic_packet_per_sfc,\n",
    "                                                                  traffic_transition_dict,\n",
    "                                                                  traffic_batch_size,\n",
    "                                                                  episode)\n",
    "                    input_traffic_storage = class_traffic_clustering.input_traffic_transistion_storage(x_traffic_mem, \n",
    "                                                                                           traffic_mem_size, \n",
    "                                                                                           input_train_traffic_mem_cnt)\n",
    "\n",
    "                    input_train_traffic_mem_cnt = input_traffic_storage[0]\n",
    "                    traffic_input_dataset = input_traffic_storage[1]\n",
    "                    #print(traffic_input_dataset)\n",
    "                    #kmeans_runtime, Aggo_runtime, birch_runtime, GM_runtime\n",
    "                    #print(\"input_train_traffic_mem_cnt: \", input_train_traffic_mem_cnt)\n",
    "                    \n",
    "                    if input_train_traffic_mem_cnt > 10:\n",
    "                        #pass\n",
    "                        #kmean_cluster = class_traffic_clustering.kmean(n_clusters, traffic_input_dataset, kmeans_runtime)\n",
    "                        #print(\"kmean_cluster: \", kmean_cluster)\n",
    "                        #epi_clustering_status[0][1] = kmean_cluster\n",
    "\n",
    "                        #hierarchical_cluster = class_traffic_clustering.hierarchical(n_clusters, traffic_input_dataset, \n",
    "                        #                                                             Aggo_runtime)\n",
    "                        #print(\"hierarchical_cluster: \", hierarchical_cluster)\n",
    "                        \n",
    "                        \n",
    "                        birch_cluster = class_traffic_clustering.brc(n_clusters, traffic_input_dataset, birch_runtime)\n",
    "                        epi_clustering_status[0][1] = birch_cluster\n",
    "\n",
    "                        #birch_cluster = class_traffic_clustering.brc(n_clusters, traffic_input_dataset, birch_runtime)\n",
    "                        #print(\"birch_cluster: \", birch_cluster)\n",
    "                        #epi_clustering_status[2][1] = birch_cluster\n",
    "\n",
    "                        #spectral_cluster = class_traffic_clustering.spectral(n_clusters, traffic_input_dataset)\n",
    "                        ##print(\"spectral_cluster: \", spectral_cluster)\n",
    "                        #epi_clustering_status[3][1] = spectral_cluster \n",
    "\n",
    "                        #GM_cluster = class_traffic_clustering.GM(n_clusters, traffic_input_dataset, GM_runtime)\n",
    "                        #print(\"GM_cluster: \", GM_cluster)\n",
    "                        #epi_clustering_status[3][1] = GM_cluster\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "            #print(\"epi_clustering_status: \", epi_clustering_status)    \n",
    "            if sfc > classifier_training_interval and sfc%classifier_training_interval == 0:#sfc > classifier_training_interval: # input_train_traffic_mem_cnt > 10:\n",
    "                for key, val in epi_clustering_status.items():\n",
    "                    #print(\"key: \", val[0])\n",
    "                    clusterModel = val[0]\n",
    "                    train_traffic_classifier_input_dataset, train_traffic_classifier_output_dataset = [], []\n",
    "                    test_traffic_classifier_input_dataset, test_traffic_classifier_output_dataset = [], []\n",
    "                    traffic_output_dataset = val[1]\n",
    "                    \n",
    "                    class_traffic_classifer = traffic_classifer(traffic_input_dataset, \n",
    "                                                            traffic_output_dataset, \n",
    "                                                            episode, cluster_testing)\n",
    "                    \n",
    "                    \n",
    "                    classifier_datasets = class_traffic_classifer.classifier_dataset(train_traffic_classifier_input_dataset, \n",
    "                                                                                 train_traffic_classifier_output_dataset,\n",
    "                                                                                 test_traffic_classifier_input_dataset,  \n",
    "                                                                                 test_traffic_classifier_output_dataset)\n",
    "                    \n",
    "                    temp_val1 = classifier_datasets[1].count(1)/len(classifier_datasets[1])\n",
    "                    temp_val2 = classifier_datasets[1].count(0)/len(classifier_datasets[1])\n",
    "                    baseline = (temp_val1* temp_val1) + (temp_val2*temp_val2)\n",
    "                    \n",
    "                    predict_dataset = [] #[sfc, sfc_priority_level, sfc_int_flow, pred_num, pred_traffic_packet_per_sfc] #temp_pr_ft #\n",
    "                    #print(\"classifier_datasets: \", classifier_datasets)\n",
    "                    #print(\"0 classifier_datasets: \", classifier_datasets[0])\n",
    "                    #print(\"1 classifier_datasets: \", classifier_datasets[1])\n",
    "                    #print(\"2 classifier_datasets: \", classifier_datasets[2])\n",
    "                    #print(\"3 classifier_datasets: \", classifier_datasets[3])\n",
    "                    Classifier_training = class_traffic_classifer.classifier_models(classifier_models, \n",
    "                                                                                    classifier_datasets,\n",
    "                                                                                    predict_dataset,\n",
    "                                                                                    classifier_training_status,\n",
    "                                                                                    Switch_classifier_info)\n",
    "\n",
    "                    #print(\"Classifier_training: \", Classifier_training)\n",
    "                    classifier_measurements[clusterModel] = Classifier_training\n",
    "                    #print(\"classifier_measurements: \", classifier_measurements)\n",
    "                    \n",
    "                    \n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "        cnt_sfc += 1\n",
    "    #print(\"epi_clustering_status: \", epi_clustering_status)\n",
    "    #print(\"len traffic_input_dataset: \", len(traffic_input_dataset))\n",
    "    if a%cluster_num_episode==0:\n",
    "        pass\n",
    "        '''\n",
    "        clusterplot_val = class_traffic_clustering.clusteringplot(epi_clustering_status, \n",
    "                                                                  traffic_input_dataset, \n",
    "                                                                  cluster_label_val,\n",
    "                                                                  epi_temp_sfc_arrived)\n",
    "        \n",
    "        #print(\"classifier_measurements: \", classifier_measurements)\n",
    "    \n",
    "        classifers_plots = class_traffic_classifer.classifier_plot(classifier_measurements, cluster_num_sfc, baseline)\n",
    "\n",
    "        title = 'Runtime for cluster model'\n",
    "        #kmeans_runtime, Aggo_runtime, birch_runtime, GM_runtime\n",
    "        Y_axis_Rcluster = [sum(kmeans_runtime), sum(Aggo_runtime), sum(birch_runtime), sum(GM_runtime)]\n",
    "        #print(\"X_axis_Rcluster: \", X_axis_Rcluster)\n",
    "        X_axis_Rcluster = ['K-means', 'Agglomerative', 'BIRCH', 'GM']\n",
    "        fig = plt.figure()\n",
    "        plt.barh(X_axis_Rcluster, Y_axis_Rcluster, color = '#27408B', edgecolor ='#2E2E2E',hatch = '///') \n",
    "        plt.ylabel(\"Cluster Modelling\")\n",
    "        plt.xlabel(\"Runtime (ns)\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        ##saving the figure\n",
    "        dt = datetime.now()\n",
    "        # getting the timestamp\n",
    "        ts = datetime.timestamp(dt)\n",
    "        # convert to datetime\n",
    "        date_time = datetime.fromtimestamp(ts)\n",
    "        # convert timestamp to string in dd-mm-yyyy HH:MM:SS\n",
    "        str_date_time = date_time.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "        #print(str_date_time)\n",
    "        \n",
    "        figure_title = str(title)+\"_\"+str(cluster_num_sfc)+\"_\"+str(episode)+str(str_date_time)\n",
    "        fig.savefig('Generated_plots/cluster/epi_2000_100sfc_agglo_DT/Plot_'+str(figure_title)+'.png')\n",
    "        fig.savefig('Generated_plots/cluster/epi_2000_100sfc_agglo_DT/Plot_'+str(figure_title)+'.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "        #saving clustering data\n",
    "        cluster_data = [\"Title: \" + str(figure_title ) +\\\n",
    "                        \"X_axis_Rcluster: \" + str(X_axis_Rcluster) +\\\n",
    "                        \"Y_axis_Rcluster: \" + str(Y_axis_Rcluster)]\n",
    "\n",
    "        fname = figure_title\n",
    "        print(fname)\n",
    "        f = open('Generated_plots/cluster/epi_2000_100sfc_agglo_DT/'+fname,\"a\")\n",
    "        f.write(str(cluster_data))\n",
    "        f.close()\n",
    "        print(\"XXXXXXXXXXXXXXXXX\") \n",
    "        '''\n",
    "    \n",
    "    a += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "##print((\"epi_clustering_status: \", epi_clustering_status))\n",
    "##print(traffic_input_dataset, len(traffic_input_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alggo + DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial_DDQN_100__Netrail__cpucore_12_num_vnf_per_core_4_saving_services\n",
      "XXXXXXXXXXXXXXXXX\n",
      "\n",
      "Start Time: 1701024600.6999483\n",
      "activate agent DDQN\n",
      "DDQN Linear agent\n",
      "ridge\n",
      "DDPG agent\n",
      "\n",
      "Episode:  0\n",
      "sfc_poisson:  [51]\n",
      "-> 51\n",
      "sfc_poisson:  [55]\n",
      "-> 55\n",
      "sfc_poisson:  [47]\n",
      "-> 47\n",
      "sfc_poisson:  [53]\n",
      "-> 100\n",
      "sfc_poisson:  [58]\n",
      "-> 105\n",
      "sfc_poisson:  [63]\n",
      "-> 110\n",
      "sfc_poisson:  [53]\n",
      "-> 100\n",
      "sfc_poisson:  [53]\n",
      "-> 100\n",
      "sfc_poisson:  [57]\n",
      "-> 104\n",
      "sfc_poisson:  [48]\n",
      "-> 95\n",
      "sfc_poisson:  [55]\n",
      "-> 102\n",
      "sfc_poisson:  [56]\n",
      "-> 103\n",
      "sfc_poisson:  [57]\n",
      "-> 104\n",
      "sfc_poisson:  [58]\n",
      "-> 105\n",
      "sfc_poisson:  [58]\n",
      "-> 105\n",
      "sfc_poisson:  [53]\n",
      "-> 100\n",
      "sfc_poisson:  [48]\n",
      "-> 95\n",
      "sfc_poisson:  [53]\n",
      "-> 100\n",
      "sfc_poisson:  [61]\n",
      "-> 108\n",
      "sfc_poisson:  [44]\n",
      "-> 91\n",
      "sfc_poisson:  [38]\n",
      "-> 85\n",
      "sfc_poisson:  [48]\n",
      "-> 95\n",
      "sfc_poisson:  [42]\n",
      "-> 89\n",
      "sfc_poisson:  [31]\n",
      "-> 78\n",
      "sfc_poisson:  [36]\n",
      "-> 83\n",
      "sfc_poisson:  [54]\n",
      "-> 101\n",
      "sfc_poisson:  [44]\n",
      "-> 91\n",
      "sfc_poisson:  [58]\n",
      "-> 105\n",
      "sfc_poisson:  [36]\n",
      "-> 83\n",
      "sfc_poisson:  [59]\n",
      "-> 106\n",
      "sfc_poisson:  [48]\n",
      "-> 95\n",
      "sfc_poisson:  [45]\n",
      "-> 92\n",
      "sfc_poisson:  [47]\n",
      "-> 94\n",
      "sfc_poisson:  [53]\n",
      "-> 100\n",
      "sfc_poisson:  [55]\n",
      "-> 102\n",
      "sfc_poisson:  [52]\n",
      "-> 99\n",
      "sfc_poisson:  [47]\n",
      "-> 94\n",
      "sfc_poisson:  [52]\n",
      "-> 99\n",
      "sfc_poisson:  [36]\n",
      "-> 83\n",
      "sfc_poisson:  [52]\n",
      "-> 99\n",
      "sfc_poisson:  [53]\n",
      "-> 100\n",
      "sfc_poisson:  [58]\n",
      "-> 105\n",
      "sfc_poisson:  [35]\n",
      "-> 82\n",
      "sfc_poisson:  [48]\n",
      "-> 95\n",
      "sfc_poisson:  [57]\n",
      "-> 104\n",
      "sfc_poisson:  [53]\n",
      "-> 100\n",
      "sfc_poisson:  [62]\n",
      "-> 109\n",
      "sfc_poisson:  [52]\n",
      "-> 99\n",
      "sfc_poisson:  [48]\n",
      "-> 95\n",
      "sfc_poisson:  [36]\n",
      "-> 83\n",
      "arr_sfc_dist:  [47]\n",
      "sfc_arrival_num:  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]]\n",
      "SFC:  0\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  1\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  2\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  3\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  4\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  5\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  6\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  7\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  8\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  9\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  10\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  11\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  12\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  13\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  14\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  15\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  16\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  17\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  18\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  19\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  20\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  21\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  22\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  23\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  24\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  25\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  26\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  27\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  28\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  29\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  30\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  31\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  32\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  33\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  34\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  35\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  36\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  37\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  38\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  39\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  40\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  41\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  42\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  43\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  44\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  45\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "SFC:  46\n",
      "pr_model:  ridge\n",
      "sch_model:  DDPG\n",
      "*************\n",
      "cnt_sfc_arrival:  47\n",
      "Total Services for Episode 0 is 47 Num accepted 18 Num rejected 29\n",
      "runtime 5.3\n",
      "Trial_DDQN_100_Netrail_cpucore_12_num_vnf_per_core_4_mean_18_sfc_50_epi_1_runtime_5.3_clustermodel_['brich']_classifier_Support Vector Machines_switch_sfc_expiring_check_False26-11-2023-18-50-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cluster_mod, classifier_mod  = \n",
    "#'brich', 'Support Vector Machines'\n",
    "\n",
    "classifier_training_status = False\n",
    "classifier_prediction_status = True\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    sch_model_overall_cnt_sfc_starved_before_serving, sch_model_overall_cnt_sfc_Prclass_starved_before_serving = {},{}\n",
    "    overall_sfc_arrival_detail,  overall_sfc_arrival_num = [],[]\n",
    "    overall_sfc_priority, copy_overall_epi_pr, overall_episode_envpriority  = [], [],[]\n",
    "    copy_overall_episode_envpriority = []\n",
    "    #sclassifier_models ={}\n",
    "    \n",
    "    #saving the arrival services\n",
    "    saving_services = global_services.vNetwork_service_details_er\n",
    "    #print(\"saving_services: \", saving_services)\n",
    "    f_name = str(trial)+'_'+ str('_Netrail_')+\\\n",
    "            '_cpucore_' + str(arr_core_cpu[0])+'_num_vnf_per_core_'+\\\n",
    "            str(arr_num_vnf_per_core[0]) + str('_saving_services')\n",
    "    print(f_name)\n",
    "    #f = open('Data_storage/epi_2000_100sfc_agglo_DT/'+f_name,\"a\")\n",
    "    #f.write(str(saving_services))\n",
    "    #f.close()\n",
    "    print(\"XXXXXXXXXXXXXXXXX\")\n",
    "        \n",
    "    status_cnt_ridge = False\n",
    "    for sch_model in ar_sch_model:\n",
    "        #print(\"______________________\")\n",
    "        #print(\"sch_model: \", sch_model, status_cnt_ridge)\n",
    "        sch_model_overall_cnt_sfc_starved_before_serving[sch_model] = None\n",
    "        sch_model_overall_cnt_sfc_Prclass_starved_before_serving [sch_model] = None\n",
    "        env = substrate_network()\n",
    "        reward_func = Reward_Function()\n",
    "        temp_vals_sorted_action, temp_norm_vals_sorted_action = [], []\n",
    "        min_pr,max_pr = 0, 6\n",
    "        t_max, t_min, old_t_current = None, 0, 0\n",
    "        mvf_t_max, mvf_t_min, mvf_old_t_current = None, 0, 0\n",
    "        switch_or_run_once = False\n",
    "        overall_emb_sfc, overall_emb_sfc_h_rel = [], []\n",
    "        overall_unemb_sfc = []\n",
    "        overall_ar_sorted_sch_list =[]\n",
    "\n",
    "        # NODAL OUTAGE\n",
    "        for nodal_outage_percentage in arr_nodal_outage_percentage:\n",
    "            nodal_outage = (env.network_len_nodes*nodal_outage_percentage)/100\n",
    "            round_nodal_outage = round(nodal_outage)\n",
    "\n",
    "            # CPU CORE\n",
    "            for core_cpu in arr_core_cpu:\n",
    "\n",
    "                #VNF PER CPU CORE\n",
    "                for num_vnf_per_core in arr_num_vnf_per_core:\n",
    "                    tot_vnf_per_core = core_cpu*num_vnf_per_core\n",
    "                    tot_vnf_network = tot_vnf_per_core*env.network_len_nodes\n",
    "                    pr_epsilon, epsilon = 1, 1\n",
    "                    eps_history,network_overall_accepted_action = [],[]\n",
    "                    network_overall_accepted, network_overall_rejected = [],[]\n",
    "                    monolithic_deploy_success, monolithic_deploy_rejected = [], []\n",
    "                    overall_res_exhausted = []\n",
    "                    network_overall_count_mvnf, network_overall_sum_mvnf = [],[]\n",
    "                    network_overall_tot_decompsfc, network_overall_count_succes_decompvnf,  = [], []\n",
    "                    network_overall_count_unsucces_decompvnf, overall_remain_cpu, overall_remain_bw = [], [], []\n",
    "                    overall_count_numVNF_perNode, overall_count_num_mVNF_perNode = [], []\n",
    "                    network_overall_epsilon, network_overall_sum_rewards_perepi = [], []\n",
    "                    overall_episode_priority,  overall_episode_priority_err= [], []\n",
    "                    overall_episode_pr_rewardfunc = []\n",
    "                    over_count_macro_pr, over_count_micro_pr = [], []\n",
    "                    overall_epi_pr,overall_epi_pr_emb = [],[]\n",
    "                    overall_epi_traffic_packets_dict, overall_epi_traffic_load_dict = {}, {}\n",
    "                    start = time.time()\n",
    "                    print(\"\")\n",
    "                    print(\"Start Time:\",start)\n",
    "                    #overall runtime these needs to be initiated\n",
    "                    n_state = [8]\n",
    "                    state_memory      = np.zeros((mem_size, *n_state), dtype = np.float32) # size of mem size X input shape\n",
    "                    action_memory     = np.zeros(mem_size, dtype = np.int64)\n",
    "                    reward_memory     = np.zeros(mem_size, dtype = np.float32)\n",
    "                    next_state_memory = np.zeros((mem_size, *n_state), dtype = np.float32)\n",
    "                    terminal_memory   = np.zeros(mem_size, dtype = np.bool)\n",
    "                    vnf_cnt = 0\n",
    "                    temp_vals_sorted_action, temp_norm_vals_sorted_action = [], [] \n",
    "                    if model == 'DDQN':\n",
    "                        print(\"activate agent DDQN\")\n",
    "                        #agent\n",
    "                        agent = LinearDDQNAgent(n_state = n_state, \n",
    "                                                n_actions = env.network_len_nodes, \n",
    "                                                lr = learning_rate, \n",
    "                                                epsilon = epsilon,\n",
    "                                                mem_size = mem_size,\n",
    "                                                batch_size= batch_size,\n",
    "                                                vnf_cnt = vnf_cnt,\n",
    "                                                gamma =gamma,\n",
    "                                                eps_dec =eps_dec,\n",
    "                                                eps_min =eps_min,\n",
    "                                                replace = replace,\n",
    "                                                model = model,\n",
    "                                                state_memory = state_memory,\n",
    "                                                action_memory = action_memory,\n",
    "                                                reward_memory = reward_memory,\n",
    "                                                next_state_memory = next_state_memory,\n",
    "                                                terminal_memory = terminal_memory,\n",
    "                                                Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                                drop_prob =drop_prob,\n",
    "                                                chkpt_dir = 'models/',\n",
    "                                                algo = 'DDQNAgent',\n",
    "                                                env_name = 'BtEurope')\n",
    "                    elif model == 'DQN':\n",
    "                        agent = LinearDQNAgent(n_state = n_state, \n",
    "                                               n_actions = env.network_len_nodes, \n",
    "                                               epsilon = epsilon, \n",
    "                                               lr = learning_rate, \n",
    "                                               vnf_cnt = vnf_cnt, \n",
    "                                               Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                               drop_prob =drop_prob,\n",
    "                                               gamma =gamma,\n",
    "                                               eps_dec =eps_dec,\n",
    "                                               eps_min =eps_min, \n",
    "                                               model = model,\n",
    "                                               algo = None, \n",
    "                                               env_name = None,\n",
    "                                               chkpt_dir ='tmp/dqn')\n",
    "                    #added on 01032022 Heuristic model\n",
    "                    else:\n",
    "                        print(\"Activate Heuristic model\")\n",
    "                        agent = HeuAgent(n_state = n_state, \n",
    "                                         n_actions = env.network_len_nodes)  \n",
    "                        \n",
    "                    cnt_ridge = 0\n",
    "                    # Priority\n",
    "                    if switch_priorty is True:\n",
    "                        pr_mem_size = 5000\n",
    "                        pr_n_state = [3]  #  waiting time, delay, jitter, Packetloss,\n",
    "                        pr_state_input = 'waiting time, delay, jitter, Packetloss'\n",
    "                        # [episode, sfc, flow behaviour, delay, jitter, Packetloss, reliability(removed), wating time]\n",
    "                        pr_n_actions = 1\n",
    "                        pr_state_memory      = np.zeros((pr_mem_size, *pr_n_state), dtype = np.float32) # size of mem size X input shape\n",
    "                        pr_reward_memory     = np.zeros(pr_mem_size, dtype = np.float32)\n",
    "                        pr_next_state_memory = np.zeros((pr_mem_size, *pr_n_state), dtype = np.float32)\n",
    "                        pr_terminal_memory   = np.zeros(pr_mem_size, dtype = np.bool)\n",
    "                        sfc_cnt = 0\n",
    "                        if pr_model == 'DDPG':\n",
    "                            print(\"pr_model: \",pr_model)\n",
    "                            pr_action_memory = np.zeros((pr_mem_size, pr_n_actions), dtype = np.float32)\n",
    "                            pr_agent = DDPG_Agent(pr_n_state       = pr_n_state, \n",
    "                                                  pr_n_actions     = pr_n_actions,\n",
    "                                                  alpha            = alpha, \n",
    "                                                  beta             = beta,\n",
    "                                                  tau              = tau, \n",
    "                                                  sfc_cnt          = sfc_cnt,\n",
    "                                                  mem_size         = mem_size,\n",
    "                                                  batch_size       = batch_size,\n",
    "                                                  gamma            = gamma,\n",
    "                                                  pr_model         = pr_model,\n",
    "                                                  pr_state_memory  = pr_state_memory,\n",
    "                                                  pr_action_memory = pr_action_memory,\n",
    "                                                  pr_reward_memory = pr_reward_memory,\n",
    "                                                  pr_next_state_memory = pr_next_state_memory,\n",
    "                                                  pr_terminal_memory = pr_terminal_memory,\n",
    "                                                  Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                                  chkpt_dir = 'models/',\n",
    "                                                  algo = 'DDPGAgent',\n",
    "                                                  env_name = 'Netrail')\n",
    "                        elif pr_model == 'DDQN':\n",
    "                            print(pr_model)\n",
    "                            pr_action_memory = np.zeros((pr_mem_size), dtype = np.float32)\n",
    "                            pr_agent = PR_DDQNAgent(pr_n_state       = pr_n_state,\n",
    "                                                 pr_n_actions     = pr_n_actions,\n",
    "                                                 lr               = learning_rate, \n",
    "                                                 pr_epsilon       = pr_epsilon,\n",
    "                                                 mem_size         = mem_size,\n",
    "                                                 batch_size       = batch_size,\n",
    "                                                 sfc_cnt          = sfc_cnt,\n",
    "                                                 gamma            = gamma,\n",
    "                                                 eps_dec          = eps_dec,\n",
    "                                                 eps_min          = eps_min,\n",
    "                                                 replace          = replace,\n",
    "                                                 pr_model         = pr_model,\n",
    "                                                 pr_state_memory  = pr_state_memory,\n",
    "                                                 pr_action_memory = pr_action_memory,\n",
    "                                                 pr_reward_memory = pr_reward_memory,\n",
    "                                                 pr_next_state_memory = pr_next_state_memory,\n",
    "                                                 pr_terminal_memory = pr_terminal_memory,\n",
    "                                                 Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                                 drop_prob =drop_prob,\n",
    "                                                 chkpt_dir = 'models/',\n",
    "                                                 algo = 'PR_DDQNAgent',\n",
    "                                                 env_name = 'Netrail')    \n",
    "                        elif pr_model == 'ridge':\n",
    "                            print(pr_model)\n",
    "                            #Lasso model\n",
    "                            alpha = 0.0001\n",
    "                            input_train_data,target_train_data = [], [] \n",
    "                            input_test_data, target_test_data  = [], []\n",
    "                            input_train_mem     = np.zeros((mem_size, *pr_n_state), dtype = np.float32)\n",
    "                            target_train_mem    = np.zeros(mem_size, dtype = np.float32)\n",
    "                            input_test_mem      = np.zeros((mem_size, *pr_n_state), dtype = np.float32)\n",
    "                            target_test_mem     = np.zeros(mem_size, dtype = np.float32)\n",
    "                            # define model\n",
    "                            ridge_model = Ridge(alpha=alpha, normalize = True) #Lasso(alpha=alpha, normalize = True)\n",
    "                            cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeat, random_state=1)\n",
    "                            done_status = \"Train\"\n",
    "                        else: \n",
    "                            #print(\"here\")\n",
    "                            print(pr_model)\n",
    "                            hidden_neuron = 20\n",
    "                            input_train_data,target_train_data = [], [] \n",
    "                            input_test_data, target_test_data  = [], []\n",
    "                            pr_MLP_model = MLP(pr_n_state = pr_n_state, \n",
    "                                               pr_n_actions = pr_n_actions, \n",
    "                                               hidden_neuron = hidden_neuron)\n",
    "                            input_train_mem     = np.zeros((mem_size, *pr_n_state), dtype = np.float)\n",
    "                            target_train_mem    = np.zeros(mem_size, dtype = np.float)\n",
    "                            input_test_mem      = np.zeros((mem_size, *pr_n_state), dtype = np.float)\n",
    "                            target_test_mem     = np.zeros(mem_size, dtype = np.float)\n",
    "                            done_status = \"Train\"\n",
    "                            \n",
    "                    \n",
    "                    # Admission control: Scheduling \n",
    "                    if switch_scheduling is True:\n",
    "                        sch_mem_size = 5000\n",
    "                        sch_n_state = [4]  #  waiting time, delay, jitter, Packetloss,\n",
    "                        sch_state_input = 'waiting time, reliability, priority'\n",
    "                        # [episode, sfc, flow behaviour, delay, jitter, Packetloss, reliability(removed), wating time]\n",
    "                        sch_n_actions = 1\n",
    "                        sch_n_tempsfc = [10]\n",
    "                        sch_state_memory      = np.zeros((sch_mem_size,*sch_n_state), dtype = np.float32) # size of mem size X input shape\n",
    "                        #print(sch_state_memory)\n",
    "                        sch_reward_memory     = np.zeros(sch_mem_size, dtype = np.float32)\n",
    "                        sch_next_state_memory = np.zeros((sch_mem_size, *sch_n_state), dtype = np.float32)\n",
    "                        sch_terminal_memory   = np.zeros(sch_mem_size, dtype = np.bool)\n",
    "                        sfc_cnt = 0\n",
    "                        if sch_model == 'DDPG':\n",
    "                            #print(\"sch_model\", sch_model)\n",
    "                            sch_action_memory     = np.zeros((sch_mem_size, sch_n_actions), dtype = np.float32)\n",
    "                            sch_agent = DDPG_Agent(sch_n_state      = sch_n_state, \n",
    "                                                   sch_n_actions    = sch_n_actions,\n",
    "                                                   alpha            = alpha, \n",
    "                                                   beta             = beta,\n",
    "                                                   tau              = tau, \n",
    "                                                   sfc_cnt          = sfc_cnt,\n",
    "                                                   mem_size         = mem_size,\n",
    "                                                   batch_size       = batch_size,\n",
    "                                                   gamma            = gamma,\n",
    "                                                   sch_model        = sch_model,\n",
    "                                                   sch_state_memory  = sch_state_memory,\n",
    "                                                   sch_action_memory = sch_action_memory,\n",
    "                                                   sch_reward_memory = sch_reward_memory,\n",
    "                                                   sch_next_state_memory = sch_next_state_memory,\n",
    "                                                   sch_terminal_memory = sch_terminal_memory,\n",
    "                                                   Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                                   chkpt_dir = 'models/',\n",
    "                                                   algo = 'DDPGAgent',\n",
    "                                                   env_name = 'Netrail')\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                    a = 0\n",
    "                    cnt_tot_sfc , tr_cnt_tot_sfc, eval_cnt_tot_sfc, pred_cnt_tot_sfc= 0, 0, 0, 0\n",
    "                    count_macro_pr, count_micro_pr = 0, 0\n",
    "                    tr_count_macro_pr, tr_count_micro_pr = 0, 0\n",
    "                    eval_count_macro_pr, eval_count_micro_pr = 0, 0\n",
    "                    pred_count_macro_pr, pred_count_micro_pr = 0, 0\n",
    "                    tr_mem_cnt, test_mem_cnt = 0, 0\n",
    "                    input_train_traffic_mem_cnt, test_traffic_mem_cnt = 0, 0\n",
    "                    epi_tr_correct_perc,  epi_pred_correct_perc = [], []\n",
    "                    overall_deployed_sfc_per_episode, overall_deployed_priority_per_episode = [], []\n",
    "                    epi_cnt_High_pr, epi_cnt_Medium_pr, epi_cnt_Low_pr = [], [], []\n",
    "                    overall_epi_cnt_High_pr_emb, overall_epi_cnt_Medium_pr_emb, overall_epi_cnt_Low_pr_emb = [], [], []\n",
    "                    overall_epi_cnt_High_traffic0_emb, overall_epi_cnt_Medium_traffic0_emb, overall_epi_cnt_Low_traffic0_emb = [], [], []\n",
    "                    overall_epi_cnt_High_traffic1_emb, overall_epi_cnt_Medium_traffic1_emb, overall_epi_cnt_Low_traffic1_emb = [], [], []\n",
    "                    overall_epi_cnt_low_pr_traffic_1 = []\n",
    "                    overall_cnt_sfc_starved_before_serving, overall_cnt_sfc_Prclass_starved_before_serving = [], []\n",
    "                    epi_cnt_sfc_starved_before_serving, epi_cnt_sfc_Prclass_starved_before_serving = [], []\n",
    "                    epi_expected_wt_B = {}\n",
    "                    overall_epi_traffic_class_gen ={}\n",
    "                    overall_epi_predicted_traffic_classes_dict = {}\n",
    "                    overall_epi_sfc_pr_traffic_class_emb, overall_epi_sfc_pr_traffic_class_all = {}, {}\n",
    "                        \n",
    "\n",
    "                    for per_episode in global_services.vNetwork_service_details_er: \n",
    "                        episode = a\n",
    "                        print(\"\")\n",
    "                        print(\"Episode: \", episode)\n",
    "                        action, vnf_res_req = False, False\n",
    "                        pref_node_val = False\n",
    "                        cnt_macro_pr, epi_cnt_macro_pr = 0,0 \n",
    "                        cnt_High_pr, cnt_Medium_pr, cnt_Low_pr = 0, 0, 0\n",
    "                        cnt_High_pr_emb, cnt_Medium_pr_emb, cnt_Low_pr_emb = 0, 0, 0\n",
    "                        cnt_High_traffic0_emb, cnt_Medium_traffic0_emb, cnt_Low_traffic0_emb = 0, 0, 0\n",
    "                        cnt_High_traffic1_emb, cnt_Medium_traffic1_emb, cnt_Low_traffic1_emb = 0, 0, 0\n",
    "                        epi_pr = {\"H\": None,\"M\": None, \"L\": None}\n",
    "                        epi_pr_emb = {\"H\": None,\"M\": None, \"L\": None}\n",
    "                        cnt_sfc_starved_before_serving = 0\n",
    "                        cnt_rel = 0\n",
    "                        epi_cnt_low_pr_traffic_1 = 0\n",
    "                        epi_classification_grp_acc = {}\n",
    "                        kmeans_runtime, Aggo_runtime, birch_runtime, GM_runtime = [],[],[],[]\n",
    "                        traffic_input_dataset = []\n",
    "                        sfc_starved_before_serving, sfc_Prclass_starved_before_serving = [], []\n",
    "                        nodal_actionspace, removed_nodes = [], []\n",
    "                        epi_emb_sfc, epi_emb_sfc_h_rel = [], []\n",
    "                        traffic_sfc_arrival = []\n",
    "                        traffic_transition_dict, cluster_label_val = {}, {}\n",
    "                        arr_traffic_packet_per_epi = []\n",
    "                        epi_unemb_sfc = []\n",
    "                        epi_temp_sfc_arrived = []\n",
    "                        epi_traffic_class_gen = {}\n",
    "                        epi_predicted_traffic_classes_arr = []\n",
    "                        epi_predicted_traffic_classes_dict = {}\n",
    "                        epi_sfc_pr_traffic_class_emb, epi_sfc_pr_traffic_class_all = {}, {}\n",
    "                        \n",
    "                        episode_priority, episode_priority_err, episode_envpriority, episode_pr_rewardfunc = [], [], [], []\n",
    "                        actionspace = [i for i in range(env.network_len_nodes)]\n",
    "                        original_actionspace = actionspace\n",
    "                        # to remove certain number of the nodes from the network when outage happens\n",
    "                        for key in range(round_nodal_outage):\n",
    "                            random_action = random.choice(actionspace)\n",
    "                            actionspace.remove(random_action)\n",
    "                            removed_nodes.append(random_action)                 \n",
    "                        #----Resource Initialization for the network Network----#\n",
    "                        env.substrate_node_init(removed_nodes, tot_vnf_per_core)\n",
    "                        env.substrate_link_init()\n",
    "                        if switch_priorty is True and pr_model == 'DDPG':\n",
    "                            pr_agent.noise.reset()\n",
    "                        else:\n",
    "                            pass\n",
    "                        #---- End of Initialization of Resource ----#\n",
    "                        count_accpeted, count_rejected, count_success,count_failed  = 0,0,0,0\n",
    "                        reward_per_episode, sum_reward_per_episode = [], []\n",
    "                        deployed_sfc_per_episode, deployed_priority_per_episode = [], []\n",
    "                        micro_vnf_collection = []\n",
    "                        tot_decompsfc = 0\n",
    "                        undecomp_accepted = 0\n",
    "                        res_exhausted = 0\n",
    "                        count_mvnf_perepi =[]\n",
    "                        sum_count_mvnf_perepi =0\n",
    "                        sum_rewards_perepi = 0\n",
    "                        count_succes_decompvnf, count_unsucces_decompvnf = 0, 0\n",
    "                        nodal_avail = 1 #at the start of the episode the network will be fully available.\n",
    "                        rewards_perepi = []\n",
    "                        std_value = 1\n",
    "                        \n",
    "                        # Priority!\n",
    "                        epi_tr_count_macro_pr,   epi_tr_count_micro_pr   = 0, 0\n",
    "                        epi_eval_count_macro_pr, epi_eval_count_micro_pr = 0, 0\n",
    "                        epi_pred_count_macro_pr, epi_pred_count_micro_pr = 0, 0\n",
    "                        \n",
    "                        # arrival distriution of SFC using Poisson dis.\n",
    "                        if len(overall_sfc_arrival_num) != len(global_services.vNetwork_service_details_er):\n",
    "                            arr_sfc_dist = []\n",
    "                            class_arr_sfc_dist = arrival_sfc(num_sfc, arr_sfc_dist)\n",
    "                            arr_sfc_dist = list(class_arr_sfc_dist.sfc_poisson_dist())\n",
    "                            #print(\"==> SFC dist: \",arr_sfc_dist)\n",
    "                            sfc_arrival_detail, sfc_arrival_num = [], []\n",
    "                            arr_sfc_detail = class_arr_sfc_dist.sfc_details(per_episode, \n",
    "                                                                            arr_sfc_dist, \n",
    "                                                                            sfc_arrival_detail, \n",
    "                                                                            sfc_arrival_num)\n",
    "                            overall_sfc_arrival_detail.append(sfc_arrival_detail)\n",
    "                            overall_sfc_arrival_num.append(sfc_arrival_num)\n",
    "                            print(\"sfc_arrival_num: \", sfc_arrival_num)\n",
    "\n",
    "                        else: \n",
    "                            pass\n",
    "                        #print(\"Length of overall_sfc_arrival_num: \",len(overall_sfc_arrival_num))\n",
    "                        #print(\"overall_sfc_arrival_num: \", overall_sfc_arrival_num)\n",
    "                        sfc_arrival_detail = overall_sfc_arrival_detail[episode]\n",
    "                        sfc_arrival_num = overall_sfc_arrival_num[episode]\n",
    "                        sfc_priority, sfc_priority_err, sfc_envpriority, sfc_pr_rewardfunc = [], [], [], []\n",
    "                        done_pr   = False\n",
    "                        ar_sfc_current_waitingtime = {}\n",
    "                        ar_sorted_sch_list =[]\n",
    "                        cnt_poisson_dist = 0\n",
    "                        for arr_sfc_de in sfc_arrival_detail:\n",
    "                            ar_sfc_start = time.perf_counter() # step 1\n",
    "                            cnt_sfc_arrival = 0\n",
    "                            temp_my_dict = {}\n",
    "                            ar_starving_sfc = []\n",
    "                            #print(\"\")\n",
    "                            #print(\"arr_sfc_de:\", arr_sfc_de)\n",
    "                            #print(\"cnt_poisson_dist: \", cnt_poisson_dist, \" arr num: \", sfc_arrival_num[cnt_poisson_dist])\n",
    "                            if sch_model == 'WFQ':\n",
    "                                h_pr_queue, m_pr_queue, l_pr_queue = [], [], []\n",
    "                            if not sfc_arrival_num[cnt_poisson_dist]:\n",
    "                                sorted_sch_list =[]\n",
    "                            ar_sfc_end = time.perf_counter() #step 1\n",
    "                            expected_wt_A = round(ar_sfc_end - ar_sfc_start,6)\n",
    "\n",
    "                            for arv_per_sfc, arv_per_link in arr_sfc_de:\n",
    "                                sfc = arv_per_sfc[0][1]['sfc']\n",
    "                                #print(\"arv_per_sfc: \", arv_per_sfc)\n",
    "                                print(\"SFC: \", sfc)\n",
    "                                ar_link_bw = []\n",
    "                                for link in arv_per_link:\n",
    "                                    temp_bw = link[2]['bw']\n",
    "                                    ar_link_bw.append(temp_bw)\n",
    "                                ar_sfc_cpu = []\n",
    "                                for temp_sfc in arv_per_sfc:\n",
    "                                    temp_cpu = temp_sfc[1]['cpu_req']\n",
    "                                    ar_sfc_cpu.append(temp_cpu)     \n",
    "                                # Priority for Arriving SFC\n",
    "                                env_priority = None\n",
    "                                env_priority = arv_per_sfc[0][1]['env_priority']\n",
    "                                env_priority = env_priority - env_priority % 0.01\n",
    "                                done_sch = False\n",
    "                                A_cost, B_cost, S_cost = 0,0,0\n",
    "                                reward_pts = 1000\n",
    "                                #print(\"SFC: \", sfc)\n",
    "                                if status_cnt_ridge is False:\n",
    "                                    print(\"pr_model: \", pr_model)\n",
    "                                    if switch_priorty is True:\n",
    "                                            pr_sfc_start = time.perf_counter()\n",
    "                                            threshold_latency          = arv_per_sfc[0][1]['norm_delay'] \n",
    "                                            threshold_jitter           = arv_per_sfc[0][1]['norm_jitter']     \n",
    "                                            threshold_packetloss       = arv_per_sfc[0][1]['norm_packetloss'] \n",
    "                                            threshold_theo_wating_time = arv_per_sfc[0][1]['norm_waiting_time'] \n",
    "                                            \n",
    "                                            # creating priority observation\n",
    "                                            pr_obs = [threshold_latency, threshold_jitter, threshold_packetloss]\n",
    "                                            if pr_model == 'ridge':\n",
    "                                                X = pr_obs\n",
    "                                                Y = [env_priority]\n",
    "                                                sfc_envpriority.append(env_priority)\n",
    "                                                env_pr_level = truncate(Y[0],2)\n",
    "                                                if env_pr_level == 1:\n",
    "                                                    macro_env_pr_level = int(truncate(env_pr_level, 0))\n",
    "                                                    micro_env_pr_level = round((truncate(env_pr_level, 1)- macro_env_pr_level),2)\n",
    "                                                else:\n",
    "                                                    macro_env_pr_level = int(truncate(env_pr_level*10, 0))\n",
    "                                                    micro_env_pr_level = int(truncate(env_pr_level*100, 1))- macro_env_pr_level*10\n",
    "                                                if done_status == \"Train\": \n",
    "                                                    tr_cnt_tot_sfc += 1\n",
    "                                                    train_dataset = prepare_train_data(input_train_mem, target_train_mem, \n",
    "                                                                                       input_train_data, target_train_data,\n",
    "                                                                                       X, Y, \n",
    "                                                                                       tr_mem_cnt, tr_mem_size, tr_batch_size)\n",
    "                                                    input_train_data = train_dataset[0]\n",
    "                                                    target_train_data = train_dataset[1]\n",
    "                                                    tr_mem_cnt = train_dataset[2]\n",
    "                                                    #print(\"train_dataset: \", train_dataset)\n",
    "                                                    if tr_cnt_tot_sfc < tr_batch_size:\n",
    "                                                        temp_priority_level = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "                                                        priority_level  = random.uniform(0,1) #np.random.choice(temp_priority_level)\n",
    "                                                        #print(\"priority_level: \", priority_level, type(priority_level))\n",
    "                                                    else:\n",
    "                                                        # evaluate model\n",
    "                                                        scores = cross_val_score(ridge_model, input_train_data, target_train_data, \n",
    "                                                                                 scoring='neg_mean_absolute_error', \n",
    "                                                                                 cv=cv, n_jobs= 1)\n",
    "                                                        # force scores to be positive\n",
    "                                                        scores = absolute(scores)\n",
    "                                                        # fit model\n",
    "                                                        ridge_model.fit(input_train_data, target_train_data)\n",
    "                                                        # make a prediction\n",
    "                                                        yhat = ridge_model.predict([X])\n",
    "                                                        # summarize prediction\n",
    "                                                        priority_level = yhat[0]\n",
    "                                                        #print(\"priority_level: \", priority_level, type(priority_level))\n",
    "\n",
    "                                                    #classifing the priority levels into three classess.\n",
    "                                                    if 0.8<=priority_level and priority_level<=1.0:\n",
    "                                                        priority_class = 'high'\n",
    "                                                        cnt_High_pr +=1\n",
    "                                                    elif 0.4<=priority_level and priority_level<=0.7:\n",
    "                                                        priority_class = 'medium'\n",
    "                                                        cnt_Medium_pr +=1\n",
    "                                                    else: \n",
    "                                                        priority_class = 'low'   \n",
    "                                                        cnt_Low_pr +=1\n",
    "                                                    sfc_priority.append(priority_level)\n",
    "                                                    global_services.priority_update(episode, sfc, \n",
    "                                                                                    vnf_per_service, \n",
    "                                                                                    priority_level, \n",
    "                                                                                    priority_class)\n",
    "                                                    if priority_level == 1:\n",
    "                                                        macro_priority_level = 1 #int(truncate(priority_level, 0))\n",
    "                                                        micro_priority_level = 0 #round((truncate(priority_level, 1)- macro_priority_level),2)\n",
    "                                                    else:\n",
    "                                                        macro_priority_level = int(truncate(priority_level*10, 0))\n",
    "                                                        micro_priority_level = int(truncate(priority_level*100, 1))- macro_priority_level*10\n",
    "                                                    if macro_priority_level == macro_env_pr_level:\n",
    "                                                        #first level clearance\n",
    "                                                        tr_count_macro_pr += 1\n",
    "                                                        epi_tr_count_macro_pr += 1\n",
    "                                                        if micro_priority_level == micro_env_pr_level:\n",
    "                                                            tr_count_micro_pr += 1\n",
    "                                                            epi_tr_count_micro_pr += 1\n",
    "                                                    tr_macro_acc = round(((tr_count_macro_pr/tr_cnt_tot_sfc)*100), 2)\n",
    "                                                    tr_micro_acc = round(((tr_count_micro_pr/tr_cnt_tot_sfc)*100), 2)\n",
    "                                                    if a == 0:\n",
    "                                                        pass\n",
    "                                                    else:\n",
    "                                                        if (tr_macro_acc/100) >= acc_theshold_limit:\n",
    "                                                            #done_status == \"Train\"\n",
    "                                                            done_status = \"Predict\"\n",
    "                                                            #print(\"Train --> Predict\", done_status, cnt_tot_sfc, tr_cnt_tot_sfc,\n",
    "                                                            #      \" macro acc: \", tr_macro_acc, \" micro acc: \",tr_micro_acc, tr_count_micro_pr)\n",
    "                                                else:\n",
    "                                                    pred_cnt_tot_sfc +=1\n",
    "                                                    # make a prediction\n",
    "                                                    yhat = ridge_model.predict([X])\n",
    "                                                    # summarize prediction\n",
    "                                                    priority_level = yhat[0]\n",
    "                                                    #print(\"priority_level: \", priority_level, type(priority_level))\n",
    "                                                    #classifing the priority levels into three classess.\n",
    "                                                    if 0.8 <= priority_level and priority_level <= 1.0: #between 0.7 - 1.0\n",
    "                                                        priority_class = 'high' \n",
    "                                                        cnt_High_pr +=1\n",
    "                                                    elif 0.4 <= priority_level and priority_level <= 0.7:  #between 0.4 - 0.7\n",
    "                                                        priority_class = 'medium'\n",
    "                                                        cnt_Medium_pr +=1\n",
    "                                                    else: \n",
    "                                                        priority_class = 'low' #between 0.0 - 0.3\n",
    "                                                        cnt_Low_pr +=1\n",
    "                                                    sfc_priority.append(priority_level)\n",
    "                                                    global_services.priority_update(episode, sfc, vnf_per_service, \n",
    "                                                                                    priority_level, priority_class)\n",
    "                                                    if priority_level == 1:\n",
    "                                                        macro_priority_level = 1 #int(truncate(priority_level, 0))\n",
    "                                                        micro_priority_level = 0 #round((truncate(priority_level, 1)- macro_priority_level),2)\n",
    "                                                    else:\n",
    "                                                        macro_priority_level = int(truncate(priority_level*10, 0))\n",
    "                                                        micro_priority_level = int(truncate(priority_level*100, 1))- macro_priority_level*10\n",
    "                                                        #micro_env_pr_level = round(micro_env_pr_level,2)\n",
    "                                                    if macro_priority_level == macro_env_pr_level:\n",
    "                                                        #first level clearance\n",
    "                                                        pred_count_macro_pr += 1\n",
    "                                                        epi_pred_count_macro_pr += 1\n",
    "                                                        if micro_priority_level == micro_env_pr_level:\n",
    "                                                            pred_count_micro_pr += 1\n",
    "                                                            epi_pred_count_micro_pr += 1\n",
    "                                                    pred_macro_acc =round(((pred_count_macro_pr/pred_cnt_tot_sfc)*100),2)\n",
    "                                                    pred_micro_acc =round(((pred_count_micro_pr/pred_cnt_tot_sfc)*100),2)\n",
    "                                                    if (pred_macro_acc/100) >= acc_theshold_limit:\n",
    "                                                        done_status = \"Done\"\n",
    "                                                    if overall_sfc == cnt_tot_sfc:\n",
    "                                                        pred_macro_acc =round(((pred_count_macro_pr/pred_cnt_tot_sfc)*100),2)\n",
    "                                                        pred_micro_acc =round(((pred_count_micro_pr/pred_cnt_tot_sfc)*100),2)\n",
    "                                            else: \n",
    "                                                priority_level = arv_per_sfc[0][1]['env_priority']\n",
    "                                            #print(\"SFC: \", sfc, \"priority_level: \", priority_level)\n",
    "                                            pr_sfc_end = time.perf_counter()\n",
    "                                            expected_wt_B = round((pr_sfc_end - pr_sfc_start), 6) # step2\n",
    "                                            #print(\"expected_wt_B: \", expected_wt_B)\n",
    "                                            epi_expected_wt_B[sfc] = expected_wt_B\n",
    "                                else:           \n",
    "                                    pass\n",
    "                                \n",
    "                                if switch_scheduling is True:\n",
    "                                    #print(\"switch_scheduling: \", switch_scheduling)\n",
    "                                    #print(\"episode: \", episode)\n",
    "                                    sch_sfc_start = time.perf_counter()\n",
    "                                    if sch_model == 'DDPG':\n",
    "                                        print(\"sch_model: \", sch_model)\n",
    "                                        sch_obs = []\n",
    "                                        threshold_priority_level    = arv_per_sfc[0][1]['priority_level']\n",
    "                                        threshold_reliability       = arv_per_sfc[0][1]['reliability']\n",
    "                                        threshold_theo_wating_time  = arv_per_sfc[0][1]['theo_wating_time']\n",
    "                                        norm_waitingtime            = arv_per_sfc[0][1]['norm_waiting_time']\n",
    "                                        threshold_reliability       = arv_per_sfc[0][1]['reliability']\n",
    "                                        threshold_evnpriority_level = arv_per_sfc[0][1]['env_priority']\n",
    "                                        threshold_int_flowtype      = arv_per_sfc[0][1]['int_flow_behaviour']\n",
    "                                        current_starving_sfc = None\n",
    "                                        intersec_sfc = None\n",
    "                                        starving_sfc_index = None\n",
    "                                        if threshold_priority_level <= 0.2:\n",
    "                                            current_starving_sfc = sfc\n",
    "                                            ar_starving_sfc.append(sfc)\n",
    "                                            \n",
    "                                        traffic_input_dataset,  traffic_output_dataset = [], []\n",
    "                                        class_traffic_classifer = traffic_classifer(traffic_input_dataset, \n",
    "                                                                                    traffic_output_dataset, \n",
    "                                                                                    episode, cluster_testing)\n",
    "                                        \n",
    "                                        classifier_datasets = [None, None, None, None]\n",
    "                                        if switch_clustering is True:\n",
    "                                            #print( \"==>\", switch_clustering, classifier_training_status)\n",
    "                                            mu, std = 100, 300\n",
    "                                            pred_traffic_num_sfc_requested = abs(random.normal(mu, std, 1))\n",
    "                                            while pred_traffic_num_sfc_requested < 1:\n",
    "                                                pred_traffic_num_sfc_requested = abs(random.normal(mu, std, 1))\n",
    "                                            pred_num =  abs(int(pred_traffic_num_sfc_requested))\n",
    "                                            pred_f = FBM(n  = int(pred_num), hurst=0.7, length=length, method='daviesharte')\n",
    "                                            pred_fbm_sample = abs(pred_f.fbm())\n",
    "                                            pred_fgn_sample = pred_f.fgn()\n",
    "                                            pred_t_values   = pred_f.times() #stationary traffic\n",
    "                                            pred_traffic_packet_per_sfc = sum(pred_t_values) #load\n",
    "                                            \n",
    "                                            predict_dataset = [threshold_priority_level, \n",
    "                                                               threshold_int_flowtype, pred_num, \n",
    "                                                               pred_traffic_packet_per_sfc]\n",
    "                                            #print(\"==> predict_dataset\", predict_dataset)\n",
    "                                            #print(\"classifier_models: \", classifier_models)\n",
    "                                            Classifier_training = class_traffic_classifer.classifier_models(classifier_models, \n",
    "                                                                                                            classifier_datasets,\n",
    "                                                                                                            predict_dataset,\n",
    "                                                                                                            classifier_training_status,\n",
    "                                                                                                            Switch_classifier_info)\n",
    "                                            \n",
    "                                            #print(\"Classifier_training: \", Classifier_training)\n",
    "                                            #classifier_measurements[clusterModel] = Classifier_training\n",
    "                                            traffic_pred = Classifier_training[4][0]\n",
    "                                            #print(traffic_pred)\n",
    "                                        else: \n",
    "                                            traffic_pred = -1\n",
    "                                            \n",
    "                                        #print(\"traffic_pred: \", traffic_pred)   \n",
    "                                        #updating in global level\n",
    "                                        global_services.classifier_class_update(episode, sfc, vnf_per_service, \n",
    "                                                                                traffic_pred, pred_num,\n",
    "                                                                                pred_traffic_packet_per_sfc\n",
    "                                                                               )   \n",
    "                                        \n",
    "                                        sch_obs = [threshold_theo_wating_time, \n",
    "                                                   threshold_reliability,\n",
    "                                                   threshold_priority_level, \n",
    "                                                   traffic_pred\n",
    "                                                  ]\n",
    "                                        sch_rank = sch_agent.choose_action(sch_obs, sch_n_state)\n",
    "                                        temp_my_dict[sfc] = sch_rank\n",
    "                                        sorted_sfc_sch = dict(sorted(temp_my_dict.items(), \n",
    "                                                                    key=lambda item: item[1], \n",
    "                                                                    reverse = True))\n",
    "                                        sorted_sch_list = list(sorted_sfc_sch.keys())\n",
    "                                        \n",
    "                                        if ar_starving_sfc is None or not ar_starving_sfc:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            intersec_sfc = set(sorted_sch_list).intersection([current_starving_sfc])\n",
    "                                            if intersec_sfc is None or not intersec_sfc:\n",
    "                                                pass\n",
    "                                            else:\n",
    "                                                starving_sfc_index = sorted_sch_list.index(current_starving_sfc) \n",
    "                                                \n",
    "                                        if cnt_sfc_arrival == len(sfc_arrival_num[cnt_poisson_dist])-1:\n",
    "                                            done_sch = True\n",
    "                                            next_sch_obs = [#sfc, \n",
    "                                                            #threshold_evnpriority_level, \n",
    "                                                            #threshold_priority_level, \n",
    "                                                            threshold_theo_wating_time, \n",
    "                                                            threshold_reliability,\n",
    "                                                            threshold_priority_level,\n",
    "                                                            traffic_pred\n",
    "                                                           ]\n",
    "                                        else:\n",
    "                                            next_sfc = cnt_sfc_arrival + 1\n",
    "                                            next_threshold_priority_level    = arr_sfc_de[next_sfc][0][0][1]['priority_level']\n",
    "                                            next_threshold_reliability       = arr_sfc_de[next_sfc][0][0][1]['reliability']\n",
    "                                            next_threshold_theo_wating_time  = arr_sfc_de[next_sfc][0][0][1]['theo_wating_time']\n",
    "                                            next_threshold_evnpriority_level = arr_sfc_de[next_sfc][0][0][1]['env_priority']\n",
    "                                            next_threshold_int_flowtype      = arr_sfc_de[next_sfc][0][0][1]['int_flow_behaviour']\n",
    "                                            traffic_pred = -1\n",
    "                                            next_sch_obs = [next_threshold_theo_wating_time, \n",
    "                                                            next_threshold_reliability,\n",
    "                                                            next_threshold_evnpriority_level, \n",
    "                                                            traffic_pred] \n",
    "                                        sch_next_state = next_sch_obs\n",
    "                                        #Reward function\n",
    "                                        A_cost = (threshold_priority_level*reward_pts) + (norm_waitingtime*reward_pts)\n",
    "                                        link_cost = 0 #(1/sum(ar_link_bw))*reward_pts\n",
    "                                        cpu_cost  = 0 #(1/sum(ar_sfc_cpu))*reward_pts\n",
    "                                        reliability_cost = threshold_reliability*reward_pts\n",
    "                                        B_cost = link_cost + cpu_cost + reliability_cost\n",
    "                                        decay = 0.1\n",
    "                                        alpha = 1\n",
    "                                        \n",
    "                                        pos_sfc = starving_sfc_index\n",
    "                                        if pos_sfc is None:\n",
    "                                            starvation_rate = 0\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            starvation_rate = alpha*pow(decay,pos_sfc)\n",
    "                                        S_cost = starvation_rate*reward_pts\n",
    "                                        sch_rewardfunc = A_cost + B_cost + S_cost\n",
    "                                        sch_agent.store_transition(sch_obs, \n",
    "                                                                   sch_rank, \n",
    "                                                                   sch_rewardfunc, \n",
    "                                                                   sch_next_state, \n",
    "                                                                   done_sch, \n",
    "                                                                   sfc)\n",
    "                                        sch_agent.learn()\n",
    "                                        sch_sfc_end = time.perf_counter()\n",
    "\n",
    "                                    elif sch_model == 'FIFO': \n",
    "                                        sorted_sch_list = sfc_arrival_num[cnt_poisson_dist] \n",
    "                                        sch_sfc_end = time.perf_counter()\n",
    "                                    elif sch_model == 'WFQ':\n",
    "                                        if arv_per_sfc[0][1]['priority_class'] == 'high':\n",
    "                                            h_pr_queue.append(sfc)\n",
    "                                        elif arv_per_sfc[0][1]['priority_class'] == 'medium':\n",
    "                                            m_pr_queue.append(sfc)\n",
    "                                        else: \n",
    "                                            l_pr_queue.append(sfc)\n",
    "                                        sorted_sch_list = sfc_arrival_num[cnt_poisson_dist]\n",
    "                                        sch_sfc_end = time.perf_counter()\n",
    "                                    else:\n",
    "                                        #print(sch_model)\n",
    "                                        #print(\"arv_per_sfc: \", arv_per_sfc[0][1])\n",
    "                                        temp_my_dict[sfc] = arv_per_sfc[0][1]['priority_level']\n",
    "                                        #print(temp_my_dict)\n",
    "                                        sorted_sfc_sch = dict(sorted(temp_my_dict.items(), \n",
    "                                                                    key=lambda item: item[1], \n",
    "                                                                    reverse = True))\n",
    "                                        sorted_sch_list = list(sorted_sfc_sch.keys())\n",
    "                                        sch_sfc_end = time.perf_counter()\n",
    "                                        #print(\"sorted_sch_list: \", sorted_sch_list)\n",
    "                                cnt_sfc_arrival += 1\n",
    "                            cnt_poisson_dist += 1\n",
    "                            #print(sch_model)\n",
    "                            #print(\"sorted_sch_list: \", sorted_sch_list)\n",
    "                            \n",
    "                            # Adaptive scheduling\n",
    "                            if sch_model == 'DDPG':\n",
    "                                temp_sfc_deploy = []\n",
    "                                for w in sorted_sch_list:\n",
    "                                    temp_var = []\n",
    "                                    for q, p in arr_sfc_de:\n",
    "                                        if q[0][1]['sfc'] == w:\n",
    "                                            temp_var = [q,p]\n",
    "                                            temp_sfc_deploy.append(temp_var)\n",
    "                                        else:\n",
    "                                            pass\n",
    "                            elif sch_model == 'FIFO': \n",
    "                                temp_sfc_deploy = arr_sfc_de\n",
    "                            elif sch_model == 'WFQ':\n",
    "                                temp_sfc_deploy = []\n",
    "                                wfq_sch_sfc_start = time.perf_counter()\n",
    "                                sorted_sch_list = wfq_sch(arr_sfc_de, h_pr_queue, m_pr_queue, l_pr_queue)\n",
    "                                wfq_sch_sfc_end = time.perf_counter()\n",
    "                                for w in sorted_sch_list:\n",
    "                                    temp_var = []\n",
    "                                    for q, p in arr_sfc_de:\n",
    "                                        if q[0][1]['sfc'] == w:\n",
    "                                            temp_var = [q,p]\n",
    "                                            temp_sfc_deploy.append(temp_var)\n",
    "                                        else:\n",
    "                                            pass\n",
    "                            else: #priority-based!!\n",
    "                                temp_sfc_deploy = []\n",
    "                                for w in sorted_sch_list:\n",
    "                                    temp_var = []\n",
    "                                    for q, p in arr_sfc_de:\n",
    "                                        if q[0][1]['sfc'] == w:\n",
    "                                            temp_var = [q,p]\n",
    "                                            temp_sfc_deploy.append(temp_var)\n",
    "                                        else:\n",
    "                                            pass\n",
    "                            \n",
    "                            if sch_model == 'WFQ':\n",
    "                                wfq_sch_sfc = wfq_sch_sfc_end - wfq_sch_sfc_start\n",
    "                            else:\n",
    "                                wfq_sch_sfc =0\n",
    "                            \n",
    "                            sch_sfc_wt = (sch_sfc_end-sch_sfc_start)+wfq_sch_sfc\n",
    "                            #print(\"sch_sfc_wt: \", sch_sfc_wt)\n",
    "                            temp_val = [std_value for i in (range(env.network_len_nodes))]\n",
    "                            prob_val = [float(i)/sum(temp_val) for i in temp_val]\n",
    "                            # services will arrive here.\n",
    "                            num_sfc_per_arrival = 0\n",
    "                            sfc_waited_time =  0\n",
    "                            \n",
    "                            # edited on 15th sept 2022\n",
    "                            #print(\"sorted_sch_list: \", sorted_sch_list)\n",
    "                            for t_sfc in sorted_sch_list:\n",
    "                                ar_sfc_current_waitingtime[t_sfc] = sfc_waited_time\n",
    "                            temp_sorted_sch_list = sorted_sch_list.copy()\n",
    "                            ar_sorted_sch_list.append(sorted_sch_list)\n",
    "                            #print(\"ar_sfc_current_waitingtime: \", ar_sfc_current_waitingtime)\n",
    "                            #print(\"overall_expected_wt_B: \", overall_expected_wt_B)\n",
    "                            #print(\"ar_sfc_current_waitingtime: \", ar_sfc_current_waitingtime)\n",
    "                            for per_sfc, per_link in temp_sfc_deploy:\n",
    "                                done  = False\n",
    "                                sfc   = per_sfc[0][1]['sfc'] \n",
    "                                epi_temp_sfc_arrived.append(sfc)\n",
    "                                arr_traffic_packet_per_sfc = []\n",
    "                                min_samples = min_samples\n",
    "                                dict_traffic_packet_per_sfc = {}\n",
    "                                #print(\"\")\n",
    "                                #print(\"SFC\", sfc)\n",
    "                                sfc_priority_level =  per_sfc[0][1]['priority_level']\n",
    "                                sfc_priority_class =  per_sfc[0][1]['priority_class']\n",
    "                                sfc_int_flow       =  per_sfc[0][1]['int_flow_behaviour']\n",
    "                                traffic_N = 0 #None\n",
    "                                traffic_load = 0 #None\n",
    "                                temp_s = 0\n",
    "                                temp_pr_ft = [sfc, sfc_priority_level, sfc_int_flow, traffic_N, traffic_load]\n",
    "                                traffic_transition_dict[sfc] = temp_pr_ft\n",
    "                                #print(\"traffic_transition_dict: \", traffic_transition_dict)\n",
    "                                cnt_tot_sfc += 1\n",
    "                                \n",
    "                                num_of_vl = per_link[0][2]['numvl']\n",
    "                                decompmvnf = False\n",
    "                                res_exh = False\n",
    "                                #checking if the deploying sfc is within the waiting threshold\n",
    "                                #sfc_wt = per_sfc[0][1]['norm_waiting_time'] #theo_wating_time'\n",
    "                                sfc_wt = per_sfc[0][1]['theo_wating_time']\n",
    "                                sfc_rel = per_sfc[0][1]['reliability'] #reliability_factor\n",
    "                                sfc_rel_factor = per_sfc[0][1]['reliability_factor']\n",
    "                                sfc_start = time.perf_counter() \n",
    "                                traffic_pred = per_sfc[0][1]['classifier_class']\n",
    "                                traffic_load = per_sfc[0][1]['traffic_load']\n",
    "                                epi_traffic_class_gen[sfc] = traffic_pred\n",
    "                                epi_predicted_traffic_classes_arr.append(traffic_pred)\n",
    "                                epi_sfc_pr_traffic_class_all[sfc] = [sfc_priority_level, sfc_priority_class, traffic_pred]\n",
    "                                \n",
    "                                if sfc_priority_class == 'low'and traffic_load == 1:\n",
    "                                    epi_cnt_low_pr_traffic_1 += 1\n",
    "\n",
    "                                if ar_sfc_current_waitingtime[sfc] >= sfc_wt:\n",
    "                                    #print(\"Starved: Deployment time\", sfc_deployment_time)\n",
    "                                    cnt_sfc_starved_before_serving += 1\n",
    "                                    sfc_priority_class =  per_sfc[0][1]['priority_class']\n",
    "                                    sfc_starved_before_serving.append(sfc)\n",
    "                                    sfc_Prclass_starved_before_serving.append(sfc_priority_class)\n",
    "                                    #print(\"expired SFC:\", sfc, \"sfc_priority_class:\", sfc_priority_class) \n",
    "                                    #print(\"did SFC starve??\", sfc)\n",
    "                                    epi_unemb_sfc.append(sfc)\n",
    "                                    emb_status = 'starved'\n",
    "                                    sfc_deployment_time = sfc_deployment_time#\n",
    "                                    global_services.emb_update(episode, sfc, vnf_per_service, emb_status, \n",
    "                                                               sfc_deployment_time)   \n",
    "                                else:\n",
    "                                    sfc_priority_class =  per_sfc[0][1]['priority_class']\n",
    "                                    #microvnf\n",
    "                                    micro_requested_services, micro_requested_nodes = [], []\n",
    "                                    vnf_per_service = vnf_per_service\n",
    "                                    #Estimating the number of VNF persent per sfc\n",
    "                                    state_space_size = vnf_per_service\n",
    "                                    link_space_size = num_of_vl\n",
    "                                    action_space_size = env.network_len_nodes\n",
    "                                    actions_per_iteration,accpeted_action = [], []\n",
    "                                    update_actionspace, updated_path_perSFC  = [], []\n",
    "                                    reward_per_current_states, rewardlink_per_current_states = [], []\n",
    "                                    new_reward_per_current_states = []\n",
    "                                    reward_per_service, rewardDelay_per_current_states= [], []\n",
    "                                    sum_reward_per_service, score_per_service =[], []\n",
    "                                    micro_vnf_id, global_mVNF_action = [], []\n",
    "                                    global_updated_path_perVNF = []\n",
    "                                    my_dict = {}\n",
    "                                    dict_node_avail = {}\n",
    "                                    sorted_list = []\n",
    "                                    rewards = 0\n",
    "                                    reward_action = 0\n",
    "                                    global_decomp = []\n",
    "                                    emb_mVNFID = []\n",
    "                                    #Problem-solving algorithm starts\n",
    "                                    count_vnf, count_updated_path_perSFC = 0, 0\n",
    "                                    count_VNF, sum_count_mvnf_persfc = 0,0\n",
    "                                    count_mvnf_persfc = []\n",
    "                                    done_microvnf = False\n",
    "                                    for node, available_res in env.network_nodes:\n",
    "                                        my_dict[node] = available_res['emb_vnf']\n",
    "                                        sorted_action = dict(sorted(my_dict.items(), key=lambda item: item[1], reverse = False))\n",
    "                                    keys_sorted_action = sorted_action.keys()\n",
    "                                    action_space = list(map(int,keys_sorted_action))\n",
    "                                    up_actionspace = action_space\n",
    "                                    temp_sorted_action = sorted_action\n",
    "                                    vals_sorted_action = sorted_action.values()\n",
    "                                    if sum(vals_sorted_action) == 0:\n",
    "                                        break\n",
    "                                    norm_vals_sorted_action = [float(i)/sum(vals_sorted_action) for i in vals_sorted_action]\n",
    "                                    #resource got exhusted\n",
    "                                    sum_resouces = sum(list(vals_sorted_action))\n",
    "                                    nodal_avail = round(sum_resouces/tot_vnf_network, 3)\n",
    "                                    #Reachitechture\n",
    "                                    catalog_mVNF= {}\n",
    "                                    for node in original_actionspace:\n",
    "                                        catalog_mVNF[node] = []\n",
    "                                        temp= {}  \n",
    "                                    # checking the existence of the mVNF                                    \n",
    "                                    temp_accepted_action = {}\n",
    "                                    for node in original_actionspace:\n",
    "                                        temp_accepted_action[node] = []\n",
    "                                    rewards_persfc = []\n",
    "                                    for vnf,requested_res in per_sfc:\n",
    "                                        vnf_start = time.perf_counter_ns()\n",
    "                                        done_vnf = False\n",
    "                                        vnf_cnt +=1\n",
    "                                        global_services.reset(episode, sfc, vnf)\n",
    "                                        updated_path_perVNF = []\n",
    "                                        local_rewardVNF = 0\n",
    "                                        count_mvnf = 0\n",
    "                                        reward_vnfs = []                                   \n",
    "                                        if not action_space:\n",
    "                                            res_exh = True\n",
    "                                            sum_resouces = sum(list(vals_sorted_action))\n",
    "                                            break     \n",
    "                                        temp_val[action] = temp_val[action] + pref_node_val\n",
    "                                        prob_val = [float(i)/sum(temp_val) for i in temp_val]\n",
    "                                        temp_vals_sorted_action = vals_sorted_action\n",
    "                                        temp_norm_vals_sorted_action = norm_vals_sorted_action\n",
    "                                        prev_action = action\n",
    "                                        prev_vnf_res_req = vnf_res_req\n",
    "                                        vnf_res_req = requested_res['cpu_req']\n",
    "                                        pr_level = requested_res['priority_level']\n",
    "                                        sum_resouces = sum(list(vals_sorted_action))\n",
    "                                        nodal_avail = round(sum_resouces/tot_vnf_network, 3)                                     \n",
    "                                        decompose_criteria = decomp_identifier(vnf_res_req, nodal_avail, max_vnf_resource, \n",
    "                                                                               episode, sfc, vnf, vnf_per_service)\n",
    "                                        #Decomposition check 1: based on VNF resource\n",
    "                                        if vnf_res_req > threshold_decompose:\n",
    "                                            decompose_candidate = decompose_criteria.potential_vnf(threshold_decompose, \n",
    "                                                                                                   done_potential_vnf = False)\n",
    "                                            done_decompvnf = decompose_criteria.done_potential_vnf                                \n",
    "                                            global_services.update_vnf(episode,sfc, vnf,decompose_candidate, done_decompvnf)\n",
    "                                            decomp= decompose_criteria.decomp\n",
    "                                            num_micro_vnf = decompose_criteria.num_micro_vnf\n",
    "                                            if done_decompvnf == False:\n",
    "                                                pass\n",
    "                                        #Decomposition check 2: based on nodal availability \n",
    "                                        elif nodal_avail < threshold_nodal_decomp:\n",
    "                                            decompose_candidate = decompose_criteria.nodal_availability(done_nodal = False)\n",
    "                                            done_decompvnf = decompose_criteria.done_nodal\n",
    "                                            global_services.update_vnf(episode,sfc, vnf,decompose_candidate, done_decompvnf)\n",
    "                                            decomp = decompose_criteria.decomp\n",
    "                                            num_micro_vnf = decompose_criteria.num_micro_vnf\n",
    "                                            unique_id = decompose_criteria.unique_id\n",
    "                                            if unique_id:\n",
    "                                                micro_vnf_id.append(unique_id)\n",
    "                                            if done_decompvnf == False:\n",
    "                                                pass\n",
    "                                        else:\n",
    "                                            pass\n",
    "                                        reward_per_current_vnfstates = []\n",
    "                                        rewardlink_per_current_vnfstates = []\n",
    "                                        microvnf_accpeted_action = [] \n",
    "                                        t_microvnf_accpeted_action = []\n",
    "                                        micro_const = microvnf_construction(max_vnf_resource = max_vnf_resource, \n",
    "                                                                            episode = episode, \n",
    "                                                                            sfc = sfc, vnf = vnf, cpu_req = vnf_res_req, \n",
    "                                                                            n =vnf_per_service , val =0, \n",
    "                                                                            pr_level =pr_level, sfc_rel = sfc_rel, \n",
    "                                                                            traffic_pred = traffic_pred)\n",
    "                                        micro_vnf, micro_vnf_res = -1, -1\n",
    "                                        vnf_link_resources = per_link\n",
    "                                        if requested_res['microvnf_done_status'] == True:\n",
    "                                            done_mVNF = True\n",
    "                                            #print(\"==> miroserives\")\n",
    "                                            decomp =[]\n",
    "                                            global_decomp.append(decomp)\n",
    "                                            if decomp is None or not decomp:\n",
    "                                                    break\n",
    "                                            emb_micro = micro_const.microvnf_agent(episode = episode, \n",
    "                                                                                   sfc = sfc, \n",
    "                                                                                   vnf = vnf, \n",
    "                                                                                   decomp = decomp , \n",
    "                                                                                   n_actions = env.network_len_nodes, \n",
    "                                                                                   lr = learning_rate, \n",
    "                                                                                   epsilon = epsilon,\n",
    "                                                                                   mem_size = mem_size,\n",
    "                                                                                   batch_size = batch_size, \n",
    "                                                                                   vnf_cnt = vnf_cnt,\n",
    "                                                                                   gamma        = gamma,\n",
    "                                                                                   eps_dec      = eps_dec,\n",
    "                                                                                   eps_min      = eps_min,\n",
    "                                                                                   replace      = replace,\n",
    "                                                                                   state_memory = state_memory,\n",
    "                                                                                   action_memory = action_memory,\n",
    "                                                                                   reward_memory = reward_memory,\n",
    "                                                                                   next_state_memory = next_state_memory,\n",
    "                                                                                   terminal_memory = terminal_memory,\n",
    "                                                                                   Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                                                                   drop_prob = drop_prob,\n",
    "                                                                                   chkpt_dir = 'models/',\n",
    "                                                                                   algo = 'DDQNAgent',\n",
    "                                                                                   env_name = 'BtEurope',\n",
    "                                                                                   count_VNF = vnf,\n",
    "                                                                                   done = done,\n",
    "                                                                                   amount_local_reward = amount_local_reward,\n",
    "                                                                                   reward_per_current_vnfstates = reward_per_current_vnfstates,\n",
    "                                                                                   microvnf_accpeted_action = microvnf_accpeted_action, \n",
    "                                                                                   accpeted_action = accpeted_action, \n",
    "                                                                                   updated_path_perVNF = updated_path_perVNF,\n",
    "                                                                                   reward_per_current_states = reward_per_current_states,\n",
    "                                                                                   nodal_avail = nodal_avail,\n",
    "                                                                                   sum_resouces = sum_resouces,\n",
    "                                                                                   state_space_size = state_space_size, \n",
    "                                                                                   threshold_delay = threshold_delay,\n",
    "                                                                                   rewardlink_per_current_states = rewardlink_per_current_states,\n",
    "                                                                                   vnf_link_resources = vnf_link_resources, \n",
    "                                                                                   temp_accepted_action = temp_accepted_action,\n",
    "                                                                                   original_actionspace = original_actionspace, #added om 07032022\n",
    "                                                                                   mvf_t_max = mvf_t_max, \n",
    "                                                                                   mvf_t_min = mvf_t_min, \n",
    "                                                                                   mvf_old_t_current = mvf_old_t_current, \n",
    "                                                                                   new_reward_per_current_states = new_reward_per_current_states,\n",
    "                                                                                   traffic_pred = traffic_pred,\n",
    "                                                                                   traffic_load = traffic_load\n",
    "                                                                                  )\n",
    "                                            count_VNF = emb_micro[0]\n",
    "                                            reward_per_current_states = emb_micro[1]\n",
    "                                            rewardlink_per_current_states = emb_micro[2]\n",
    "                                            accpeted_action = emb_micro[3]\n",
    "                                            updated_path_perVNF = emb_micro[4]\n",
    "                                            vnf_cnt = emb_micro[5] -1\n",
    "                                        else:\n",
    "                                            #print(\"here\")\n",
    "                                            #print(\"sfc:\", sfc, \"traffic_pred\", traffic_pred, classifier_training_status)\n",
    "                                            \n",
    "                                            vals_sorted_action = list(vals_sorted_action)\n",
    "                                            micro_vnf, micro_vnf_res = -1, -1\n",
    "                                            #print(traffic_pred)\n",
    "                                            observation   = [episode,sfc, vnf,vnf_res_req, pr_level, \n",
    "                                                             micro_vnf, micro_vnf_res, traffic_pred]\n",
    "                                            state_dim     = [len(observation)]\n",
    "                                            done_mVNF     = False\n",
    "                                            action_space = up_actionspace\n",
    "                                            action = agent.choose_action(observation,\n",
    "                                                                         prob_val,\n",
    "                                                                         temp_val, \n",
    "                                                                         original_actionspace)\n",
    "                                            vnf_end = time.perf_counter_ns()\n",
    "                                            vnf_exutime = vnf_end - vnf_start\n",
    "                                            if not action in up_actionspace:\n",
    "                                                reward_action_not = -1000\n",
    "                                            else:\n",
    "                                                reward_action_not = 0    \n",
    "                                            state = vnf \n",
    "                                            subjected_to = vnf_constraints(amount_local_reward, vnf_res_req,nodal_avail,\n",
    "                                                                           max_vnf_resource, episode, sfc, vnf, vnf_per_service,\n",
    "                                                                           sum_resouces, accpeted_action, switch_decomp)\n",
    "                                            local_rewardVNF = subjected_to.constraint_1(action)\n",
    "                                            if local_rewardVNF == amount_local_reward:\n",
    "                                                pref_node_val = 10 #amount_local_reward #amount_local_reward #1\n",
    "                                            else:\n",
    "                                                pref_node_val = 0\n",
    "                                                \n",
    "                                            # Modifying Reward Function\n",
    "                                            for node, available_res in env.network_nodes:   \n",
    "                                                dict_node_avail[node] = (available_res['emb_vnf'] / tot_vnf_per_core)\n",
    "                                                \n",
    "                                            # Part 1: Reward based on the quality of Nodes selected\n",
    "                                            reward_Node_based = local_rewardVNF*dict_node_avail[str(action)]\n",
    "                                            reward_vnfs.append(reward_Node_based)  \n",
    "                                            \n",
    "                                            # Part 2: Reward based on the Priority service\n",
    "                                            reward_pr = local_rewardVNF*priority_level\n",
    "                                            reward_vnfs.append(reward_pr)\n",
    "                                            \n",
    "                                            # Part 3: Reward based on the sfc_rel\n",
    "                                            reward_rel = local_rewardVNF*sfc_rel\n",
    "                                            reward_vnfs.append(reward_rel)\n",
    "                                            \n",
    "                                            # Part 4: Reward based on the traffic_pred\n",
    "                                            #print(\"traffic_pred\", traffic_pred)\n",
    "                                            if traffic_load == None:\n",
    "                                                traffic_load = 0\n",
    "                                            else:\n",
    "                                                traffic_load = traffic_load\n",
    "\n",
    "                                            if local_rewardVNF == 1000:\n",
    "                                                reward_traffic_pred = local_rewardVNF*(traffic_pred*traffic_load)\n",
    "                                                #print(\"reward_traffic_pred:\", reward_traffic_pred, traffic_pred, local_rewardVNF)\n",
    "                                            else: \n",
    "                                                if traffic_pred == -1:\n",
    "                                                    reward_traffic_pred = -(local_rewardVNF*(traffic_pred*traffic_load))\n",
    "                                                else:\n",
    "                                                    reward_traffic_pred = (local_rewardVNF*(traffic_pred*traffic_load))\n",
    "                                            #print(\"reward_traffic_pred:\", reward_traffic_pred, traffic_pred, local_rewardVNF)\n",
    "                                            reward_vnfs.append(reward_traffic_pred)\n",
    "                                            \n",
    "                                            # Part 5: Reward based on scheduling delay\n",
    "                                            t_current = vnf_exutime\n",
    "                                            if t_max is None or  t_current >= t_max: t_max = t_current\n",
    "                                            if (t_current < t_max and t_min == 0) or (t_current <= t_min): t_min = t_current\n",
    "                                            elif (t_current >= old_t_current and t_min == 0):t_min = old_t_current\n",
    "                                            norm_time = 1 - Normalization(t_current, t_max, t_min)\n",
    "                                            time_reward = local_rewardVNF*norm_time\n",
    "                                            reward_vnfs.append(time_reward)\n",
    "                                            old_t_current = t_current\n",
    "                                            total_reward_vnf = round(sum(reward_vnfs), 4)\n",
    "                                            new_reward_per_current_states.append(total_reward_vnf)\n",
    "                                            if local_rewardVNF != -1000:\n",
    "                                                local_rewardVNF = local_rewardVNF\n",
    "                                                reward_per_current_states.append(local_rewardVNF)\n",
    "                                            else:\n",
    "                                                local_rewardVNF = local_rewardVNF \n",
    "                                                if switch_decomp is False:\n",
    "                                                    decomp = None\n",
    "                                                    decomp = []\n",
    "                                                else: \n",
    "                                                    decomp = subjected_to.decomp\n",
    "                                                    decomp = decomp    \n",
    "                                                global_decomp.append(decomp)\n",
    "                                                decompmvnf = True\n",
    "                                                if decomp is None or not decomp:\n",
    "                                                    global_reward = 0\n",
    "                                                    #print(\"==> no decomp possible\")\n",
    "                                                    local_reward = local_rewardVNF\n",
    "                                                    rewards = total_reward_vnf + global_reward\n",
    "                                                    agent.store_transition(observation, action, \n",
    "                                                                           rewards, next_state, \n",
    "                                                                           done, vnf_cnt)\n",
    "                                                    agent.learn()\n",
    "                                                    epsilon = agent.epsilon\n",
    "                                                    break\n",
    "                                                else:\n",
    "                                                    #print(\"VNF:\", vnf)\n",
    "                                                    #print(\"decomp\")\n",
    "                                                    local_rewardVNF = 1000 \n",
    "                                                    unique_id = decomp[2]\n",
    "                                                    if unique_id:\n",
    "                                                        micro_vnf_id.append(unique_id)    \n",
    "                                                for item in catalog_mVNF.items():\n",
    "                                                    #print(\"item\", item)\n",
    "                                                    mVNFID_temp = []\n",
    "                                                    for k in item[1]:\n",
    "                                                        repeat_mVNFid = {e for e in unique_id if e == k}\n",
    "                                                        mVNFID_temp = temp_accepted_action[item[0]]\n",
    "                                                        if(repeat_mVNFid):\n",
    "                                                            if k in mVNFID_temp:\n",
    "                                                                mVNFID_temp = mVNFID_temp\n",
    "                                                            else:\n",
    "                                                                mVNFID_temp = mVNFID_temp + [k]\n",
    "                                                        temp_accepted_action[item[0]] = mVNFID_temp\n",
    "                                                count_mvnf = len(unique_id)\n",
    "                                                count_mvnf_persfc.append(count_mvnf)\n",
    "                                                prev_maction = False\n",
    "                                                emb_micro = micro_const.microvnf_agent(episode = episode, \n",
    "                                                                   sfc = sfc, \n",
    "                                                                   vnf = vnf, \n",
    "                                                                   decomp = decomp , \n",
    "                                                                   n_actions = env.network_len_nodes, \n",
    "                                                                   lr = learning_rate, \n",
    "                                                                   epsilon = epsilon,\n",
    "                                                                   mem_size = mem_size,\n",
    "                                                                   batch_size = batch_size, \n",
    "                                                                   vnf_cnt = vnf_cnt,\n",
    "                                                                   gamma        = gamma,\n",
    "                                                                   eps_dec      = eps_dec,\n",
    "                                                                   eps_min      = eps_min,\n",
    "                                                                   replace      = replace,\n",
    "                                                                   state_memory = state_memory,\n",
    "                                                                   action_memory = action_memory,\n",
    "                                                                   reward_memory = reward_memory,\n",
    "                                                                   next_state_memory = next_state_memory,\n",
    "                                                                   terminal_memory = terminal_memory,\n",
    "                                                                   Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                                                   drop_prob =drop_prob,\n",
    "                                                                   chkpt_dir = 'models/',\n",
    "                                                                   algo = 'DDQNAgent',\n",
    "                                                                   env_name = 'BtEurope',\n",
    "                                                                   count_VNF = vnf,\n",
    "                                                                   done = done,\n",
    "                                                                   amount_local_reward = amount_local_reward,\n",
    "                                                                   reward_per_current_vnfstates = reward_per_current_vnfstates,\n",
    "                                                                   microvnf_accpeted_action = microvnf_accpeted_action, \n",
    "                                                                   t_microvnf_accpeted_action = t_microvnf_accpeted_action, \n",
    "                                                                   accpeted_action = accpeted_action, \n",
    "                                                                   updated_path_perVNF = updated_path_perVNF,\n",
    "                                                                   reward_per_current_states = reward_per_current_states,\n",
    "                                                                   nodal_avail = nodal_avail,\n",
    "                                                                   sum_resouces = sum_resouces,\n",
    "                                                                   state_space_size = state_space_size, \n",
    "                                                                   threshold_delay = threshold_delay,\n",
    "                                                                   rewardlink_per_current_states = rewardlink_per_current_states,\n",
    "                                                                   vnf_link_resources = vnf_link_resources,\n",
    "                                                                   temp_accepted_action = temp_accepted_action,\n",
    "                                                                   prob_val = prob_val,\n",
    "                                                                   temp_val = temp_val,\n",
    "                                                                   original_actionspace = original_actionspace, #added on 07032022\n",
    "                                                                   prev_maction = prev_maction,\n",
    "                                                                   min_pr = min_pr, #added on 13th April 2022\n",
    "                                                                   max_pr = max_pr,  #added on 13th April 2022\n",
    "                                                                   mvf_t_max = mvf_t_max, \n",
    "                                                                   mvf_t_min = mvf_t_min, \n",
    "                                                                   mvf_old_t_current = mvf_old_t_current, \n",
    "                                                                   new_reward_per_current_states = new_reward_per_current_states,\n",
    "                                                                   traffic_pred = traffic_pred,\n",
    "                                                                   traffic_load = traffic_load\n",
    "                                                                   )\n",
    "                                                mvf_t_max = emb_micro[7]\n",
    "                                                mvf_t_min = emb_micro[8]\n",
    "                                                new_reward_per_current_states = emb_micro[10]\n",
    "                                                empt = []\n",
    "                                                temp_count = 0\n",
    "                                                for key in  microvnf_accpeted_action:\n",
    "                                                    if type(key) is list:\n",
    "                                                        pass\n",
    "                                                    else:\n",
    "                                                        empt = [unique_id[temp_count]]\n",
    "                                                        catalog_mVNF[key] = unique_id[temp_count]  \n",
    "                                                        if key in temp.keys():\n",
    "                                                            empt = empt + temp[key]  \n",
    "                                                        catalog_mVNF[key] = empt       \n",
    "                                                        temp = dict(sorted(catalog_mVNF.items(), \n",
    "                                                                           key=lambda item: item[1], \n",
    "                                                                           reverse = False)) \n",
    "                                                    temp_count +=1\n",
    "                                                count_VNF = emb_micro[0]\n",
    "                                                #print(\"count_VNF: \", count_VNF)\n",
    "                                                reward_per_current_states = emb_micro[1]\n",
    "                                                rewardlink_per_current_states = emb_micro[2]\n",
    "                                                accpeted_action = emb_micro[3]\n",
    "                                                updated_path_perVNF = emb_micro[4]\n",
    "                                                vnf_cnt = emb_micro[5]-1\n",
    "                                                global_mVNF_action.append(microvnf_accpeted_action)\n",
    "                                                global_updated_path_perVNF.append(updated_path_perVNF)    \n",
    "                                            #Constraint 3 and 4: : checking for the available resouce for Links#\n",
    "                                            vnf_link_resources = per_link \n",
    "                                            env_networkgraph   = env.network_graph\n",
    "                                            env_networkedges   = env.network_edges\n",
    "                                            subjected_to.constraint_3(state_space_size, \n",
    "                                                                   vnf_link_resources,\n",
    "                                                                   env_networkgraph, \n",
    "                                                                   env_networkedges, \n",
    "                                                                   threshold_delay, \n",
    "                                                                   updated_path_perSFC\n",
    "                                                                   )\n",
    "                                            rewardlink_per_current_states = subjected_to.local_reward_link\n",
    "                                            updated_path_perSFC = subjected_to.updated_path_perSFC\n",
    "                                            #Next state\n",
    "                                            next_state = vnf+1\n",
    "                                            if next_state >= state_space_size:\n",
    "                                                next_state = 0\n",
    "                                                next_traffic_pred = -1\n",
    "                                                next_pr_level = round(rand.uniform(0,1), 2)\n",
    "\n",
    "                                                if len(sorted_sch_list) == num_sfc_per_arrival+1: \n",
    "                                                    next_sfc = max(sorted_sch_list)+1\n",
    "                                                else: \n",
    "                                                    next_sfc = sorted_sch_list[num_sfc_per_arrival+1]\n",
    "\n",
    "                                                next_observation = [a, next_sfc,next_state,vnf_res_req, next_pr_level, \n",
    "                                                                    micro_vnf, micro_vnf_res, next_traffic_pred]\n",
    "                                            else: \n",
    "                                                next_state = next_state\n",
    "                                                next_observation = [a, sfc,next_state,vnf_res_req, pr_level, \n",
    "                                                                    micro_vnf, micro_vnf_res, traffic_pred]\n",
    "                                            state = observation \n",
    "                                            next_state = next_observation\n",
    "                                        #Reward Function\n",
    "                                        global_reward = 0\n",
    "                                        if (sum(reward_per_current_states) == amount_local_reward*state_space_size) and \\\n",
    "                                            (sum(rewardlink_per_current_states) == amount_local_reward*link_space_size):\n",
    "                                            done = True\n",
    "                                            deployed_sfc_per_episode.append(sfc)\n",
    "                                            deployed_priority_per_episode.append(pr_level)\n",
    "                                            vnf_node_resources = per_sfc \n",
    "                                            vnf_link_resources = per_link \n",
    "                                            reward_func.reward_vnf(global_reward,\n",
    "                                                                   rewardDelay_per_current_states, \n",
    "                                                                   rewardlink_per_current_states, \n",
    "                                                                   actions_per_iteration,\n",
    "                                                                   accpeted_action,\n",
    "                                                                   vnf_node_resources, \n",
    "                                                                   count_vnf, \n",
    "                                                                   updated_path_perSFC,\n",
    "                                                                   vnf_link_resources, \n",
    "                                                                   count_updated_path_perSFC,\n",
    "                                                                   count_success,\n",
    "                                                                   global_decomp,\n",
    "                                                                   emb_mVNFID,\n",
    "                                                                   switch_reconst,\n",
    "                                                                   global_updated_path_perVNF)\n",
    "                                            count_updated_path_perSFC = reward_func.count_updated_path_perSFC\n",
    "                                            count_VNF = reward_func.count_vnf\n",
    "                                            global_reward = reward_func.global_reward\n",
    "                                        local_reward = local_rewardVNF\n",
    "                                        rewards = total_reward_vnf + global_reward\n",
    "                                        rewards_persfc.append(rewards)\n",
    "                                        if done_mVNF == True:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            #storing the transistions\n",
    "                                            agent.store_transition(state, action, rewards, next_state, done, vnf_cnt)\n",
    "                                            #learning:  providing state, action, reward and next state to the NN for learning.\n",
    "                                            agent.learn()\n",
    "                                            epsilon = agent.epsilon\n",
    "                                    sum_rewards_persfc = sum(rewards_persfc)\n",
    "                                    rewards_perepi.append(sum_rewards_persfc)\n",
    "                                    \n",
    "                                    if state_space_size == count_VNF and link_space_size == count_updated_path_perSFC:\n",
    "                                        if decompmvnf is True:\n",
    "                                            count_succes_decompvnf +=1\n",
    "                                        sum_count_mvnf_persfc = sum(count_mvnf_persfc)\n",
    "                                        count_mvnf_perepi.append(sum_count_mvnf_persfc)\n",
    "                                        count_success += 1\n",
    "                                        count_accpeted +=1\n",
    "                                        #print(\"sfc: \",sfc, \" priority: \", sfc_priority_level , \" predc class: \", traffic_pred)\n",
    "                                        epi_sfc_pr_traffic_class_emb[sfc] = [sfc_priority_level, sfc_priority_class, traffic_pred]\n",
    "                                    \n",
    "                                        \n",
    "                                        if sfc_priority_class == \"high\":\n",
    "                                            cnt_High_pr_emb += 1\n",
    "                                            if traffic_pred == 1:\n",
    "                                                cnt_High_traffic1_emb += 1\n",
    "                                            else:\n",
    "                                                cnt_High_traffic0_emb += 1\n",
    "                                                \n",
    "                                        elif sfc_priority_class == \"medium\":\n",
    "                                            cnt_Medium_pr_emb += 1\n",
    "                                            if traffic_pred == 1:\n",
    "                                                cnt_Medium_traffic1_emb += 1\n",
    "                                            else:\n",
    "                                                cnt_Medium_traffic0_emb += 1\n",
    "                                        else:\n",
    "                                            cnt_Low_pr_emb += 1\n",
    "                                            if traffic_pred == 1:\n",
    "                                                cnt_Low_traffic1_emb += 1\n",
    "                                            else: \n",
    "                                                cnt_Low_traffic0_emb += 1\n",
    "                                                \n",
    "                                            \n",
    "                                        epi_emb_sfc.append(sfc) #array of embedded SFCs\n",
    "                                        if sfc_rel >= 0.7:\n",
    "                                            epi_emb_sfc_h_rel.append(sfc)\n",
    "                                            if sfc_priority_class == 'high':\n",
    "                                                cnt_rel += 1\n",
    "                                        #print(cnt_rel)\n",
    "                                        \n",
    "                                        # update emd status: added 0n 11jan 2023\n",
    "                                        emb_status = True                                        \n",
    "                                    else:\n",
    "                                        # update emd status: added 0n 11jan 2023\n",
    "                                        epi_unemb_sfc.append(sfc)\n",
    "                                        emb_status = False\n",
    "                                        count_unsucces_decompvnf += 1\n",
    "                                        count_mvnf_perepi.append(0)\n",
    "                                    if res_exh is True:\n",
    "                                        res_exhausted +=1\n",
    "                                    num_sfc_per_arrival +=1\n",
    "                                    \n",
    "                                    sfc_end = time.perf_counter() #_ns()\n",
    "                                    sfc_deployment_time = round(sfc_end-sfc_start,6) # time take by the agent to deploy the agent\n",
    "                                    #print(\"SFC: \", sfc, sfc_deployment_time)\n",
    "                                    \n",
    "                                    # update emd status: added 0n 11jan 2023\n",
    "                                    global_services.emb_update(episode, sfc, vnf_per_service, emb_status, \n",
    "                                                               sfc_deployment_time)\n",
    "\n",
    "                                    if switch_sfc_expiring_check == True:\n",
    "                                        for t_sfc in temp_sorted_sch_list: \n",
    "                                            if sfc == t_sfc:\n",
    "                                                expected_wt_C = 0\n",
    "                                                ar_sfc_current_waitingtime[sfc] += expected_wt_C\n",
    "                                            else: \n",
    "                                                expected_wt_C = sfc_deployment_time\n",
    "                                                ar_sfc_current_waitingtime[t_sfc] += expected_wt_C\n",
    "                                        #print(\"expected_wt_C: \", expected_wt_C)\n",
    "                                        temp_sorted_sch_list.remove(sfc)\n",
    "                                    #else: #commented on 15th sept 2022\n",
    "                                     #   pass # commented on 15th sept 2022\n",
    "                                    #print(ar_sfc_current_waitingtime)   \n",
    "                                    network_overall_accepted_action.append(accpeted_action)\n",
    "                                    count_rejected = len(epi_temp_sfc_arrived) - count_accpeted\n",
    "                                    count_failed   = len(epi_temp_sfc_arrived) - count_success \n",
    "                                    \n",
    "                                    #episode_pr_rewardfunc.append(sfc_pr_rewardfunc)\n",
    "                                    #print(\"sfc_priority: \", sfc_priority)\n",
    "                                    episode_priority.append(sfc_priority)\n",
    "                                    #episode_envpriority.append(sfc_envpriority)\n",
    "                                    episode_priority_err.append(sfc_priority_err)   \n",
    "                        \n",
    "                            #print(\"epi_expected_wt_B: \", epi_expected_wt_B)\n",
    "                            #print(\"episode\", episode)\n",
    "                            #overall_expected_wt_B[episode] = epi_expected_wt_B\n",
    "                            #print(\"overall_expected_wt_B: \", overall_expected_wt_B)\n",
    "                        #print(\"epi_traffic_class_gen: \", epi_traffic_class_gen, len(epi_traffic_class_gen))\n",
    "                        #print(\"epi_predicted_traffic_classes_arr: \", epi_predicted_traffic_classes_arr)\n",
    "                        count_class_0_predicted_traffic = epi_predicted_traffic_classes_arr.count(0)\n",
    "                        count_class_1_predicted_traffic = epi_predicted_traffic_classes_arr.count(1)\n",
    "                        #print(count_class_0_predicted_traffic, count_class_1_predicted_traffic)\n",
    "                        epi_predicted_traffic_classes_dict[0] = count_class_0_predicted_traffic\n",
    "                        epi_predicted_traffic_classes_dict[1] = count_class_1_predicted_traffic\n",
    "                        #print(\"epi_sfc_pr_traffic_class_emb: \", epi_sfc_pr_traffic_class_emb)\n",
    "                        \n",
    "                        overall_epi_predicted_traffic_classes_dict[a] =  epi_predicted_traffic_classes_dict   \n",
    "                        overall_epi_sfc_pr_traffic_class_emb[a] = epi_sfc_pr_traffic_class_emb \n",
    "                        overall_epi_sfc_pr_traffic_class_all[a] = epi_sfc_pr_traffic_class_all\n",
    "                             \n",
    "                        overall_epi_traffic_class_gen[a] = epi_traffic_class_gen\n",
    "                        overall_ar_sorted_sch_list.append(ar_sorted_sch_list)\n",
    "                        epi_cnt_sfc_starved_before_serving.append(cnt_sfc_starved_before_serving)\n",
    "                        epi_cnt_sfc_Prclass_starved_before_serving.append(cnt_sfc_starved_before_serving)\n",
    "                        overall_cnt_sfc_starved_before_serving.append(sfc_starved_before_serving)\n",
    "                        overall_cnt_sfc_Prclass_starved_before_serving.append(sfc_Prclass_starved_before_serving)\n",
    "                        \n",
    "                        overall_epi_cnt_High_pr_emb.append(cnt_High_pr_emb)\n",
    "                        overall_epi_cnt_Medium_pr_emb.append(cnt_Medium_pr_emb)\n",
    "                        overall_epi_cnt_Low_pr_emb.append(cnt_Low_pr_emb)\n",
    "                        \n",
    "                        epi_pr_emb[\"H\"] = cnt_High_pr_emb\n",
    "                        epi_pr_emb[\"M\"] = cnt_Medium_pr_emb\n",
    "                        epi_pr_emb[\"L\"] = cnt_Low_pr_emb\n",
    "                        \n",
    "                        overall_epi_pr_emb.append(epi_pr_emb)\n",
    "                        overall_unemb_sfc.append(epi_unemb_sfc)\n",
    "                        overall_emb_sfc.append(epi_emb_sfc)\n",
    "                        overall_emb_sfc_h_rel.append(epi_emb_sfc_h_rel)\n",
    "                        \n",
    "                        overall_epi_cnt_High_traffic0_emb.append(cnt_High_traffic0_emb)                      \n",
    "                        overall_epi_cnt_Medium_traffic0_emb.append(cnt_Medium_traffic0_emb)\n",
    "                        overall_epi_cnt_Low_traffic0_emb.append(cnt_Low_traffic0_emb)\n",
    "                        overall_epi_cnt_High_traffic1_emb.append(cnt_High_traffic1_emb) \n",
    "                        overall_epi_cnt_Medium_traffic1_emb.append(cnt_Medium_traffic1_emb) \n",
    "                        overall_epi_cnt_Low_traffic1_emb.append(cnt_Low_traffic1_emb)\n",
    "                        overall_epi_cnt_low_pr_traffic_1.append(epi_cnt_low_pr_traffic_1)\n",
    "                        \n",
    "                    \n",
    "\n",
    "                        #print(\"overall_epi_pr_emb: \", overall_epi_pr_emb)\n",
    "                        if len(overall_sfc_priority) != len(global_services.vNetwork_service_details_er):\n",
    "                            overall_sfc_priority.append(sfc_priority)\n",
    "                            epi_cnt_High_pr.append(cnt_High_pr)\n",
    "                            epi_cnt_Medium_pr.append(cnt_Medium_pr)\n",
    "                            epi_cnt_Low_pr.append(cnt_Low_pr)\n",
    "                            epi_pr[\"H\"] = cnt_High_pr\n",
    "                            epi_pr[\"M\"] = cnt_Medium_pr\n",
    "                            epi_pr[\"L\"] = cnt_Low_pr\n",
    "                            #print(\"epi_pr: \", epi_pr)\n",
    "                            #print(\"cnt_High_pr: \", cnt_High_pr, \"cnt_Medium_pr: \", cnt_Medium_pr, \"cnt_Low_pr: \", cnt_Low_pr)\n",
    "                            overall_epi_pr.append(epi_pr)\n",
    "                            status_cnt_ridge = False\n",
    "                            copy_overall_epi_pr = overall_epi_pr.copy()\n",
    "                        else:\n",
    "                            status_cnt_ridge = True\n",
    "                            copy_overall_episode_envpriority = overall_episode_envpriority.copy()\n",
    "                            #print(\"copy_overall_epi_pr: \",copy_overall_epi_pr)\n",
    "                            #print(\"XXXX\")\n",
    "                        \n",
    "                        overall_episode_priority.append(episode_priority)\n",
    "                        if sch_model == 'DDPG':  \n",
    "                            overall_episode_envpriority.append(sfc_envpriority)\n",
    "                        else: \n",
    "                            copy_overall_episode_envpriority = overall_episode_envpriority.copy()\n",
    "                            \n",
    "                        overall_episode_priority_err.append(episode_priority_err)\n",
    "                        overall_deployed_sfc_per_episode.append(deployed_sfc_per_episode)\n",
    "                        overall_deployed_priority_per_episode.append(deployed_priority_per_episode)\n",
    "                        sum_rewards_perepi = sum(rewards_perepi)\n",
    "                        network_overall_sum_rewards_perepi.append(sum_rewards_perepi)\n",
    "                        sum_count_mvnf_perepi = sum(count_mvnf_perepi)\n",
    "                        network_overall_sum_mvnf.append(sum_count_mvnf_perepi)\n",
    "                        network_overall_count_mvnf.append(count_mvnf_perepi)\n",
    "                        network_overall_accepted.append(count_success)\n",
    "                        network_overall_rejected.append(count_failed)\n",
    "                        tot_decompsfc = (count_succes_decompvnf + count_unsucces_decompvnf) \n",
    "                        undecomp_accepted = num_sfc-tot_decompsfc\n",
    "                        undecomp_rejected = tot_decompsfc\n",
    "                        monolithic_deploy_success.append(undecomp_accepted)\n",
    "                        monolithic_deploy_rejected.append(undecomp_rejected)\n",
    "                        overall_res_exhausted.append(res_exhausted)\n",
    "                        network_overall_tot_decompsfc.append(tot_decompsfc)\n",
    "                        network_overall_count_succes_decompvnf.append(count_succes_decompvnf)\n",
    "                        network_overall_count_unsucces_decompvnf.append(count_unsucces_decompvnf)\n",
    "                        overall_epi_traffic_packets_dict[a] = epi_traffic_packets_dict\n",
    "                        overall_epi_traffic_load_dict[a] = epi_traffic_load_dict\n",
    "                        \n",
    "                        if switch_priorty == True:\n",
    "                            if pr_model == 'DDPG' or pr_model == 'DDQN': \n",
    "                                pass\n",
    "                            else:\n",
    "                                if done_status == \"Train\":\n",
    "                                    cnt_macro_pr = tr_count_macro_pr\n",
    "                                    acc = round(tr_macro_acc,2)\n",
    "                                    epi_cnt_macro_pr = epi_tr_count_macro_pr\n",
    "                                    epi_acc = round((epi_cnt_macro_pr/num_sfc)*100,2)\n",
    "                                    epi_tr_correct_perc.append(epi_acc)\n",
    "                                elif done_status == \"Eval\":\n",
    "                                    cnt_macro_pr = eval_count_macro_pr\n",
    "                                    acc = eval_macro_acc\n",
    "                                    epi_cnt_macro_pr = epi_eval_count_macro_pr\n",
    "                                    epi_acc = round((epi_cnt_macro_pr/num_sfc)*100,2)\n",
    "                                else:\n",
    "                                    cnt_macro_pr = pred_count_macro_pr\n",
    "                                    acc = pred_macro_acc\n",
    "                                    epi_cnt_macro_pr = epi_pred_count_macro_pr\n",
    "                                    epi_acc = round((epi_cnt_macro_pr/num_sfc)*100,2)\n",
    "                                    epi_pred_correct_perc.append(epi_acc)\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                        if switch_clustering is True:\n",
    "                            #pass\n",
    "                            if a%num_episode==0:\n",
    "                                pass\n",
    "                                #clusterplot_val = class_traffic_clustering.clusteringplot(epi_clustering_status, \n",
    "                                #                                                          traffic_input_dataset, \n",
    "                                #                                                          cluster_label_val)\n",
    "\n",
    "                          \n",
    "                        print(\"*************\")\n",
    "                        print(\"cnt_sfc_arrival: \", cnt_sfc_arrival)\n",
    "                        print(\"Total Services for Episode\",a,\"is\",len(epi_temp_sfc_arrived), \"Num accepted\",count_success,\"Num rejected\",count_failed)\n",
    "                        #print(\"Episode\",a,\" Done Status: \", done_status,\" count_macro_pr: \", cnt_macro_pr,\" Epi count_macro_pr: \", epi_cnt_macro_pr, \"Epi Acc: \", epi_acc, acc)\n",
    "                        #print('Mean MAE: %.3f (%.3f)' % (m(scores), std(scores)))\n",
    "                        #print(\"Episode\",a,\" count_macro_pr: \", count_macro_pr, \" Episode count macro: \", epi_cnt_macro_pr)\n",
    "                        count_numVNF_perNode = []\n",
    "                        count_num_mVNF_perNode = []\n",
    "                        for ac in [i for i in range(env.network_len_nodes)]:\n",
    "                            count =0\n",
    "                            count_mVNF = 0\n",
    "                            for key in network_overall_accepted_action:\n",
    "                                for action in key:\n",
    "                                    if type(action) is list:\n",
    "                                        for maction in action:\n",
    "                                            if maction is ac:\n",
    "                                                count_mVNF += 1\n",
    "                                    else:\n",
    "                                        if action is ac:\n",
    "                                            count +=1\n",
    "                            count_numVNF_perNode.append(count)\n",
    "                            count_num_mVNF_perNode.append(count_mVNF)\n",
    "                        overall_count_numVNF_perNode.append(count_numVNF_perNode)\n",
    "                        overall_count_num_mVNF_perNode.append(count_num_mVNF_perNode)\n",
    "                        qq = list(env.network_graph.nodes(data=True))\n",
    "                        remain_cpu = []\n",
    "                        for key, val in qq:\n",
    "                            cpu = val['emb_vnf']\n",
    "                            #print(key, cpu)\n",
    "                            remain_cpu.append(cpu)\n",
    "                        sum_remain_cpu = sum(remain_cpu)\n",
    "                        #remaining BW\n",
    "                        residual_bw_res = []\n",
    "                        for key in env.network_edges: \n",
    "                            residual_bw_res.append(key[2]['bw'])\n",
    "                        overall_remain_cpu.append(sum_remain_cpu)\n",
    "                        overall_remain_bw.append(residual_bw_res)\n",
    "                        #print(\"epsilon\", round(epsilon, 2))\n",
    "                        network_overall_epsilon.append(round(epsilon, 2))\n",
    "                        a += 1\n",
    "                    \n",
    "                    #print(\"overall_epi_traffic_class_gen[a]:\", overall_epi_traffic_class_gen)    \n",
    "                    \n",
    "                    from statistics import mean \n",
    "                    mean_network_overall_accepted = mean(network_overall_accepted)\n",
    "                    end = time.time()  \n",
    "                    runtime = end-start\n",
    "                    runtime = round(runtime,2)\n",
    "                    print(\"runtime\",runtime)\n",
    "                    topology_name = str('Netrail')\n",
    "\n",
    "                    overall_res  = []\n",
    "                    sum_overall_res = 0\n",
    "                    for per_episode in global_services.vNetwork_service_details_er:\n",
    "                        res_perepi = []\n",
    "                        sum_res_perepi = 0\n",
    "                        for per_sfc, per_link in per_episode:\n",
    "                            res_persfc = []\n",
    "                            sum_res_persfc = 0\n",
    "                            #print(per_sfc)\n",
    "                            for vnf, val in per_sfc:\n",
    "                                res_persfc.append(val['cpu_req'])\n",
    "                            #print(res_persfc)  \n",
    "                            sum_res_persfc = sum(res_persfc)\n",
    "                            #print(sum_res_persfc)\n",
    "                            res_perepi.append(sum_res_persfc)\n",
    "                        #print(res_perepi)\n",
    "                        sum_res_perepi = sum(res_perepi)\n",
    "                        #print(sum_res_perepi)\n",
    "                        overall_res.append(sum_res_perepi)\n",
    "                    sum_overall_res = sum(overall_res)\n",
    "\n",
    "                    upper_bound = round(tot_avail_res_network/min(overall_res), 2)\n",
    "                    lower_bound = round(tot_avail_res_network/max(overall_res),2)\n",
    "\n",
    "                    theo_bound  = [lower_bound, upper_bound]\n",
    "                    #print(theo_bound)\n",
    "\n",
    "                    Bound_Cal = ['overall_res: ' +str(overall_res), 'sum_overall_res: ' +str(sum_overall_res),\\\n",
    "                         'theo_bound: '  +str(theo_bound), 'min: ' +str(min(overall_res)), 'max: ' +str(max(overall_res))             \n",
    "                        ]\n",
    "                    #print(Bound_Cal)\n",
    "                    \n",
    "                    Epsilon = 1\n",
    "                    overall_vnf = (vnf_per_service*num_sfc)*num_episode\n",
    "                    Information = 'Information: '\n",
    "                    experience_replay = ['Experience Replay:',('Memory Size: '+str(mem_size)), ('Batch Size: '+ str(batch_size)), ('Replace: '+ str(replace))]\n",
    "                    topologyname = ['Tolpology:'+ topology_name, ('Node:'+ str(env.network_len_nodes)), ('Link:' + str(env.len_tot_links))]\n",
    "                    execution = [('Episode: '+ str(num_episode)), ('SFC: ' + str(num_sfc)), ('VNF per SFC: '+ str(vnf_per_service))]\n",
    "                    Info = [Information] + [Algorithm]+ [topologyname] + [Opimizer] + [execution]\n",
    "                    #print(Info)    \n",
    "                    Parameters = 'Parameters: '\n",
    "                    NN_para = ['Neural Network: '+ ('Neuron units: '+ str(Neurons_per_HiddenLayer)), 'Hidden Layer: '+str(Hidden_layer),\\\n",
    "                               'Drop Probability: '+ str(drop_prob)]\n",
    "                    learn_para = ['Learning :'+ ('Learning Rate: ' + str(learning_rate)), ('Epsilon: '+ str(Epsilon)), ('Gamma: '+ str(gamma)),\\\n",
    "                                 ('Epsilon Dec: '+ str(eps_dec)), 'Epsilon Min: '+ str(eps_min), Opimizer]\n",
    "                    RL_para = ['RL rewards function: ' + ('Reward: ' +str(amount_local_reward)) , ('Penalty: ' + str(amount_local_penalty))]\n",
    "\n",
    "                    Decomp = ['Decomp: ' +'switch_reconst: ' +str(switch_reconst),'switch_decomp: ' + str(switch_decomp),\\\n",
    "                              'n_state: ' + str(n_state), ' with No Constraint 2 '\n",
    "                             ]\n",
    "                    para = [Parameters] + [NN_para] + [learn_para]  + [experience_replay] + [RL_para] + [Decomp]\n",
    "                    Topology_resource_init = ['Topology Resouce Initilization: '+ ('Latency: ' + str(threshold_delay)),\\\n",
    "                                              'CPU core: ' + str(core_cpu), 'VNF per Core: ' + str(num_vnf_per_core),\\\n",
    "                                              'Total VNF per core' + str(tot_vnf_per_core)]\n",
    "                    Nodal_outage_para = ['Nodal Outage: ' + 'Outage Percentage: '+str(nodal_outage_percentage),\\\n",
    "                                         'No of nodes outaged: ' + str(round_nodal_outage)]\n",
    "                    # Result\n",
    "                    computation_time = 'Runtime: '+str(runtime)+ ' seconds'\n",
    "                    results = ['Results: ' + computation_time, \n",
    "                               'Total Generated VNF: ' + str(overall_vnf),'Mean of Overall VNF: '+ str(mean_network_overall_accepted),\\\n",
    "                               'SAR: '+str(network_overall_accepted), 'Monolithic_deploy_success: ' + str(monolithic_deploy_success),\\\n",
    "                               'Monolithic_deploy_rejected: ' + str(monolithic_deploy_rejected),\\\n",
    "                               'Network_overall_tot_decompsfc: ' + str(network_overall_tot_decompsfc),\\\n",
    "                               'Network_overall_count_succes_decompvnf: ' + str(network_overall_count_succes_decompvnf),\\\n",
    "                               'network_overall_count_unsucces_decompvnf: ' + str(network_overall_count_unsucces_decompvnf),\\\n",
    "                               'network_overall_sum_mvnf: ' + str(network_overall_sum_mvnf),\\\n",
    "                               'network_overall_count_mvnf: ' +str(network_overall_count_mvnf),\\\n",
    "                               'network_overall_sum_rewards_perepi: '+str(network_overall_sum_rewards_perepi),\\\n",
    "                               'network_overall_epsilon: '+ str(network_overall_epsilon)\n",
    "                              ]\n",
    "                    #print(results)\n",
    "                    resource_residual = ['Resource Residual: ' + 'VNF residual: '+ str(overall_remain_cpu) +\\\n",
    "                                         'Link Residual: '+ str(overall_remain_bw) + \\\n",
    "                                         #'VNF Distribution: '+str(Network_count_numVNF_perNode),\\\n",
    "                                         'overall_count_numVNF_perNode: ' + str(overall_count_numVNF_perNode),\\\n",
    "                                         'overall_count_num_mVNF_perNode: ' + str(overall_count_num_mVNF_perNode),\\\n",
    "                                         'overall_res_exhausted: '+ str(overall_res_exhausted)\n",
    "                                        ]\n",
    "                    priority = [' Priority: ' + 'pr_HL: ' +str(pr_HL) + \\\n",
    "                                ' overall_episode_pr_rewardfunc: ' + str(overall_episode_pr_rewardfunc)+\\\n",
    "                                ' overall_sfc_priority: ' +str(overall_sfc_priority)+\\\n",
    "                                ' overall_episode_envpriority: '+ str(overall_episode_envpriority)+\\\n",
    "                                ' copy_overall_episode_envpriority: ' + str(copy_overall_episode_envpriority)+\\\n",
    "                                ' epi_cnt_sfc_starved_before_serving: ' + str( epi_cnt_sfc_starved_before_serving)+\\\n",
    "                                ' overall_cnt_sfc_starved_before_serving: ' + str(overall_cnt_sfc_starved_before_serving)+\\\n",
    "                                ' epi_cnt_sfc_Prclass_starved_before_serving: ' + str( epi_cnt_sfc_Prclass_starved_before_serving)+\\\n",
    "                                ' overall_cnt_sfc_Prclass_starved_before_serving: ' + str(overall_cnt_sfc_Prclass_starved_before_serving)+\\\n",
    "                                ' copy_overall_epi_pr: ' + str(copy_overall_epi_pr)+\\\n",
    "                                ' overall_epi_pr: ' + str(overall_epi_pr)+\\\n",
    "                                ' overall_epi_pr_emb: ' + str(overall_epi_pr_emb)\n",
    "                                #'over_count_micro_pr:' +str(over_count_micro_pr)\n",
    "                               ]\n",
    "                    \n",
    "                    #print(\"overall_emb_sfc: \", overall_emb_sfc, \" overall_emb_sfc_h_rel:\", overall_emb_sfc_h_rel)\n",
    "                    #print(\"overall_ar_sorted_sch_list: \", overall_ar_sorted_sch_list)\n",
    "                    embeded_sfc = ['Emb_SFC: ' +\\\n",
    "                                   'overall_emb_sfc: '+ str(overall_emb_sfc) +\\\n",
    "                                   'overall_UNemb_sfc: '+ str(overall_unemb_sfc) +\\\n",
    "                                   'overall_emb_sfc_h_rel:' + str(overall_emb_sfc_h_rel) +\\\n",
    "                                   'overall_ar_sorted_sch_list: ' +str(overall_ar_sorted_sch_list) +\\\n",
    "                                   'overall_epi_cnt_High_pr_emb: ' + str(overall_epi_cnt_High_pr_emb) +\\\n",
    "                                   'overall_epi_cnt_Medium_pr_emb: ' + str(overall_epi_cnt_Medium_pr_emb) +\\\n",
    "                                   'overall_epi_cnt_Low_pr_emb: ' +str(overall_epi_cnt_Low_pr_emb)\n",
    "                                  ]\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    traffic_part = ['Traffic: '+ '_clustermodel_' +str(Switch_cluster_info) + '_classifier_' +str(Switch_classifier_info)+\\\n",
    "                                    #'TRAINING: overall_epi_traffic_load_dict: ' + str(overall_epi_traffic_load_dict) +\\\n",
    "                                    #'TRAINING: overall_epi_traffic_packets_dict: '+str(overall_epi_traffic_packets_dict) +\\\n",
    "                                    #' ==> classifier_measurements: ' +str(classifier_measurements) +\\\n",
    "                                    ' ==> overall_epi_traffic_class_gen: '+ str(overall_epi_traffic_class_gen) +\\\n",
    "                                    ' ==> overall_epi_predicted_traffic_classes_dict'+ str(overall_epi_predicted_traffic_classes_dict) +\\\n",
    "                                    ' ==> overall_epi_sfc_pr_traffic_class_emb'+ str(overall_epi_sfc_pr_traffic_class_emb) +\\\n",
    "                                    ' ==> overall_epi_sfc_pr_traffic_class_all' + str(overall_epi_sfc_pr_traffic_class_all) +\\\n",
    "                                    ' ==> overall_epi_cnt_High_traffic0_emb: ' + str(overall_epi_cnt_High_traffic0_emb) +\\\n",
    "                                    ' ==> overall_epi_cnt_Medium_traffic0_emb: ' + str(overall_epi_cnt_Medium_traffic0_emb) +\\\n",
    "                                    ' ==> overall_epi_cnt_Low_traffic0_emb: ' + str(overall_epi_cnt_Low_traffic0_emb) +\\\n",
    "                                    ' ==> overall_epi_cnt_High_traffic1_emb: ' + str(overall_epi_cnt_High_traffic1_emb) +\\\n",
    "                                    ' ==> overall_epi_cnt_Medium_traffic1_emb: ' +str( overall_epi_cnt_Medium_traffic1_emb) +\\\n",
    "                                    ' ==> overall_epi_cnt_Low_traffic1_emb: ' +str(overall_epi_cnt_Low_traffic1_emb) +\\\n",
    "                                    ' overall_epi_cnt_low_pr_traffic_1: ' +str(overall_epi_cnt_low_pr_traffic_1)\n",
    "                                     ]\n",
    "\n",
    "                    #print(\"traffic_part: \",traffic_part)\n",
    "                    data = [Info] + [para] + [Topology_resource_init] + [Nodal_outage_para] + [computation_time] + [results] + \\\n",
    "                           [resource_residual] + [extra_note] + [Bound_Cal] + [priority]+[embeded_sfc] +[traffic_part]              \n",
    "                    #print(data)\n",
    "                    \n",
    "                    dt = datetime.now()\n",
    "                    # getting the timestamp\n",
    "                    ts = datetime.timestamp(dt)\n",
    "                    # convert to datetime\n",
    "                    date_time = datetime.fromtimestamp(ts)\n",
    "                    # convert timestamp to string in dd-mm-yyyy HH:MM:SS\n",
    "                    str_date_time = date_time.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "                    #print(str_date_time)\n",
    "                    \n",
    "                    \n",
    "                    fname = str(trial)+'_'+ str(topology_name)+\\\n",
    "                            '_cpucore_' + str(core_cpu)+'_num_vnf_per_core_'+\\\n",
    "                            str(num_vnf_per_core) +\\\n",
    "                            '_mean_' + str(round(mean_network_overall_accepted,3))+\\\n",
    "                            '_sfc_' + str(num_sfc)+\\\n",
    "                            '_epi_'+str(num_episode)+ '_runtime_' +str(runtime) +\\\n",
    "                            '_clustermodel_' +str(Switch_cluster_info) +\\\n",
    "                            '_classifier_' +str(Switch_classifier_info)+\\\n",
    "                            '_switch_sfc_expiring_check_' +str(switch_sfc_expiring_check) +\\\n",
    "                            str(str_date_time)\n",
    "                    \n",
    "                           # '_switch_reconst_' +str(switch_reconst) +\\\n",
    "                            #'_switch_decomp_' + str(switch_decomp) + \\\n",
    "                            #'_switch_priorty_' + str(switch_priorty)+ \\\n",
    "                            #'_pr_model_'+ str(pr_model)+\\\n",
    "                            #'_sch_model_'+ str(sch_model)\n",
    "                            \n",
    "                    print(fname)\n",
    "                    #f = open('Data_storage/epi_2000_100sfc_agglo_DT/'+fname,\"a\")\n",
    "                    #f.write(str(data))\n",
    "                    #f.close()\n",
    "                    #print(\"XXXXXXXXXXXXXXXXX\")#\n",
    "\n",
    "        if len(overall_sfc_priority) != len(global_services.vNetwork_service_details_er):\n",
    "            status_cnt_ridge = False\n",
    "        else:\n",
    "            status_cnt_ridge = True\n",
    "            #print(\"XXXX\")\n",
    "        #print(\"overall_ar_sorted_sch_list: \",overall_ar_sorted_sch_list)   \n",
    "        #print(\"overall_epi_pr: \", overall_epi_pr)\n",
    "        #print(\"\")\n",
    "        #print(\"Copy_overall_epi_pr: \", copy_overall_epi_pr)\n",
    "        #print(\"overall_epi_pr_emb: \", overall_epi_pr_emb)\n",
    "        #print(\"overall_sfc_priority: \", overall_sfc_priority)\n",
    "        sch_model_overall_cnt_sfc_starved_before_serving[sch_model] = overall_cnt_sfc_starved_before_serving    \n",
    "        sch_model_overall_cnt_sfc_Prclass_starved_before_serving[sch_model] = overall_cnt_sfc_Prclass_starved_before_serving \n",
    "        #print(sch_model_overall_cnt_sfc_starved_before_serving)\n",
    "        #print(sch_model_overall_cnt_sfc_Prclass_starved_before_serving)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without HD services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fname)\n",
    "fname = \"Netrail_cpucore_12_4_mean_19.626_sfc_100_epi_2000_runtime_73311.93_clustermodel_['brich']_classifier_Support Vector Machines_switch_sfc_expiring_check_False01-04-2023\"\n",
    "#\"Netrail_12_8_100epi_cluster_brich_classfier_Support Vector Machine_admission-control_false_mean_40.615\"\n",
    "\n",
    "#f = open('Data_storage/without_admission_controller/'+fname,\"a\")\n",
    "#f.write(str(data))\n",
    "#f.close()\n",
    "#print(\"XXXXXXXXXXXXXXXXX\")#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "switch_clustering = False\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    sch_model_overall_cnt_sfc_starved_before_serving, sch_model_overall_cnt_sfc_Prclass_starved_before_serving = {},{}\n",
    "    overall_sfc_arrival_detail,  overall_sfc_arrival_num = [],[]\n",
    "    overall_sfc_priority, copy_overall_epi_pr, overall_episode_envpriority  = [], [],[]\n",
    "    copy_overall_episode_envpriority = []\n",
    "    #sclassifier_models ={}\n",
    "    \n",
    "    #saving the arrival services\n",
    "    saving_services = global_services.vNetwork_service_details_er\n",
    "    #print(\"saving_services: \", saving_services)\n",
    "    f_name = str(trial)+'_'+ str('_Netrail_')+\\\n",
    "            '_cpucore_' + str(arr_core_cpu[0])+'_num_vnf_per_core_'+\\\n",
    "            str(arr_num_vnf_per_core[0]) + str('_saving_services')\n",
    "    print(f_name)\n",
    "    #f = open('Data_storage/epi_2000_100sfc_agglo_DT/'+f_name,\"a\")\n",
    "    #f.write(str(saving_services))\n",
    "    #f.close()\n",
    "    print(\"XXXXXXXXXXXXXXXXX\")\n",
    "        \n",
    "    status_cnt_ridge = False\n",
    "    for sch_model in ar_sch_model:\n",
    "        #print(\"______________________\")\n",
    "        #print(\"sch_model: \", sch_model, status_cnt_ridge)\n",
    "        sch_model_overall_cnt_sfc_starved_before_serving[sch_model] = None\n",
    "        sch_model_overall_cnt_sfc_Prclass_starved_before_serving [sch_model] = None\n",
    "        env = substrate_network()\n",
    "        reward_func = Reward_Function()\n",
    "        temp_vals_sorted_action, temp_norm_vals_sorted_action = [], []\n",
    "        min_pr,max_pr = 0, 6\n",
    "        t_max, t_min, old_t_current = None, 0, 0\n",
    "        mvf_t_max, mvf_t_min, mvf_old_t_current = None, 0, 0\n",
    "        switch_or_run_once = False\n",
    "        overall_emb_sfc, overall_emb_sfc_h_rel = [], []\n",
    "        overall_unemb_sfc = []\n",
    "        overall_ar_sorted_sch_list =[]\n",
    "\n",
    "        # NODAL OUTAGE\n",
    "        for nodal_outage_percentage in arr_nodal_outage_percentage:\n",
    "            nodal_outage = (env.network_len_nodes*nodal_outage_percentage)/100\n",
    "            round_nodal_outage = round(nodal_outage)\n",
    "\n",
    "            # CPU CORE\n",
    "            for core_cpu in arr_core_cpu:\n",
    "\n",
    "                #VNF PER CPU CORE\n",
    "                for num_vnf_per_core in arr_num_vnf_per_core:\n",
    "                    tot_vnf_per_core = core_cpu*num_vnf_per_core\n",
    "                    tot_vnf_network = tot_vnf_per_core*env.network_len_nodes\n",
    "                    pr_epsilon, epsilon = 1, 1\n",
    "                    eps_history,network_overall_accepted_action = [],[]\n",
    "                    network_overall_accepted, network_overall_rejected = [],[]\n",
    "                    monolithic_deploy_success, monolithic_deploy_rejected = [], []\n",
    "                    overall_res_exhausted = []\n",
    "                    network_overall_count_mvnf, network_overall_sum_mvnf = [],[]\n",
    "                    network_overall_tot_decompsfc, network_overall_count_succes_decompvnf,  = [], []\n",
    "                    network_overall_count_unsucces_decompvnf, overall_remain_cpu, overall_remain_bw = [], [], []\n",
    "                    overall_count_numVNF_perNode, overall_count_num_mVNF_perNode = [], []\n",
    "                    network_overall_epsilon, network_overall_sum_rewards_perepi = [], []\n",
    "                    overall_episode_priority,  overall_episode_priority_err= [], []\n",
    "                    overall_episode_pr_rewardfunc = []\n",
    "                    over_count_macro_pr, over_count_micro_pr = [], []\n",
    "                    overall_epi_pr,overall_epi_pr_emb = [],[]\n",
    "                    overall_epi_traffic_packets_dict, overall_epi_traffic_load_dict = {}, {}\n",
    "                    start = time.time()\n",
    "                    print(\"\")\n",
    "                    print(\"Start Time:\",start)\n",
    "                    #overall runtime these needs to be initiated\n",
    "                    n_state = [8]\n",
    "                    state_memory      = np.zeros((mem_size, *n_state), dtype = np.float32) # size of mem size X input shape\n",
    "                    action_memory     = np.zeros(mem_size, dtype = np.int64)\n",
    "                    reward_memory     = np.zeros(mem_size, dtype = np.float32)\n",
    "                    next_state_memory = np.zeros((mem_size, *n_state), dtype = np.float32)\n",
    "                    terminal_memory   = np.zeros(mem_size, dtype = np.bool)\n",
    "                    vnf_cnt = 0\n",
    "                    temp_vals_sorted_action, temp_norm_vals_sorted_action = [], [] \n",
    "                    if model == 'DDQN':\n",
    "                        print(\"activate agent DDQN\")\n",
    "                        #agent\n",
    "                        agent = LinearDDQNAgent(n_state = n_state, \n",
    "                                                n_actions = env.network_len_nodes, \n",
    "                                                lr = learning_rate, \n",
    "                                                epsilon = epsilon,\n",
    "                                                mem_size = mem_size,\n",
    "                                                batch_size= batch_size,\n",
    "                                                vnf_cnt = vnf_cnt,\n",
    "                                                gamma =gamma,\n",
    "                                                eps_dec =eps_dec,\n",
    "                                                eps_min =eps_min,\n",
    "                                                replace = replace,\n",
    "                                                model = model,\n",
    "                                                state_memory = state_memory,\n",
    "                                                action_memory = action_memory,\n",
    "                                                reward_memory = reward_memory,\n",
    "                                                next_state_memory = next_state_memory,\n",
    "                                                terminal_memory = terminal_memory,\n",
    "                                                Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                                drop_prob =drop_prob,\n",
    "                                                chkpt_dir = 'models/',\n",
    "                                                algo = 'DDQNAgent',\n",
    "                                                env_name = 'BtEurope')\n",
    "                    elif model == 'DQN':\n",
    "                        agent = LinearDQNAgent(n_state = n_state, \n",
    "                                               n_actions = env.network_len_nodes, \n",
    "                                               epsilon = epsilon, \n",
    "                                               lr = learning_rate, \n",
    "                                               vnf_cnt = vnf_cnt, \n",
    "                                               Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                               drop_prob =drop_prob,\n",
    "                                               gamma =gamma,\n",
    "                                               eps_dec =eps_dec,\n",
    "                                               eps_min =eps_min, \n",
    "                                               model = model,\n",
    "                                               algo = None, \n",
    "                                               env_name = None,\n",
    "                                               chkpt_dir ='tmp/dqn')\n",
    "                    #added on 01032022 Heuristic model\n",
    "                    else:\n",
    "                        print(\"Activate Heuristic model\")\n",
    "                        agent = HeuAgent(n_state = n_state, \n",
    "                                         n_actions = env.network_len_nodes)  \n",
    "                        \n",
    "                    cnt_ridge = 0\n",
    "                    # Priority\n",
    "                    if switch_priorty is True:\n",
    "                        pr_mem_size = 5000\n",
    "                        pr_n_state = [3]  #  waiting time, delay, jitter, Packetloss,\n",
    "                        pr_state_input = 'waiting time, delay, jitter, Packetloss'\n",
    "                        # [episode, sfc, flow behaviour, delay, jitter, Packetloss, reliability(removed), wating time]\n",
    "                        pr_n_actions = 1\n",
    "                        pr_state_memory      = np.zeros((pr_mem_size, *pr_n_state), dtype = np.float32) # size of mem size X input shape\n",
    "                        pr_reward_memory     = np.zeros(pr_mem_size, dtype = np.float32)\n",
    "                        pr_next_state_memory = np.zeros((pr_mem_size, *pr_n_state), dtype = np.float32)\n",
    "                        pr_terminal_memory   = np.zeros(pr_mem_size, dtype = np.bool)\n",
    "                        sfc_cnt = 0\n",
    "                        if pr_model == 'DDPG':\n",
    "                            print(\"pr_model: \",pr_model)\n",
    "                            pr_action_memory = np.zeros((pr_mem_size, pr_n_actions), dtype = np.float32)\n",
    "                            pr_agent = DDPG_Agent(pr_n_state       = pr_n_state, \n",
    "                                                  pr_n_actions     = pr_n_actions,\n",
    "                                                  alpha            = alpha, \n",
    "                                                  beta             = beta,\n",
    "                                                  tau              = tau, \n",
    "                                                  sfc_cnt          = sfc_cnt,\n",
    "                                                  mem_size         = mem_size,\n",
    "                                                  batch_size       = batch_size,\n",
    "                                                  gamma            = gamma,\n",
    "                                                  pr_model         = pr_model,\n",
    "                                                  pr_state_memory  = pr_state_memory,\n",
    "                                                  pr_action_memory = pr_action_memory,\n",
    "                                                  pr_reward_memory = pr_reward_memory,\n",
    "                                                  pr_next_state_memory = pr_next_state_memory,\n",
    "                                                  pr_terminal_memory = pr_terminal_memory,\n",
    "                                                  Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                                  chkpt_dir = 'models/',\n",
    "                                                  algo = 'DDPGAgent',\n",
    "                                                  env_name = 'Netrail')\n",
    "                        elif pr_model == 'DDQN':\n",
    "                            print(pr_model)\n",
    "                            pr_action_memory = np.zeros((pr_mem_size), dtype = np.float32)\n",
    "                            pr_agent = PR_DDQNAgent(pr_n_state       = pr_n_state,\n",
    "                                                 pr_n_actions     = pr_n_actions,\n",
    "                                                 lr               = learning_rate, \n",
    "                                                 pr_epsilon       = pr_epsilon,\n",
    "                                                 mem_size         = mem_size,\n",
    "                                                 batch_size       = batch_size,\n",
    "                                                 sfc_cnt          = sfc_cnt,\n",
    "                                                 gamma            = gamma,\n",
    "                                                 eps_dec          = eps_dec,\n",
    "                                                 eps_min          = eps_min,\n",
    "                                                 replace          = replace,\n",
    "                                                 pr_model         = pr_model,\n",
    "                                                 pr_state_memory  = pr_state_memory,\n",
    "                                                 pr_action_memory = pr_action_memory,\n",
    "                                                 pr_reward_memory = pr_reward_memory,\n",
    "                                                 pr_next_state_memory = pr_next_state_memory,\n",
    "                                                 pr_terminal_memory = pr_terminal_memory,\n",
    "                                                 Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                                 drop_prob =drop_prob,\n",
    "                                                 chkpt_dir = 'models/',\n",
    "                                                 algo = 'PR_DDQNAgent',\n",
    "                                                 env_name = 'Netrail')    \n",
    "                        elif pr_model == 'ridge':\n",
    "                            print(pr_model)\n",
    "                            #Lasso model\n",
    "                            alpha = 0.0001\n",
    "                            input_train_data,target_train_data = [], [] \n",
    "                            input_test_data, target_test_data  = [], []\n",
    "                            input_train_mem     = np.zeros((mem_size, *pr_n_state), dtype = np.float32)\n",
    "                            target_train_mem    = np.zeros(mem_size, dtype = np.float32)\n",
    "                            input_test_mem      = np.zeros((mem_size, *pr_n_state), dtype = np.float32)\n",
    "                            target_test_mem     = np.zeros(mem_size, dtype = np.float32)\n",
    "                            # define model\n",
    "                            ridge_model = Ridge(alpha=alpha, normalize = True) #Lasso(alpha=alpha, normalize = True)\n",
    "                            cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeat, random_state=1)\n",
    "                            done_status = \"Train\"\n",
    "                        else: \n",
    "                            #print(\"here\")\n",
    "                            print(pr_model)\n",
    "                            hidden_neuron = 20\n",
    "                            input_train_data,target_train_data = [], [] \n",
    "                            input_test_data, target_test_data  = [], []\n",
    "                            pr_MLP_model = MLP(pr_n_state = pr_n_state, \n",
    "                                               pr_n_actions = pr_n_actions, \n",
    "                                               hidden_neuron = hidden_neuron)\n",
    "                            input_train_mem     = np.zeros((mem_size, *pr_n_state), dtype = np.float)\n",
    "                            target_train_mem    = np.zeros(mem_size, dtype = np.float)\n",
    "                            input_test_mem      = np.zeros((mem_size, *pr_n_state), dtype = np.float)\n",
    "                            target_test_mem     = np.zeros(mem_size, dtype = np.float)\n",
    "                            done_status = \"Train\"\n",
    "                            \n",
    "                    \n",
    "                    # Admission control: Scheduling \n",
    "                    if switch_scheduling is True:\n",
    "                        sch_mem_size = 5000\n",
    "                        sch_n_state = [4]  #  waiting time, delay, jitter, Packetloss,\n",
    "                        sch_state_input = 'waiting time, reliability, priority'\n",
    "                        # [episode, sfc, flow behaviour, delay, jitter, Packetloss, reliability(removed), wating time]\n",
    "                        sch_n_actions = 1\n",
    "                        sch_n_tempsfc = [10]\n",
    "                        sch_state_memory      = np.zeros((sch_mem_size,*sch_n_state), dtype = np.float32) # size of mem size X input shape\n",
    "                        #print(sch_state_memory)\n",
    "                        sch_reward_memory     = np.zeros(sch_mem_size, dtype = np.float32)\n",
    "                        sch_next_state_memory = np.zeros((sch_mem_size, *sch_n_state), dtype = np.float32)\n",
    "                        sch_terminal_memory   = np.zeros(sch_mem_size, dtype = np.bool)\n",
    "                        sfc_cnt = 0\n",
    "                        if sch_model == 'DDPG':\n",
    "                            #print(\"sch_model\", sch_model)\n",
    "                            sch_action_memory     = np.zeros((sch_mem_size, sch_n_actions), dtype = np.float32)\n",
    "                            sch_agent = DDPG_Agent(sch_n_state      = sch_n_state, \n",
    "                                                   sch_n_actions    = sch_n_actions,\n",
    "                                                   alpha            = alpha, \n",
    "                                                   beta             = beta,\n",
    "                                                   tau              = tau, \n",
    "                                                   sfc_cnt          = sfc_cnt,\n",
    "                                                   mem_size         = mem_size,\n",
    "                                                   batch_size       = batch_size,\n",
    "                                                   gamma            = gamma,\n",
    "                                                   sch_model        = sch_model,\n",
    "                                                   sch_state_memory  = sch_state_memory,\n",
    "                                                   sch_action_memory = sch_action_memory,\n",
    "                                                   sch_reward_memory = sch_reward_memory,\n",
    "                                                   sch_next_state_memory = sch_next_state_memory,\n",
    "                                                   sch_terminal_memory = sch_terminal_memory,\n",
    "                                                   Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                                   chkpt_dir = 'models/',\n",
    "                                                   algo = 'DDPGAgent',\n",
    "                                                   env_name = 'Netrail')\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                    a = 0\n",
    "                    cnt_tot_sfc , tr_cnt_tot_sfc, eval_cnt_tot_sfc, pred_cnt_tot_sfc= 0, 0, 0, 0\n",
    "                    count_macro_pr, count_micro_pr = 0, 0\n",
    "                    tr_count_macro_pr, tr_count_micro_pr = 0, 0\n",
    "                    eval_count_macro_pr, eval_count_micro_pr = 0, 0\n",
    "                    pred_count_macro_pr, pred_count_micro_pr = 0, 0\n",
    "                    tr_mem_cnt, test_mem_cnt = 0, 0\n",
    "                    input_train_traffic_mem_cnt, test_traffic_mem_cnt = 0, 0\n",
    "                    epi_tr_correct_perc,  epi_pred_correct_perc = [], []\n",
    "                    overall_deployed_sfc_per_episode, overall_deployed_priority_per_episode = [], []\n",
    "                    epi_cnt_High_pr, epi_cnt_Medium_pr, epi_cnt_Low_pr = [], [], []\n",
    "                    epi_cnt_High_pr_emb, epi_cnt_Medium_pr_emb, epi_cnt_Low_pr_emb = [], [], []\n",
    "                    overall_cnt_sfc_starved_before_serving, overall_cnt_sfc_Prclass_starved_before_serving = [], []\n",
    "                    epi_cnt_sfc_starved_before_serving, epi_cnt_sfc_Prclass_starved_before_serving = [], []\n",
    "                    epi_expected_wt_B = {}\n",
    "                    \n",
    "\n",
    "                    for per_episode in global_services.vNetwork_service_details_er: \n",
    "                        episode = a\n",
    "                        #print(\"\")\n",
    "                        #print(\"Episode: \", episode)\n",
    "                        action, vnf_res_req = False, False\n",
    "                        pref_node_val = False\n",
    "                        cnt_macro_pr, epi_cnt_macro_pr = 0,0 \n",
    "                        cnt_High_pr, cnt_Medium_pr, cnt_Low_pr = 0, 0, 0\n",
    "                        cnt_High_pr_emb, cnt_Medium_pr_emb, cnt_Low_pr_emb = 0, 0, 0\n",
    "                        epi_pr = {\"H\": None,\"M\": None, \"L\": None}\n",
    "                        epi_pr_emb = {\"H\": None,\"M\": None, \"L\": None}\n",
    "                        cnt_sfc_starved_before_serving = 0\n",
    "                        cnt_rel = 0\n",
    "                        epi_classification_grp_acc = {}\n",
    "                        kmeans_runtime, Aggo_runtime, birch_runtime, GM_runtime = [],[],[],[]\n",
    "                        traffic_input_dataset = []\n",
    "                        sfc_starved_before_serving, sfc_Prclass_starved_before_serving = [], []\n",
    "                        nodal_actionspace, removed_nodes = [], []\n",
    "                        epi_emb_sfc, epi_emb_sfc_h_rel = [], []\n",
    "                        traffic_sfc_arrival = []\n",
    "                        traffic_transition_dict, cluster_label_val = {}, {}\n",
    "                        arr_traffic_packet_per_epi = []\n",
    "                        epi_unemb_sfc = []\n",
    "                        epi_temp_sfc_arrived = []\n",
    "                        \n",
    "                        episode_priority, episode_priority_err, episode_envpriority, episode_pr_rewardfunc = [], [], [], []\n",
    "                        actionspace = [i for i in range(env.network_len_nodes)]\n",
    "                        original_actionspace = actionspace\n",
    "                        # to remove certain number of the nodes from the network when outage happens\n",
    "                        for key in range(round_nodal_outage):\n",
    "                            random_action = random.choice(actionspace)\n",
    "                            actionspace.remove(random_action)\n",
    "                            removed_nodes.append(random_action)                 \n",
    "                        #----Resource Initialization for the network Network----#\n",
    "                        env.substrate_node_init(removed_nodes, tot_vnf_per_core)\n",
    "                        env.substrate_link_init()\n",
    "                        if switch_priorty is True and pr_model == 'DDPG':\n",
    "                            pr_agent.noise.reset()\n",
    "                        else:\n",
    "                            pass\n",
    "                        #---- End of Initialization of Resource ----#\n",
    "                        count_accpeted, count_rejected, count_success,count_failed  = 0,0,0,0\n",
    "                        reward_per_episode, sum_reward_per_episode = [], []\n",
    "                        deployed_sfc_per_episode, deployed_priority_per_episode = [], []\n",
    "                        micro_vnf_collection = []\n",
    "                        tot_decompsfc = 0\n",
    "                        undecomp_accepted = 0\n",
    "                        res_exhausted = 0\n",
    "                        count_mvnf_perepi =[]\n",
    "                        sum_count_mvnf_perepi =0\n",
    "                        sum_rewards_perepi = 0\n",
    "                        count_succes_decompvnf, count_unsucces_decompvnf = 0, 0\n",
    "                        nodal_avail = 1 #at the start of the episode the network will be fully available.\n",
    "                        rewards_perepi = []\n",
    "                        std_value = 1\n",
    "                        \n",
    "                        # Priority!\n",
    "                        epi_tr_count_macro_pr,   epi_tr_count_micro_pr   = 0, 0\n",
    "                        epi_eval_count_macro_pr, epi_eval_count_micro_pr = 0, 0\n",
    "                        epi_pred_count_macro_pr, epi_pred_count_micro_pr = 0, 0\n",
    "                        \n",
    "                        # arrival distriution of SFC using Poisson dis.\n",
    "                        if len(overall_sfc_arrival_num) != len(global_services.vNetwork_service_details_er):\n",
    "                            arr_sfc_dist = []\n",
    "                            class_arr_sfc_dist = arrival_sfc(num_sfc, arr_sfc_dist)\n",
    "                            arr_sfc_dist = list(class_arr_sfc_dist.sfc_poisson_dist())\n",
    "                            #print(\"==> SFC dist: \",arr_sfc_dist)\n",
    "                            sfc_arrival_detail, sfc_arrival_num = [], []\n",
    "                            arr_sfc_detail = class_arr_sfc_dist.sfc_details(per_episode, \n",
    "                                                                            arr_sfc_dist, \n",
    "                                                                            sfc_arrival_detail, \n",
    "                                                                            sfc_arrival_num)\n",
    "                            overall_sfc_arrival_detail.append(sfc_arrival_detail)\n",
    "                            overall_sfc_arrival_num.append(sfc_arrival_num)\n",
    "                            #print(\"sfc_arrival_num: \", sfc_arrival_num)\n",
    "\n",
    "                        else: \n",
    "                            pass\n",
    "                        #print(\"Length of overall_sfc_arrival_num: \",len(overall_sfc_arrival_num))\n",
    "                        #print(\"overall_sfc_arrival_num: \", overall_sfc_arrival_num)\n",
    "                        sfc_arrival_detail = overall_sfc_arrival_detail[episode]\n",
    "                        sfc_arrival_num = overall_sfc_arrival_num[episode]\n",
    "                        sfc_priority, sfc_priority_err, sfc_envpriority, sfc_pr_rewardfunc = [], [], [], []\n",
    "                        done_pr   = False\n",
    "                        ar_sfc_current_waitingtime = {}\n",
    "                        ar_sorted_sch_list =[]\n",
    "                        cnt_poisson_dist = 0\n",
    "                        for arr_sfc_de in sfc_arrival_detail:\n",
    "                            ar_sfc_start = time.perf_counter() # step 1\n",
    "                            cnt_sfc_arrival = 0\n",
    "                            temp_my_dict = {}\n",
    "                            ar_starving_sfc = []\n",
    "                            #print(\"\")\n",
    "                            #print(\"cnt_poisson_dist: \", cnt_poisson_dist, \" arr num: \", sfc_arrival_num[cnt_poisson_dist])\n",
    "                            if sch_model == 'WFQ':\n",
    "                                h_pr_queue, m_pr_queue, l_pr_queue = [], [], []\n",
    "                            if not sfc_arrival_num[cnt_poisson_dist]:\n",
    "                                sorted_sch_list =[]\n",
    "                            ar_sfc_end = time.perf_counter() #step 1\n",
    "                            expected_wt_A = round(ar_sfc_end - ar_sfc_start,6)\n",
    "\n",
    "                            for arv_per_sfc, arv_per_link in arr_sfc_de:\n",
    "                                sfc = arv_per_sfc[0][1]['sfc']\n",
    "                                #print(\"arv_per_sfc: \", arv_per_sfc)\n",
    "                                #print(\"SFC: \", sfc)\n",
    "                                ar_link_bw = []\n",
    "                                for link in arv_per_link:\n",
    "                                    temp_bw = link[2]['bw']\n",
    "                                    ar_link_bw.append(temp_bw)\n",
    "                                ar_sfc_cpu = []\n",
    "                                for temp_sfc in arv_per_sfc:\n",
    "                                    temp_cpu = temp_sfc[1]['cpu_req']\n",
    "                                    ar_sfc_cpu.append(temp_cpu)     \n",
    "                                # Priority for Arriving SFC\n",
    "                                env_priority = None\n",
    "                                env_priority = arv_per_sfc[0][1]['env_priority']\n",
    "                                env_priority = env_priority - env_priority % 0.01\n",
    "                                done_sch = False\n",
    "                                A_cost, B_cost, S_cost = 0,0,0\n",
    "                                reward_pts = 1000\n",
    "                                #print(\"SFC: \", sfc)\n",
    "                                if status_cnt_ridge is False:\n",
    "                                    #print(\"pr_model: \", pr_model)\n",
    "                                    if switch_priorty is True:\n",
    "                                            pr_sfc_start = time.perf_counter()\n",
    "                                            threshold_latency          = arv_per_sfc[0][1]['norm_delay'] \n",
    "                                            threshold_jitter           = arv_per_sfc[0][1]['norm_jitter']     \n",
    "                                            threshold_packetloss       = arv_per_sfc[0][1]['norm_packetloss'] \n",
    "                                            threshold_theo_wating_time = arv_per_sfc[0][1]['norm_waiting_time'] \n",
    "                                            \n",
    "                                            # creating priority observation\n",
    "                                            pr_obs = [threshold_latency, threshold_jitter, threshold_packetloss]\n",
    "                                            if pr_model == 'ridge':\n",
    "                                                X = pr_obs\n",
    "                                                Y = [env_priority]\n",
    "                                                sfc_envpriority.append(env_priority)\n",
    "                                                env_pr_level = truncate(Y[0],2)\n",
    "                                                if env_pr_level == 1:\n",
    "                                                    macro_env_pr_level = int(truncate(env_pr_level, 0))\n",
    "                                                    micro_env_pr_level = round((truncate(env_pr_level, 1)- macro_env_pr_level),2)\n",
    "                                                else:\n",
    "                                                    macro_env_pr_level = int(truncate(env_pr_level*10, 0))\n",
    "                                                    micro_env_pr_level = int(truncate(env_pr_level*100, 1))- macro_env_pr_level*10\n",
    "                                                if done_status == \"Train\": \n",
    "                                                    tr_cnt_tot_sfc += 1\n",
    "                                                    train_dataset = prepare_train_data(input_train_mem, target_train_mem, \n",
    "                                                                                       input_train_data, target_train_data,\n",
    "                                                                                       X, Y, \n",
    "                                                                                       tr_mem_cnt, tr_mem_size, tr_batch_size)\n",
    "                                                    input_train_data = train_dataset[0]\n",
    "                                                    target_train_data = train_dataset[1]\n",
    "                                                    tr_mem_cnt = train_dataset[2]\n",
    "                                                    #print(\"train_dataset: \", train_dataset)\n",
    "                                                    if tr_cnt_tot_sfc < tr_batch_size:\n",
    "                                                        temp_priority_level = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "                                                        priority_level  = random.uniform(0,1) #np.random.choice(temp_priority_level)\n",
    "                                                        #print(\"priority_level: \", priority_level, type(priority_level))\n",
    "                                                    else:\n",
    "                                                        # evaluate model\n",
    "                                                        scores = cross_val_score(ridge_model, input_train_data, target_train_data, \n",
    "                                                                                 scoring='neg_mean_absolute_error', \n",
    "                                                                                 cv=cv, n_jobs= 1)\n",
    "                                                        # force scores to be positive\n",
    "                                                        scores = absolute(scores)\n",
    "                                                        # fit model\n",
    "                                                        ridge_model.fit(input_train_data, target_train_data)\n",
    "                                                        # make a prediction\n",
    "                                                        yhat = ridge_model.predict([X])\n",
    "                                                        # summarize prediction\n",
    "                                                        priority_level = yhat[0]\n",
    "                                                        #print(\"priority_level: \", priority_level, type(priority_level))\n",
    "\n",
    "                                                    #classifing the priority levels into three classess.\n",
    "                                                    if 0.8<=priority_level and priority_level<=1.0:\n",
    "                                                        priority_class = 'high'\n",
    "                                                        cnt_High_pr +=1\n",
    "                                                    elif 0.4<=priority_level and priority_level<=0.7:\n",
    "                                                        priority_class = 'medium'\n",
    "                                                        cnt_Medium_pr +=1\n",
    "                                                    else: \n",
    "                                                        priority_class = 'low'   \n",
    "                                                        cnt_Low_pr +=1\n",
    "                                                    sfc_priority.append(priority_level)\n",
    "                                                    global_services.priority_update(episode, sfc, \n",
    "                                                                                    vnf_per_service, \n",
    "                                                                                    priority_level, \n",
    "                                                                                    priority_class)\n",
    "                                                    if priority_level == 1:\n",
    "                                                        macro_priority_level = 1 #int(truncate(priority_level, 0))\n",
    "                                                        micro_priority_level = 0 #round((truncate(priority_level, 1)- macro_priority_level),2)\n",
    "                                                    else:\n",
    "                                                        macro_priority_level = int(truncate(priority_level*10, 0))\n",
    "                                                        micro_priority_level = int(truncate(priority_level*100, 1))- macro_priority_level*10\n",
    "                                                    if macro_priority_level == macro_env_pr_level:\n",
    "                                                        #first level clearance\n",
    "                                                        tr_count_macro_pr += 1\n",
    "                                                        epi_tr_count_macro_pr += 1\n",
    "                                                        if micro_priority_level == micro_env_pr_level:\n",
    "                                                            tr_count_micro_pr += 1\n",
    "                                                            epi_tr_count_micro_pr += 1\n",
    "                                                    tr_macro_acc = round(((tr_count_macro_pr/tr_cnt_tot_sfc)*100), 2)\n",
    "                                                    tr_micro_acc = round(((tr_count_micro_pr/tr_cnt_tot_sfc)*100), 2)\n",
    "                                                    if a == 0:\n",
    "                                                        pass\n",
    "                                                    else:\n",
    "                                                        if (tr_macro_acc/100) >= acc_theshold_limit:\n",
    "                                                            #done_status == \"Train\"\n",
    "                                                            done_status = \"Predict\"\n",
    "                                                            #print(\"Train --> Predict\", done_status, cnt_tot_sfc, tr_cnt_tot_sfc,\n",
    "                                                            #      \" macro acc: \", tr_macro_acc, \" micro acc: \",tr_micro_acc, tr_count_micro_pr)\n",
    "                                                else:\n",
    "                                                    pred_cnt_tot_sfc +=1\n",
    "                                                    # make a prediction\n",
    "                                                    yhat = ridge_model.predict([X])\n",
    "                                                    # summarize prediction\n",
    "                                                    priority_level = yhat[0]\n",
    "                                                    #print(\"priority_level: \", priority_level, type(priority_level))\n",
    "                                                    #classifing the priority levels into three classess.\n",
    "                                                    if 0.8 <= priority_level and priority_level <= 1.0: #between 0.7 - 1.0\n",
    "                                                        priority_class = 'high' \n",
    "                                                        cnt_High_pr +=1\n",
    "                                                    elif 0.4 <= priority_level and priority_level <= 0.7:  #between 0.4 - 0.7\n",
    "                                                        priority_class = 'medium'\n",
    "                                                        cnt_Medium_pr +=1\n",
    "                                                    else: \n",
    "                                                        priority_class = 'low' #between 0.0 - 0.3\n",
    "                                                        cnt_Low_pr +=1\n",
    "                                                    sfc_priority.append(priority_level)\n",
    "                                                    global_services.priority_update(episode, sfc, vnf_per_service, \n",
    "                                                                                    priority_level, priority_class)\n",
    "                                                    if priority_level == 1:\n",
    "                                                        macro_priority_level = 1 #int(truncate(priority_level, 0))\n",
    "                                                        micro_priority_level = 0 #round((truncate(priority_level, 1)- macro_priority_level),2)\n",
    "                                                    else:\n",
    "                                                        macro_priority_level = int(truncate(priority_level*10, 0))\n",
    "                                                        micro_priority_level = int(truncate(priority_level*100, 1))- macro_priority_level*10\n",
    "                                                        #micro_env_pr_level = round(micro_env_pr_level,2)\n",
    "                                                    if macro_priority_level == macro_env_pr_level:\n",
    "                                                        #first level clearance\n",
    "                                                        pred_count_macro_pr += 1\n",
    "                                                        epi_pred_count_macro_pr += 1\n",
    "                                                        if micro_priority_level == micro_env_pr_level:\n",
    "                                                            pred_count_micro_pr += 1\n",
    "                                                            epi_pred_count_micro_pr += 1\n",
    "                                                    pred_macro_acc =round(((pred_count_macro_pr/pred_cnt_tot_sfc)*100),2)\n",
    "                                                    pred_micro_acc =round(((pred_count_micro_pr/pred_cnt_tot_sfc)*100),2)\n",
    "                                                    if (pred_macro_acc/100) >= acc_theshold_limit:\n",
    "                                                        done_status = \"Done\"\n",
    "                                                    if overall_sfc == cnt_tot_sfc:\n",
    "                                                        pred_macro_acc =round(((pred_count_macro_pr/pred_cnt_tot_sfc)*100),2)\n",
    "                                                        pred_micro_acc =round(((pred_count_micro_pr/pred_cnt_tot_sfc)*100),2)\n",
    "                                            else: \n",
    "                                                priority_level = arv_per_sfc[0][1]['env_priority']\n",
    "                                            #print(\"SFC: \", sfc, \"priority_level: \", priority_level)\n",
    "                                            pr_sfc_end = time.perf_counter()\n",
    "                                            expected_wt_B = round((pr_sfc_end - pr_sfc_start), 6) # step2\n",
    "                                            #print(\"expected_wt_B: \", expected_wt_B)\n",
    "                                            epi_expected_wt_B[sfc] = expected_wt_B\n",
    "                                else:           \n",
    "                                    pass\n",
    "                                \n",
    "                                if switch_scheduling is True:\n",
    "                                    #print(\"switch_scheduling: \", switch_scheduling)\n",
    "                                    #print(\"episode: \", episode)\n",
    "                                    sch_sfc_start = time.perf_counter()\n",
    "                                    if sch_model == 'DDPG':\n",
    "                                        #print(\"sch_model: \", sch_model)\n",
    "                                        sch_obs = []\n",
    "                                        threshold_priority_level    = arv_per_sfc[0][1]['priority_level']\n",
    "                                        threshold_reliability       = arv_per_sfc[0][1]['reliability']\n",
    "                                        threshold_theo_wating_time  = arv_per_sfc[0][1]['theo_wating_time']\n",
    "                                        norm_waitingtime            = arv_per_sfc[0][1]['norm_waiting_time']\n",
    "                                        threshold_reliability       = arv_per_sfc[0][1]['reliability']\n",
    "                                        threshold_evnpriority_level = arv_per_sfc[0][1]['env_priority']\n",
    "                                        threshold_int_flowtype      = arv_per_sfc[0][1]['int_flow_behaviour']\n",
    "                                        current_starving_sfc = None\n",
    "                                        intersec_sfc = None\n",
    "                                        starving_sfc_index = None\n",
    "                                        if threshold_priority_level <= 0.2:\n",
    "                                            current_starving_sfc = sfc\n",
    "                                            ar_starving_sfc.append(sfc)\n",
    "                                            \n",
    "                                        traffic_input_dataset,  traffic_output_dataset = [], []\n",
    "                                        class_traffic_classifer = traffic_classifer(traffic_input_dataset, \n",
    "                                                                                    traffic_output_dataset, \n",
    "                                                                                    episode, cluster_testing)\n",
    "                                        \n",
    "                                        classifier_datasets = [None, None, None, None]\n",
    "                                        if switch_clustering is True:\n",
    "                                            #print( \"==>\", switch_clustering, classifier_training_status)\n",
    "                                            mu, std = 100, 300\n",
    "                                            pred_traffic_num_sfc_requested = abs(random.normal(mu, std, 1))\n",
    "                                            while pred_traffic_num_sfc_requested < 1:\n",
    "                                                pred_traffic_num_sfc_requested = abs(random.normal(mu, std, 1))\n",
    "                                            pred_num =  abs(int(pred_traffic_num_sfc_requested))\n",
    "                                            pred_f = FBM(n  = int(pred_num), hurst=0.7, length=length, method='daviesharte')\n",
    "                                            pred_fbm_sample = abs(pred_f.fbm())\n",
    "                                            pred_fgn_sample = pred_f.fgn()\n",
    "                                            pred_t_values   = pred_f.times() #stationary traffic\n",
    "                                            pred_traffic_packet_per_sfc = sum(pred_t_values) #load\n",
    "                                            \n",
    "                                            predict_dataset = [threshold_priority_level, \n",
    "                                                               threshold_int_flowtype, pred_num, \n",
    "                                                               pred_traffic_packet_per_sfc]\n",
    "                                            #print(\"==> predict_dataset\", predict_dataset)\n",
    "                                            #print(\"classifier_models: \", classifier_models)\n",
    "                                            Classifier_training = class_traffic_classifer.classifier_models(classifier_models, \n",
    "                                                                                                            classifier_datasets,\n",
    "                                                                                                            predict_dataset,\n",
    "                                                                                                            classifier_training_status)\n",
    "                                            \n",
    "                                            #print(\"Classifier_training: \", Classifier_training)\n",
    "                                            #classifier_measurements[clusterModel] = Classifier_training\n",
    "                                            traffic_pred = Classifier_training[4][0]\n",
    "                                            #print(traffic_pred)\n",
    "                                        else: \n",
    "                                            #print(\"no clustering\")\n",
    "                                            traffic_pred = -1\n",
    "                                            pred_num = -1\n",
    "                                            pred_traffic_packet_per_sfc = -1\n",
    "                                            \n",
    "                                        #print(\"traffic_pred: \", traffic_pred)   \n",
    "                                        #updating in global level\n",
    "                                        global_services.classifier_class_update(episode, sfc, vnf_per_service, \n",
    "                                                                                traffic_pred, pred_num,\n",
    "                                                                                pred_traffic_packet_per_sfc\n",
    "                                                                               )   \n",
    "                                        \n",
    "                                        sch_obs = [threshold_theo_wating_time, \n",
    "                                                   threshold_reliability,\n",
    "                                                   threshold_priority_level, \n",
    "                                                   traffic_pred\n",
    "                                                  ]\n",
    "                                        sch_rank = sch_agent.choose_action(sch_obs, sch_n_state)\n",
    "                                        temp_my_dict[sfc] = sch_rank\n",
    "                                        sorted_sfc_sch = dict(sorted(temp_my_dict.items(), \n",
    "                                                                    key=lambda item: item[1], \n",
    "                                                                    reverse = True))\n",
    "                                        sorted_sch_list = list(sorted_sfc_sch.keys())\n",
    "                                        \n",
    "                                        if ar_starving_sfc is None or not ar_starving_sfc:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            intersec_sfc = set(sorted_sch_list).intersection([current_starving_sfc])\n",
    "                                            if intersec_sfc is None or not intersec_sfc:\n",
    "                                                pass\n",
    "                                            else:\n",
    "                                                starving_sfc_index = sorted_sch_list.index(current_starving_sfc) \n",
    "                                                \n",
    "                                        if cnt_sfc_arrival == len(sfc_arrival_num[cnt_poisson_dist])-1:\n",
    "                                            done_sch = True\n",
    "                                            next_sch_obs = [#sfc, \n",
    "                                                            #threshold_evnpriority_level, \n",
    "                                                            #threshold_priority_level, \n",
    "                                                            threshold_theo_wating_time, \n",
    "                                                            threshold_reliability,\n",
    "                                                            threshold_priority_level,\n",
    "                                                            traffic_pred\n",
    "                                                           ]\n",
    "                                        else:\n",
    "                                            next_sfc = cnt_sfc_arrival + 1\n",
    "                                            next_threshold_priority_level    = arr_sfc_de[next_sfc][0][0][1]['priority_level']\n",
    "                                            next_threshold_reliability       = arr_sfc_de[next_sfc][0][0][1]['reliability']\n",
    "                                            next_threshold_theo_wating_time  = arr_sfc_de[next_sfc][0][0][1]['theo_wating_time']\n",
    "                                            next_threshold_evnpriority_level = arr_sfc_de[next_sfc][0][0][1]['env_priority']\n",
    "                                            next_threshold_int_flowtype      = arr_sfc_de[next_sfc][0][0][1]['int_flow_behaviour']\n",
    "                                            traffic_pred = -1\n",
    "                                            next_sch_obs = [next_threshold_theo_wating_time, \n",
    "                                                            next_threshold_reliability,\n",
    "                                                            next_threshold_evnpriority_level, \n",
    "                                                            traffic_pred] \n",
    "                                        sch_next_state = next_sch_obs\n",
    "                                        #Reward function\n",
    "                                        A_cost = (threshold_priority_level*reward_pts) + (norm_waitingtime*reward_pts)\n",
    "                                        link_cost = 0 #(1/sum(ar_link_bw))*reward_pts\n",
    "                                        cpu_cost  = 0 #(1/sum(ar_sfc_cpu))*reward_pts\n",
    "                                        reliability_cost = threshold_reliability*reward_pts\n",
    "                                        B_cost = link_cost + cpu_cost + reliability_cost\n",
    "                                        decay = 0.1\n",
    "                                        alpha = 1\n",
    "                                        \n",
    "                                        pos_sfc = starving_sfc_index\n",
    "                                        if pos_sfc is None:\n",
    "                                            starvation_rate = 0\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            starvation_rate = alpha*pow(decay,pos_sfc)\n",
    "                                        S_cost = starvation_rate*reward_pts\n",
    "                                        sch_rewardfunc = A_cost + B_cost + S_cost\n",
    "                                        sch_agent.store_transition(sch_obs, \n",
    "                                                                   sch_rank, \n",
    "                                                                   sch_rewardfunc, \n",
    "                                                                   sch_next_state, \n",
    "                                                                   done_sch, \n",
    "                                                                   sfc)\n",
    "                                        sch_agent.learn()\n",
    "                                        sch_sfc_end = time.perf_counter()\n",
    "\n",
    "                                    elif sch_model == 'FIFO': \n",
    "                                        sorted_sch_list = sfc_arrival_num[cnt_poisson_dist] \n",
    "                                        sch_sfc_end = time.perf_counter()\n",
    "                                    elif sch_model == 'WFQ':\n",
    "                                        if arv_per_sfc[0][1]['priority_class'] == 'high':\n",
    "                                            h_pr_queue.append(sfc)\n",
    "                                        elif arv_per_sfc[0][1]['priority_class'] == 'medium':\n",
    "                                            m_pr_queue.append(sfc)\n",
    "                                        else: \n",
    "                                            l_pr_queue.append(sfc)\n",
    "                                        sorted_sch_list = sfc_arrival_num[cnt_poisson_dist]\n",
    "                                        sch_sfc_end = time.perf_counter()\n",
    "                                    else:\n",
    "                                        #print(sch_model)\n",
    "                                        #print(\"arv_per_sfc: \", arv_per_sfc[0][1])\n",
    "                                        temp_my_dict[sfc] = arv_per_sfc[0][1]['priority_level']\n",
    "                                        #print(temp_my_dict)\n",
    "                                        sorted_sfc_sch = dict(sorted(temp_my_dict.items(), \n",
    "                                                                    key=lambda item: item[1], \n",
    "                                                                    reverse = True))\n",
    "                                        sorted_sch_list = list(sorted_sfc_sch.keys())\n",
    "                                        sch_sfc_end = time.perf_counter()\n",
    "                                        #print(\"sorted_sch_list: \", sorted_sch_list)\n",
    "                                cnt_sfc_arrival += 1\n",
    "                            cnt_poisson_dist += 1\n",
    "                            #print(sch_model)\n",
    "                            #print(\"sorted_sch_list: \", sorted_sch_list)\n",
    "                            \n",
    "                            # Adaptive scheduling\n",
    "                            if sch_model == 'DDPG':\n",
    "                                temp_sfc_deploy = []\n",
    "                                for w in sorted_sch_list:\n",
    "                                    temp_var = []\n",
    "                                    for q, p in arr_sfc_de:\n",
    "                                        if q[0][1]['sfc'] == w:\n",
    "                                            temp_var = [q,p]\n",
    "                                            temp_sfc_deploy.append(temp_var)\n",
    "                                        else:\n",
    "                                            pass\n",
    "                            elif sch_model == 'FIFO': \n",
    "                                temp_sfc_deploy = arr_sfc_de\n",
    "                            elif sch_model == 'WFQ':\n",
    "                                temp_sfc_deploy = []\n",
    "                                wfq_sch_sfc_start = time.perf_counter()\n",
    "                                sorted_sch_list = wfq_sch(arr_sfc_de, h_pr_queue, m_pr_queue, l_pr_queue)\n",
    "                                wfq_sch_sfc_end = time.perf_counter()\n",
    "                                for w in sorted_sch_list:\n",
    "                                    temp_var = []\n",
    "                                    for q, p in arr_sfc_de:\n",
    "                                        if q[0][1]['sfc'] == w:\n",
    "                                            temp_var = [q,p]\n",
    "                                            temp_sfc_deploy.append(temp_var)\n",
    "                                        else:\n",
    "                                            pass\n",
    "                            else: #priority-based!!\n",
    "                                temp_sfc_deploy = []\n",
    "                                for w in sorted_sch_list:\n",
    "                                    temp_var = []\n",
    "                                    for q, p in arr_sfc_de:\n",
    "                                        if q[0][1]['sfc'] == w:\n",
    "                                            temp_var = [q,p]\n",
    "                                            temp_sfc_deploy.append(temp_var)\n",
    "                                        else:\n",
    "                                            pass\n",
    "                            \n",
    "                            if sch_model == 'WFQ':\n",
    "                                wfq_sch_sfc = wfq_sch_sfc_end - wfq_sch_sfc_start\n",
    "                            else:\n",
    "                                wfq_sch_sfc =0\n",
    "                            \n",
    "                            sch_sfc_wt = (sch_sfc_end-sch_sfc_start)+wfq_sch_sfc\n",
    "                            #print(\"sch_sfc_wt: \", sch_sfc_wt)\n",
    "                            temp_val = [std_value for i in (range(env.network_len_nodes))]\n",
    "                            prob_val = [float(i)/sum(temp_val) for i in temp_val]\n",
    "                            # services will arrive here.\n",
    "                            num_sfc_per_arrival = 0\n",
    "                            sfc_waited_time =  0\n",
    "                            \n",
    "                            # edited on 15th sept 2022\n",
    "                            #print(\"sorted_sch_list: \", sorted_sch_list)\n",
    "                            for t_sfc in sorted_sch_list:\n",
    "                                ar_sfc_current_waitingtime[t_sfc] = sfc_waited_time\n",
    "                            temp_sorted_sch_list = sorted_sch_list.copy()\n",
    "                            ar_sorted_sch_list.append(sorted_sch_list)\n",
    "                            #print(\"ar_sfc_current_waitingtime: \", ar_sfc_current_waitingtime)\n",
    "                            #print(\"overall_expected_wt_B: \", overall_expected_wt_B)\n",
    "                            #print(\"ar_sfc_current_waitingtime: \", ar_sfc_current_waitingtime)\n",
    "                            for per_sfc, per_link in temp_sfc_deploy:\n",
    "                                done  = False\n",
    "                                sfc   = per_sfc[0][1]['sfc'] \n",
    "                                epi_temp_sfc_arrived.append(sfc)\n",
    "                                arr_traffic_packet_per_sfc = []\n",
    "                                min_samples = min_samples\n",
    "                                dict_traffic_packet_per_sfc = {}\n",
    "                                #print(\"\")\n",
    "                                #print(\"SFC\", sfc)\n",
    "                                sfc_priority_level =  per_sfc[0][1]['priority_level']\n",
    "                                sfc_int_flow       =  per_sfc[0][1]['int_flow_behaviour']\n",
    "                                traffic_N = 0 #None\n",
    "                                traffic_load = 0 #None\n",
    "                                temp_s = 0\n",
    "                                temp_pr_ft = [sfc, sfc_priority_level, sfc_int_flow, traffic_N, traffic_load]\n",
    "                                traffic_transition_dict[sfc] = temp_pr_ft\n",
    "                                #print(\"traffic_transition_dict: \", traffic_transition_dict)\n",
    "                                cnt_tot_sfc += 1\n",
    "\n",
    "                                num_of_vl = per_link[0][2]['numvl']\n",
    "                                decompmvnf = False\n",
    "                                res_exh = False\n",
    "                                #checking if the deploying sfc is within the waiting threshold\n",
    "                                #sfc_wt = per_sfc[0][1]['norm_waiting_time'] #theo_wating_time'\n",
    "                                sfc_wt = per_sfc[0][1]['theo_wating_time']\n",
    "                                sfc_rel = per_sfc[0][1]['reliability'] #reliability_factor\n",
    "                                sfc_rel_factor = per_sfc[0][1]['reliability_factor']\n",
    "                                sfc_start = time.perf_counter() \n",
    "                                traffic_pred = per_sfc[0][1]['classifier_class']\n",
    "                                traffic_load = per_sfc[0][1]['traffic_load']\n",
    "                                \n",
    "                                if ar_sfc_current_waitingtime[sfc] >= sfc_wt:\n",
    "                                    #print(\"Starved: Deployment time\", sfc_deployment_time)\n",
    "                                    cnt_sfc_starved_before_serving += 1\n",
    "                                    sfc_priority_class =  per_sfc[0][1]['priority_class']\n",
    "                                    sfc_starved_before_serving.append(sfc)\n",
    "                                    sfc_Prclass_starved_before_serving.append(sfc_priority_class)\n",
    "                                    #print(\"expired SFC:\", sfc, \"sfc_priority_class:\", sfc_priority_class) \n",
    "                                    #print(\"did SFC starve??\", sfc)\n",
    "                                    epi_unemb_sfc.append(sfc)\n",
    "                                    emb_status = 'starved'\n",
    "                                    sfc_deployment_time = sfc_deployment_time#\n",
    "                                    global_services.emb_update(episode, sfc, vnf_per_service, emb_status, \n",
    "                                                               sfc_deployment_time)   \n",
    "                                else:\n",
    "                                    sfc_priority_class =  per_sfc[0][1]['priority_class']\n",
    "                                    #microvnf\n",
    "                                    micro_requested_services, micro_requested_nodes = [], []\n",
    "                                    vnf_per_service = vnf_per_service\n",
    "                                    #Estimating the number of VNF persent per sfc\n",
    "                                    state_space_size = vnf_per_service\n",
    "                                    link_space_size = num_of_vl\n",
    "                                    action_space_size = env.network_len_nodes\n",
    "                                    actions_per_iteration,accpeted_action = [], []\n",
    "                                    update_actionspace, updated_path_perSFC  = [], []\n",
    "                                    reward_per_current_states, rewardlink_per_current_states = [], []\n",
    "                                    new_reward_per_current_states = []\n",
    "                                    reward_per_service, rewardDelay_per_current_states= [], []\n",
    "                                    sum_reward_per_service, score_per_service =[], []\n",
    "                                    micro_vnf_id, global_mVNF_action = [], []\n",
    "                                    global_updated_path_perVNF = []\n",
    "                                    my_dict = {}\n",
    "                                    dict_node_avail = {}\n",
    "                                    sorted_list = []\n",
    "                                    rewards = 0\n",
    "                                    reward_action = 0\n",
    "                                    global_decomp = []\n",
    "                                    emb_mVNFID = []\n",
    "                                    #Problem-solving algorithm starts\n",
    "                                    count_vnf, count_updated_path_perSFC = 0, 0\n",
    "                                    count_VNF, sum_count_mvnf_persfc = 0,0\n",
    "                                    count_mvnf_persfc = []\n",
    "                                    done_microvnf = False\n",
    "                                    for node, available_res in env.network_nodes:\n",
    "                                        my_dict[node] = available_res['emb_vnf']\n",
    "                                        sorted_action = dict(sorted(my_dict.items(), key=lambda item: item[1], reverse = False))\n",
    "                                    keys_sorted_action = sorted_action.keys()\n",
    "                                    action_space = list(map(int,keys_sorted_action))\n",
    "                                    up_actionspace = action_space\n",
    "                                    temp_sorted_action = sorted_action\n",
    "                                    vals_sorted_action = sorted_action.values()\n",
    "                                    if sum(vals_sorted_action) == 0:\n",
    "                                        break\n",
    "                                    norm_vals_sorted_action = [float(i)/sum(vals_sorted_action) for i in vals_sorted_action]\n",
    "                                    #resource got exhusted\n",
    "                                    sum_resouces = sum(list(vals_sorted_action))\n",
    "                                    nodal_avail = round(sum_resouces/tot_vnf_network, 3)\n",
    "                                    #Reachitechture\n",
    "                                    catalog_mVNF= {}\n",
    "                                    for node in original_actionspace:\n",
    "                                        catalog_mVNF[node] = []\n",
    "                                        temp= {}  \n",
    "                                    # checking the existence of the mVNF                                    \n",
    "                                    temp_accepted_action = {}\n",
    "                                    for node in original_actionspace:\n",
    "                                        temp_accepted_action[node] = []\n",
    "                                    rewards_persfc = []\n",
    "                                    for vnf,requested_res in per_sfc:\n",
    "                                        vnf_start = time.perf_counter_ns()\n",
    "                                        done_vnf = False\n",
    "                                        vnf_cnt +=1\n",
    "                                        global_services.reset(episode, sfc, vnf)\n",
    "                                        updated_path_perVNF = []\n",
    "                                        local_rewardVNF = 0\n",
    "                                        count_mvnf = 0\n",
    "                                        reward_vnfs = []                                   \n",
    "                                        if not action_space:\n",
    "                                            res_exh = True\n",
    "                                            sum_resouces = sum(list(vals_sorted_action))\n",
    "                                            break     \n",
    "                                        temp_val[action] = temp_val[action] + pref_node_val\n",
    "                                        prob_val = [float(i)/sum(temp_val) for i in temp_val]\n",
    "                                        temp_vals_sorted_action = vals_sorted_action\n",
    "                                        temp_norm_vals_sorted_action = norm_vals_sorted_action\n",
    "                                        prev_action = action\n",
    "                                        prev_vnf_res_req = vnf_res_req\n",
    "                                        vnf_res_req = requested_res['cpu_req']\n",
    "                                        pr_level = requested_res['priority_level']\n",
    "                                        sum_resouces = sum(list(vals_sorted_action))\n",
    "                                        nodal_avail = round(sum_resouces/tot_vnf_network, 3)                                     \n",
    "                                        decompose_criteria = decomp_identifier(vnf_res_req, nodal_avail, max_vnf_resource, \n",
    "                                                                               episode, sfc, vnf, vnf_per_service)\n",
    "                                        #Decomposition check 1: based on VNF resource\n",
    "                                        if vnf_res_req > threshold_decompose:\n",
    "                                            decompose_candidate = decompose_criteria.potential_vnf(threshold_decompose, \n",
    "                                                                                                   done_potential_vnf = False)\n",
    "                                            done_decompvnf = decompose_criteria.done_potential_vnf                                \n",
    "                                            global_services.update_vnf(episode,sfc, vnf,decompose_candidate, done_decompvnf)\n",
    "                                            decomp= decompose_criteria.decomp\n",
    "                                            num_micro_vnf = decompose_criteria.num_micro_vnf\n",
    "                                            if done_decompvnf == False:\n",
    "                                                pass\n",
    "                                        #Decomposition check 2: based on nodal availability \n",
    "                                        elif nodal_avail < threshold_nodal_decomp:\n",
    "                                            decompose_candidate = decompose_criteria.nodal_availability(done_nodal = False)\n",
    "                                            done_decompvnf = decompose_criteria.done_nodal\n",
    "                                            global_services.update_vnf(episode,sfc, vnf,decompose_candidate, done_decompvnf)\n",
    "                                            decomp = decompose_criteria.decomp\n",
    "                                            num_micro_vnf = decompose_criteria.num_micro_vnf\n",
    "                                            unique_id = decompose_criteria.unique_id\n",
    "                                            if unique_id:\n",
    "                                                micro_vnf_id.append(unique_id)\n",
    "                                            if done_decompvnf == False:\n",
    "                                                pass\n",
    "                                        else:\n",
    "                                            pass\n",
    "                                        reward_per_current_vnfstates = []\n",
    "                                        rewardlink_per_current_vnfstates = []\n",
    "                                        microvnf_accpeted_action = [] \n",
    "                                        t_microvnf_accpeted_action = []\n",
    "                                        micro_const = microvnf_construction(max_vnf_resource = max_vnf_resource, \n",
    "                                                                            episode = episode, \n",
    "                                                                            sfc = sfc, vnf = vnf, cpu_req = vnf_res_req, \n",
    "                                                                            n =vnf_per_service , val =0, \n",
    "                                                                            pr_level =pr_level, sfc_rel = sfc_rel, \n",
    "                                                                            traffic_pred = traffic_pred)\n",
    "                                        micro_vnf, micro_vnf_res = -1, -1\n",
    "                                        vnf_link_resources = per_link\n",
    "                                        if requested_res['microvnf_done_status'] == True:\n",
    "                                            done_mVNF = True\n",
    "                                            #print(\"==> miroserives\")\n",
    "                                            decomp =[]\n",
    "                                            global_decomp.append(decomp)\n",
    "                                            if decomp is None or not decomp:\n",
    "                                                    break\n",
    "                                            emb_micro = micro_const.microvnf_agent(episode = episode, \n",
    "                                                                                   sfc = sfc, \n",
    "                                                                                   vnf = vnf, \n",
    "                                                                                   decomp = decomp , \n",
    "                                                                                   n_actions = env.network_len_nodes, \n",
    "                                                                                   lr = learning_rate, \n",
    "                                                                                   epsilon = epsilon,\n",
    "                                                                                   mem_size = mem_size,\n",
    "                                                                                   batch_size = batch_size, \n",
    "                                                                                   vnf_cnt = vnf_cnt,\n",
    "                                                                                   gamma        = gamma,\n",
    "                                                                                   eps_dec      = eps_dec,\n",
    "                                                                                   eps_min      = eps_min,\n",
    "                                                                                   replace      = replace,\n",
    "                                                                                   state_memory = state_memory,\n",
    "                                                                                   action_memory = action_memory,\n",
    "                                                                                   reward_memory = reward_memory,\n",
    "                                                                                   next_state_memory = next_state_memory,\n",
    "                                                                                   terminal_memory = terminal_memory,\n",
    "                                                                                   Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                                                                   drop_prob = drop_prob,\n",
    "                                                                                   chkpt_dir = 'models/',\n",
    "                                                                                   algo = 'DDQNAgent',\n",
    "                                                                                   env_name = 'BtEurope',\n",
    "                                                                                   count_VNF = vnf,\n",
    "                                                                                   done = done,\n",
    "                                                                                   amount_local_reward = amount_local_reward,\n",
    "                                                                                   reward_per_current_vnfstates = reward_per_current_vnfstates,\n",
    "                                                                                   microvnf_accpeted_action = microvnf_accpeted_action, \n",
    "                                                                                   accpeted_action = accpeted_action, \n",
    "                                                                                   updated_path_perVNF = updated_path_perVNF,\n",
    "                                                                                   reward_per_current_states = reward_per_current_states,\n",
    "                                                                                   nodal_avail = nodal_avail,\n",
    "                                                                                   sum_resouces = sum_resouces,\n",
    "                                                                                   state_space_size = state_space_size, \n",
    "                                                                                   threshold_delay = threshold_delay,\n",
    "                                                                                   rewardlink_per_current_states = rewardlink_per_current_states,\n",
    "                                                                                   vnf_link_resources = vnf_link_resources, \n",
    "                                                                                   temp_accepted_action = temp_accepted_action,\n",
    "                                                                                   original_actionspace = original_actionspace, #added om 07032022\n",
    "                                                                                   mvf_t_max = mvf_t_max, \n",
    "                                                                                   mvf_t_min = mvf_t_min, \n",
    "                                                                                   mvf_old_t_current = mvf_old_t_current, \n",
    "                                                                                   new_reward_per_current_states = new_reward_per_current_states,\n",
    "                                                                                   traffic_pred = traffic_pred,\n",
    "                                                                                   traffic_load = traffic_load\n",
    "                                                                                  )\n",
    "                                            count_VNF = emb_micro[0]\n",
    "                                            reward_per_current_states = emb_micro[1]\n",
    "                                            rewardlink_per_current_states = emb_micro[2]\n",
    "                                            accpeted_action = emb_micro[3]\n",
    "                                            updated_path_perVNF = emb_micro[4]\n",
    "                                            vnf_cnt = emb_micro[5] -1\n",
    "                                        else:\n",
    "                                            #print(\"here\")\n",
    "                                            #print(\"sfc:\", sfc, \"traffic_pred\", traffic_pred, classifier_training_status)\n",
    "                                            vals_sorted_action = list(vals_sorted_action)\n",
    "                                            micro_vnf, micro_vnf_res = -1, -1\n",
    "                                            observation   = [episode,sfc, vnf,vnf_res_req, pr_level, \n",
    "                                                             micro_vnf, micro_vnf_res, traffic_pred]\n",
    "                                            state_dim     = [len(observation)]\n",
    "                                            done_mVNF     = False\n",
    "                                            action_space = up_actionspace\n",
    "                                            action = agent.choose_action(observation,\n",
    "                                                                         prob_val,\n",
    "                                                                         temp_val, \n",
    "                                                                         original_actionspace)\n",
    "                                            vnf_end = time.perf_counter_ns()\n",
    "                                            vnf_exutime = vnf_end - vnf_start\n",
    "                                            if not action in up_actionspace:\n",
    "                                                reward_action_not = -1000\n",
    "                                            else:\n",
    "                                                reward_action_not = 0    \n",
    "                                            state = vnf \n",
    "                                            subjected_to = vnf_constraints(amount_local_reward, vnf_res_req,nodal_avail,\n",
    "                                                                           max_vnf_resource, episode, sfc, vnf, vnf_per_service,\n",
    "                                                                           sum_resouces, accpeted_action, switch_decomp)\n",
    "                                            local_rewardVNF = subjected_to.constraint_1(action)\n",
    "                                            if local_rewardVNF == amount_local_reward:\n",
    "                                                pref_node_val = 10 #amount_local_reward #amount_local_reward #1\n",
    "                                            else:\n",
    "                                                pref_node_val = 0\n",
    "                                                \n",
    "                                            # Modifying Reward Function\n",
    "                                            for node, available_res in env.network_nodes:   \n",
    "                                                dict_node_avail[node] = (available_res['emb_vnf'] / tot_vnf_per_core)\n",
    "                                                \n",
    "                                            # Part 1: Reward based on the quality of Nodes selected\n",
    "                                            reward_Node_based = local_rewardVNF*dict_node_avail[str(action)]\n",
    "                                            reward_vnfs.append(reward_Node_based)  \n",
    "                                            \n",
    "                                            # Part 2: Reward based on the Priority service\n",
    "                                            reward_pr = local_rewardVNF*priority_level\n",
    "                                            reward_vnfs.append(reward_pr)\n",
    "                                            \n",
    "                                            # Part 3: Reward based on the sfc_rel\n",
    "                                            reward_rel = local_rewardVNF*sfc_rel\n",
    "                                            reward_vnfs.append(reward_rel)\n",
    "                                            \n",
    "                                            # Part 4: Reward based on the traffic_pred\n",
    "                                            if traffic_load == None:\n",
    "                                                traffic_load = 0\n",
    "                                            else:\n",
    "                                                traffic_load = traffic_load\n",
    "\n",
    "                                            if local_rewardVNF == 1000:\n",
    "                                                reward_traffic_pred = local_rewardVNF*(traffic_pred*traffic_load)\n",
    "                                                #print(\"reward_traffic_pred:\", reward_traffic_pred, traffic_pred, local_rewardVNF)\n",
    "                                            else: \n",
    "                                                if traffic_pred == -1:\n",
    "                                                    reward_traffic_pred = -(local_rewardVNF*(traffic_pred*traffic_load))\n",
    "                                                else:\n",
    "                                                    reward_traffic_pred = (local_rewardVNF*(traffic_pred*traffic_load))\n",
    "                                            #print(\"reward_traffic_pred:\", reward_traffic_pred, traffic_pred, local_rewardVNF)\n",
    "                                            reward_vnfs.append(reward_traffic_pred)\n",
    "                                            \n",
    "                                            # Part 5: Reward based on scheduling delay\n",
    "                                            t_current = vnf_exutime\n",
    "                                            if t_max is None or  t_current >= t_max: t_max = t_current\n",
    "                                            if (t_current < t_max and t_min == 0) or (t_current <= t_min): t_min = t_current\n",
    "                                            elif (t_current >= old_t_current and t_min == 0):t_min = old_t_current\n",
    "                                            norm_time = 1 - Normalization(t_current, t_max, t_min)\n",
    "                                            time_reward = local_rewardVNF*norm_time\n",
    "                                            reward_vnfs.append(time_reward)\n",
    "                                            old_t_current = t_current\n",
    "                                            total_reward_vnf = round(sum(reward_vnfs), 4)\n",
    "                                            new_reward_per_current_states.append(total_reward_vnf)\n",
    "                                            if local_rewardVNF != -1000:\n",
    "                                                local_rewardVNF = local_rewardVNF\n",
    "                                                reward_per_current_states.append(local_rewardVNF)\n",
    "                                            else:\n",
    "                                                local_rewardVNF = local_rewardVNF \n",
    "                                                if switch_decomp is False:\n",
    "                                                    decomp = None\n",
    "                                                    decomp = []\n",
    "                                                else: \n",
    "                                                    decomp = subjected_to.decomp\n",
    "                                                    decomp = decomp    \n",
    "                                                global_decomp.append(decomp)\n",
    "                                                decompmvnf = True\n",
    "                                                if decomp is None or not decomp:\n",
    "                                                    global_reward = 0\n",
    "                                                    #print(\"==> no decomp possible\")\n",
    "                                                    local_reward = local_rewardVNF\n",
    "                                                    rewards = total_reward_vnf + global_reward\n",
    "                                                    agent.store_transition(observation, action, \n",
    "                                                                           rewards, next_state, \n",
    "                                                                           done, vnf_cnt)\n",
    "                                                    agent.learn()\n",
    "                                                    epsilon = agent.epsilon\n",
    "                                                    break\n",
    "                                                else:\n",
    "                                                    #print(\"VNF:\", vnf)\n",
    "                                                    #print(\"decomp\")\n",
    "                                                    local_rewardVNF = 1000 \n",
    "                                                    unique_id = decomp[2]\n",
    "                                                    if unique_id:\n",
    "                                                        micro_vnf_id.append(unique_id)    \n",
    "                                                for item in catalog_mVNF.items():\n",
    "                                                    #print(\"item\", item)\n",
    "                                                    mVNFID_temp = []\n",
    "                                                    for k in item[1]:\n",
    "                                                        repeat_mVNFid = {e for e in unique_id if e == k}\n",
    "                                                        mVNFID_temp = temp_accepted_action[item[0]]\n",
    "                                                        if(repeat_mVNFid):\n",
    "                                                            if k in mVNFID_temp:\n",
    "                                                                mVNFID_temp = mVNFID_temp\n",
    "                                                            else:\n",
    "                                                                mVNFID_temp = mVNFID_temp + [k]\n",
    "                                                        temp_accepted_action[item[0]] = mVNFID_temp\n",
    "                                                count_mvnf = len(unique_id)\n",
    "                                                count_mvnf_persfc.append(count_mvnf)\n",
    "                                                prev_maction = False\n",
    "                                                emb_micro = micro_const.microvnf_agent(episode = episode, \n",
    "                                                                   sfc = sfc, \n",
    "                                                                   vnf = vnf, \n",
    "                                                                   decomp = decomp , \n",
    "                                                                   n_actions = env.network_len_nodes, \n",
    "                                                                   lr = learning_rate, \n",
    "                                                                   epsilon = epsilon,\n",
    "                                                                   mem_size = mem_size,\n",
    "                                                                   batch_size = batch_size, \n",
    "                                                                   vnf_cnt = vnf_cnt,\n",
    "                                                                   gamma        = gamma,\n",
    "                                                                   eps_dec      = eps_dec,\n",
    "                                                                   eps_min      = eps_min,\n",
    "                                                                   replace      = replace,\n",
    "                                                                   state_memory = state_memory,\n",
    "                                                                   action_memory = action_memory,\n",
    "                                                                   reward_memory = reward_memory,\n",
    "                                                                   next_state_memory = next_state_memory,\n",
    "                                                                   terminal_memory = terminal_memory,\n",
    "                                                                   Neurons_per_HiddenLayer = Neurons_per_HiddenLayer,\n",
    "                                                                   drop_prob =drop_prob,\n",
    "                                                                   chkpt_dir = 'models/',\n",
    "                                                                   algo = 'DDQNAgent',\n",
    "                                                                   env_name = 'BtEurope',\n",
    "                                                                   count_VNF = vnf,\n",
    "                                                                   done = done,\n",
    "                                                                   amount_local_reward = amount_local_reward,\n",
    "                                                                   reward_per_current_vnfstates = reward_per_current_vnfstates,\n",
    "                                                                   microvnf_accpeted_action = microvnf_accpeted_action, \n",
    "                                                                   t_microvnf_accpeted_action = t_microvnf_accpeted_action, \n",
    "                                                                   accpeted_action = accpeted_action, \n",
    "                                                                   updated_path_perVNF = updated_path_perVNF,\n",
    "                                                                   reward_per_current_states = reward_per_current_states,\n",
    "                                                                   nodal_avail = nodal_avail,\n",
    "                                                                   sum_resouces = sum_resouces,\n",
    "                                                                   state_space_size = state_space_size, \n",
    "                                                                   threshold_delay = threshold_delay,\n",
    "                                                                   rewardlink_per_current_states = rewardlink_per_current_states,\n",
    "                                                                   vnf_link_resources = vnf_link_resources,\n",
    "                                                                   temp_accepted_action = temp_accepted_action,\n",
    "                                                                   prob_val = prob_val,\n",
    "                                                                   temp_val = temp_val,\n",
    "                                                                   original_actionspace = original_actionspace, #added on 07032022\n",
    "                                                                   prev_maction = prev_maction,\n",
    "                                                                   min_pr = min_pr, #added on 13th April 2022\n",
    "                                                                   max_pr = max_pr,  #added on 13th April 2022\n",
    "                                                                   mvf_t_max = mvf_t_max, \n",
    "                                                                   mvf_t_min = mvf_t_min, \n",
    "                                                                   mvf_old_t_current = mvf_old_t_current, \n",
    "                                                                   new_reward_per_current_states = new_reward_per_current_states,\n",
    "                                                                   traffic_pred = traffic_pred,\n",
    "                                                                   traffic_load = traffic_load\n",
    "                                                                   )\n",
    "                                                mvf_t_max = emb_micro[7]\n",
    "                                                mvf_t_min = emb_micro[8]\n",
    "                                                new_reward_per_current_states = emb_micro[10]\n",
    "                                                empt = []\n",
    "                                                temp_count = 0\n",
    "                                                for key in  microvnf_accpeted_action:\n",
    "                                                    if type(key) is list:\n",
    "                                                        pass\n",
    "                                                    else:\n",
    "                                                        empt = [unique_id[temp_count]]\n",
    "                                                        catalog_mVNF[key] = unique_id[temp_count]  \n",
    "                                                        if key in temp.keys():\n",
    "                                                            empt = empt + temp[key]  \n",
    "                                                        catalog_mVNF[key] = empt       \n",
    "                                                        temp = dict(sorted(catalog_mVNF.items(), \n",
    "                                                                           key=lambda item: item[1], \n",
    "                                                                           reverse = False)) \n",
    "                                                    temp_count +=1\n",
    "                                                count_VNF = emb_micro[0]\n",
    "                                                #print(\"count_VNF: \", count_VNF)\n",
    "                                                reward_per_current_states = emb_micro[1]\n",
    "                                                rewardlink_per_current_states = emb_micro[2]\n",
    "                                                accpeted_action = emb_micro[3]\n",
    "                                                updated_path_perVNF = emb_micro[4]\n",
    "                                                vnf_cnt = emb_micro[5]-1\n",
    "                                                global_mVNF_action.append(microvnf_accpeted_action)\n",
    "                                                global_updated_path_perVNF.append(updated_path_perVNF)    \n",
    "                                            #Constraint 3 and 4: : checking for the available resouce for Links#\n",
    "                                            vnf_link_resources = per_link \n",
    "                                            env_networkgraph   = env.network_graph\n",
    "                                            env_networkedges   = env.network_edges\n",
    "                                            subjected_to.constraint_3(state_space_size, \n",
    "                                                                   vnf_link_resources,\n",
    "                                                                   env_networkgraph, \n",
    "                                                                   env_networkedges, \n",
    "                                                                   threshold_delay, \n",
    "                                                                   updated_path_perSFC\n",
    "                                                                   )\n",
    "                                            rewardlink_per_current_states = subjected_to.local_reward_link\n",
    "                                            updated_path_perSFC = subjected_to.updated_path_perSFC\n",
    "                                            #Next state\n",
    "                                            next_state = vnf+1\n",
    "                                            if next_state >= state_space_size:\n",
    "                                                next_state = 0\n",
    "                                                traffic_pred = -1\n",
    "                                                next_pr_level = round(rand.uniform(0,1), 2)\n",
    "\n",
    "                                                if len(sorted_sch_list) == num_sfc_per_arrival+1: \n",
    "                                                    next_sfc = max(sorted_sch_list)+1\n",
    "                                                else: \n",
    "                                                    next_sfc = sorted_sch_list[num_sfc_per_arrival+1]\n",
    "\n",
    "                                                next_observation = [a, next_sfc,next_state,vnf_res_req, \n",
    "                                                                    next_pr_level, micro_vnf, micro_vnf_res, traffic_pred]\n",
    "                                            else: \n",
    "                                                next_state = next_state\n",
    "                                                next_observation = [a, sfc,next_state,vnf_res_req, pr_level, \n",
    "                                                                    micro_vnf, micro_vnf_res, traffic_pred]\n",
    "                                            state = observation \n",
    "                                            next_state = next_observation\n",
    "                                        #Reward Function\n",
    "                                        global_reward = 0\n",
    "                                        if (sum(reward_per_current_states) == amount_local_reward*state_space_size) and \\\n",
    "                                            (sum(rewardlink_per_current_states) == amount_local_reward*link_space_size):\n",
    "                                            done = True\n",
    "                                            deployed_sfc_per_episode.append(sfc)\n",
    "                                            deployed_priority_per_episode.append(pr_level)\n",
    "                                            vnf_node_resources = per_sfc \n",
    "                                            vnf_link_resources = per_link \n",
    "                                            reward_func.reward_vnf(global_reward,\n",
    "                                                                   rewardDelay_per_current_states, \n",
    "                                                                   rewardlink_per_current_states, \n",
    "                                                                   actions_per_iteration,\n",
    "                                                                   accpeted_action,\n",
    "                                                                   vnf_node_resources, \n",
    "                                                                   count_vnf, \n",
    "                                                                   updated_path_perSFC,\n",
    "                                                                   vnf_link_resources, \n",
    "                                                                   count_updated_path_perSFC,\n",
    "                                                                   count_success,\n",
    "                                                                   global_decomp,\n",
    "                                                                   emb_mVNFID,\n",
    "                                                                   switch_reconst,\n",
    "                                                                   global_updated_path_perVNF)\n",
    "                                            count_updated_path_perSFC = reward_func.count_updated_path_perSFC\n",
    "                                            count_VNF = reward_func.count_vnf\n",
    "                                            global_reward = reward_func.global_reward\n",
    "                                        local_reward = local_rewardVNF\n",
    "                                        rewards = total_reward_vnf + global_reward\n",
    "                                        rewards_persfc.append(rewards)\n",
    "                                        if done_mVNF == True:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            #storing the transistions\n",
    "                                            agent.store_transition(state, action, rewards, next_state, done, vnf_cnt)\n",
    "                                            #learning:  providing state, action, reward and next state to the NN for learning.\n",
    "                                            agent.learn()\n",
    "                                            epsilon = agent.epsilon\n",
    "                                    sum_rewards_persfc = sum(rewards_persfc)\n",
    "                                    rewards_perepi.append(sum_rewards_persfc)\n",
    "                                    \n",
    "                                    if state_space_size == count_VNF and link_space_size == count_updated_path_perSFC:\n",
    "                                        if decompmvnf is True:\n",
    "                                            count_succes_decompvnf +=1\n",
    "                                        sum_count_mvnf_persfc = sum(count_mvnf_persfc)\n",
    "                                        count_mvnf_perepi.append(sum_count_mvnf_persfc)\n",
    "                                        count_success += 1\n",
    "                                        count_accpeted +=1\n",
    "                                        if sfc_priority_class == \"high\":\n",
    "                                            cnt_High_pr_emb += 1\n",
    "                                        elif sfc_priority_class == \"medium\":\n",
    "                                            cnt_Medium_pr_emb += 1\n",
    "                                        else:\n",
    "                                            cnt_Low_pr_emb += 1\n",
    "                                            \n",
    "                                        epi_emb_sfc.append(sfc) #array of embedded SFCs\n",
    "                                        if sfc_rel >= 0.7:\n",
    "                                            epi_emb_sfc_h_rel.append(sfc)\n",
    "                                            if sfc_priority_class == 'high':\n",
    "                                                cnt_rel += 1\n",
    "                                        #print(cnt_rel)\n",
    "                                        \n",
    "                                        # update emd status: added 0n 11jan 2023\n",
    "                                        emb_status = True                                        \n",
    "                                    else:\n",
    "                                        # update emd status: added 0n 11jan 2023\n",
    "                                        epi_unemb_sfc.append(sfc)\n",
    "                                        emb_status = False\n",
    "                                        count_unsucces_decompvnf += 1\n",
    "                                        count_mvnf_perepi.append(0)\n",
    "                                    if res_exh is True:\n",
    "                                        res_exhausted +=1\n",
    "                                    num_sfc_per_arrival +=1\n",
    "                                    \n",
    "                                    sfc_end = time.perf_counter() #_ns()\n",
    "                                    sfc_deployment_time = round(sfc_end-sfc_start,6) # time take by the agent to deploy the agent\n",
    "                                    #print(\"SFC: \", sfc, sfc_deployment_time)\n",
    "                                    \n",
    "                                    # update emd status: added 0n 11jan 2023\n",
    "                                    global_services.emb_update(episode, sfc, vnf_per_service, emb_status, \n",
    "                                                               sfc_deployment_time)\n",
    "\n",
    "                                    if switch_sfc_expiring_check == True:\n",
    "                                        for t_sfc in temp_sorted_sch_list: \n",
    "                                            if sfc == t_sfc:\n",
    "                                                expected_wt_C = 0\n",
    "                                                ar_sfc_current_waitingtime[sfc] += expected_wt_C\n",
    "                                            else: \n",
    "                                                expected_wt_C = sfc_deployment_time\n",
    "                                                ar_sfc_current_waitingtime[t_sfc] += expected_wt_C\n",
    "                                        #print(\"expected_wt_C: \", expected_wt_C)\n",
    "                                        temp_sorted_sch_list.remove(sfc)\n",
    "                                    #else: #commented on 15th sept 2022\n",
    "                                     #   pass # commented on 15th sept 2022\n",
    "                                    #print(ar_sfc_current_waitingtime)   \n",
    "                                    network_overall_accepted_action.append(accpeted_action)\n",
    "                                    count_rejected = len(epi_temp_sfc_arrived) - count_accpeted\n",
    "                                    count_failed   = len(epi_temp_sfc_arrived) - count_success \n",
    "                                    #episode_pr_rewardfunc.append(sfc_pr_rewardfunc)\n",
    "                                    #print(\"sfc_priority: \", sfc_priority)\n",
    "                                    episode_priority.append(sfc_priority)\n",
    "                                    #episode_envpriority.append(sfc_envpriority)\n",
    "                                    episode_priority_err.append(sfc_priority_err)   \n",
    "                        \n",
    "                            #print(\"epi_expected_wt_B: \", epi_expected_wt_B)\n",
    "                            #print(\"episode\", episode)\n",
    "                            #overall_expected_wt_B[episode] = epi_expected_wt_B\n",
    "                            #print(\"overall_expected_wt_B: \", overall_expected_wt_B)\n",
    "                        \n",
    "                        overall_ar_sorted_sch_list.append(ar_sorted_sch_list)\n",
    "                        epi_cnt_sfc_starved_before_serving.append(cnt_sfc_starved_before_serving)\n",
    "                        epi_cnt_sfc_Prclass_starved_before_serving.append(cnt_sfc_starved_before_serving)\n",
    "                        overall_cnt_sfc_starved_before_serving.append(sfc_starved_before_serving)\n",
    "                        overall_cnt_sfc_Prclass_starved_before_serving.append(sfc_Prclass_starved_before_serving)\n",
    "                        epi_cnt_High_pr_emb.append(cnt_High_pr_emb)\n",
    "                        epi_cnt_Medium_pr_emb.append(cnt_Medium_pr_emb)\n",
    "                        epi_cnt_Low_pr_emb.append(cnt_Low_pr_emb)\n",
    "                        epi_pr_emb[\"H\"] = cnt_High_pr_emb\n",
    "                        epi_pr_emb[\"M\"] = cnt_Medium_pr_emb\n",
    "                        epi_pr_emb[\"L\"] = cnt_Low_pr_emb\n",
    "                        overall_epi_pr_emb.append(epi_pr_emb)\n",
    "                        overall_unemb_sfc.append(epi_unemb_sfc)\n",
    "                        overall_emb_sfc.append(epi_emb_sfc)\n",
    "                        overall_emb_sfc_h_rel.append(epi_emb_sfc_h_rel)\n",
    "\n",
    "                        #print(\"overall_epi_pr_emb: \", overall_epi_pr_emb)\n",
    "                        if len(overall_sfc_priority) != len(global_services.vNetwork_service_details_er):\n",
    "                            overall_sfc_priority.append(sfc_priority)\n",
    "                            epi_cnt_High_pr.append(cnt_High_pr)\n",
    "                            epi_cnt_Medium_pr.append(cnt_Medium_pr)\n",
    "                            epi_cnt_Low_pr.append(cnt_Low_pr)\n",
    "                            epi_pr[\"H\"] = cnt_High_pr\n",
    "                            epi_pr[\"M\"] = cnt_Medium_pr\n",
    "                            epi_pr[\"L\"] = cnt_Low_pr\n",
    "                            #print(\"epi_pr: \", epi_pr)\n",
    "                            #print(\"cnt_High_pr: \", cnt_High_pr, \"cnt_Medium_pr: \", cnt_Medium_pr, \"cnt_Low_pr: \", cnt_Low_pr)\n",
    "                            overall_epi_pr.append(epi_pr)\n",
    "                            status_cnt_ridge = False\n",
    "                            copy_overall_epi_pr = overall_epi_pr.copy()\n",
    "                        else:\n",
    "                            status_cnt_ridge = True\n",
    "                            copy_overall_episode_envpriority = overall_episode_envpriority.copy()\n",
    "                            #print(\"copy_overall_epi_pr: \",copy_overall_epi_pr)\n",
    "                            #print(\"XXXX\")\n",
    "                        \n",
    "                        overall_episode_priority.append(episode_priority)\n",
    "                        if sch_model == 'DDPG':  \n",
    "                            overall_episode_envpriority.append(sfc_envpriority)\n",
    "                        else: \n",
    "                            copy_overall_episode_envpriority = overall_episode_envpriority.copy()\n",
    "                            \n",
    "                        overall_episode_priority_err.append(episode_priority_err)\n",
    "                        overall_deployed_sfc_per_episode.append(deployed_sfc_per_episode)\n",
    "                        overall_deployed_priority_per_episode.append(deployed_priority_per_episode)\n",
    "                        sum_rewards_perepi = sum(rewards_perepi)\n",
    "                        network_overall_sum_rewards_perepi.append(sum_rewards_perepi)\n",
    "                        sum_count_mvnf_perepi = sum(count_mvnf_perepi)\n",
    "                        network_overall_sum_mvnf.append(sum_count_mvnf_perepi)\n",
    "                        network_overall_count_mvnf.append(count_mvnf_perepi)\n",
    "                        network_overall_accepted.append(count_success)\n",
    "                        network_overall_rejected.append(count_failed)\n",
    "                        tot_decompsfc = (count_succes_decompvnf + count_unsucces_decompvnf) \n",
    "                        undecomp_accepted = num_sfc-tot_decompsfc\n",
    "                        undecomp_rejected = tot_decompsfc\n",
    "                        monolithic_deploy_success.append(undecomp_accepted)\n",
    "                        monolithic_deploy_rejected.append(undecomp_rejected)\n",
    "                        overall_res_exhausted.append(res_exhausted)\n",
    "                        network_overall_tot_decompsfc.append(tot_decompsfc)\n",
    "                        network_overall_count_succes_decompvnf.append(count_succes_decompvnf)\n",
    "                        network_overall_count_unsucces_decompvnf.append(count_unsucces_decompvnf)\n",
    "                        overall_epi_traffic_packets_dict[a] = epi_traffic_packets_dict\n",
    "                        overall_epi_traffic_load_dict[a] = epi_traffic_load_dict\n",
    "                        \n",
    "                        if switch_priorty == True:\n",
    "                            if pr_model == 'DDPG' or pr_model == 'DDQN': \n",
    "                                pass\n",
    "                            else:\n",
    "                                if done_status == \"Train\":\n",
    "                                    cnt_macro_pr = tr_count_macro_pr\n",
    "                                    acc = round(tr_macro_acc,2)\n",
    "                                    epi_cnt_macro_pr = epi_tr_count_macro_pr\n",
    "                                    epi_acc = round((epi_cnt_macro_pr/num_sfc)*100,2)\n",
    "                                    epi_tr_correct_perc.append(epi_acc)\n",
    "                                elif done_status == \"Eval\":\n",
    "                                    cnt_macro_pr = eval_count_macro_pr\n",
    "                                    acc = eval_macro_acc\n",
    "                                    epi_cnt_macro_pr = epi_eval_count_macro_pr\n",
    "                                    epi_acc = round((epi_cnt_macro_pr/num_sfc)*100,2)\n",
    "                                else:\n",
    "                                    cnt_macro_pr = pred_count_macro_pr\n",
    "                                    acc = pred_macro_acc\n",
    "                                    epi_cnt_macro_pr = epi_pred_count_macro_pr\n",
    "                                    epi_acc = round((epi_cnt_macro_pr/num_sfc)*100,2)\n",
    "                                    epi_pred_correct_perc.append(epi_acc)\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                        if switch_clustering is True:\n",
    "                            #pass\n",
    "                            if a%num_episode==0:\n",
    "                                pass\n",
    "                                #clusterplot_val = class_traffic_clustering.clusteringplot(epi_clustering_status, \n",
    "                                #                                                          traffic_input_dataset, \n",
    "                                #                                                          cluster_label_val)\n",
    "\n",
    "                        if episode == (num_episode - 1):\n",
    "                            #classifers_plots = class_traffic_classifer.classifier_plot(classifier_measurements, num_sfc, baseline)\n",
    "                        \n",
    "                            title = 'Runtime for cluster model'\n",
    "                            #kmeans_runtime, Aggo_runtime, birch_runtime, GM_runtime\n",
    "                            Y_axis_Rcluster = [sum(kmeans_runtime), sum(Aggo_runtime), sum(birch_runtime), sum(GM_runtime)]\n",
    "                            #print(\"X_axis_Rcluster: \", X_axis_Rcluster)\n",
    "                            X_axis_Rcluster = ['K-means', 'Agglomerative', 'BIRCH', 'GM']\n",
    "                            fig = plt.figure()\n",
    "                            plt.barh(X_axis_Rcluster, Y_axis_Rcluster, color = '#27408B', edgecolor ='#2E2E2E',hatch = '///') \n",
    "                            plt.ylabel(\"Cluster Modelling\")\n",
    "                            plt.xlabel(\"Runtime (ns)\")\n",
    "                            plt.grid()\n",
    "                            plt.show()\n",
    "                            \n",
    "                            ##saving the figure\n",
    "                            figure_title = str(title)+\"_\"+str(num_sfc)+\"_\"+str(episode)\n",
    "                            #fig.savefig('Generated_plots/cluster/epi_2000_100sfc_agglo_DT/Plot_'+str(figure_title)+'.png')\n",
    "                            #fig.savefig('Generated_plots/cluster/epi_2000_100sfc_agglo_DT/Plot_'+str(figure_title)+'.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "                            #saving clustering data\n",
    "                            cluster_data = [\"Title: \" + str(figure_title ) +\\\n",
    "                                            \"X_axis_Rcluster: \" + str(X_axis_Rcluster) +\\\n",
    "                                            \"Y_axis_Rcluster: \" + str(Y_axis_Rcluster)]\n",
    "\n",
    "                            #fname = figure_title\n",
    "                            #print(fname)\n",
    "                            #f = open('Generated_plots/cluster/epi_2000_100sfc_agglo_DT/'+fname,\"a\")\n",
    "                            #f.write(str(cluster_data))\n",
    "                            #f.close()\n",
    "                            #print(\"XXXXXXXXXXXXXXXXX\")   \n",
    "                        print(\"*************\")\n",
    "                        print(\"cnt_sfc_arrival: \", cnt_sfc_arrival)\n",
    "                        print(\"Total Services for Episode\",a,\"is\",len(epi_temp_sfc_arrived), \"Num accepted\",count_success,\"Num rejected\",count_failed)\n",
    "                        #print(\"Episode\",a,\" Done Status: \", done_status,\" count_macro_pr: \", cnt_macro_pr,\" Epi count_macro_pr: \", epi_cnt_macro_pr, \"Epi Acc: \", epi_acc, acc)\n",
    "                        #print('Mean MAE: %.3f (%.3f)' % (m(scores), std(scores)))\n",
    "                        #print(\"Episode\",a,\" count_macro_pr: \", count_macro_pr, \" Episode count macro: \", epi_cnt_macro_pr)\n",
    "                        count_numVNF_perNode = []\n",
    "                        count_num_mVNF_perNode = []\n",
    "                        for ac in [i for i in range(env.network_len_nodes)]:\n",
    "                            count =0\n",
    "                            count_mVNF = 0\n",
    "                            for key in network_overall_accepted_action:\n",
    "                                for action in key:\n",
    "                                    if type(action) is list:\n",
    "                                        for maction in action:\n",
    "                                            if maction is ac:\n",
    "                                                count_mVNF += 1\n",
    "                                    else:\n",
    "                                        if action is ac:\n",
    "                                            count +=1\n",
    "                            count_numVNF_perNode.append(count)\n",
    "                            count_num_mVNF_perNode.append(count_mVNF)\n",
    "                        overall_count_numVNF_perNode.append(count_numVNF_perNode)\n",
    "                        overall_count_num_mVNF_perNode.append(count_num_mVNF_perNode)\n",
    "                        qq = list(env.network_graph.nodes(data=True))\n",
    "                        remain_cpu = []\n",
    "                        for key, val in qq:\n",
    "                            cpu = val['emb_vnf']\n",
    "                            #print(key, cpu)\n",
    "                            remain_cpu.append(cpu)\n",
    "                        sum_remain_cpu = sum(remain_cpu)\n",
    "                        #remaining BW\n",
    "                        residual_bw_res = []\n",
    "                        for key in env.network_edges: \n",
    "                            residual_bw_res.append(key[2]['bw'])\n",
    "                        overall_remain_cpu.append(sum_remain_cpu)\n",
    "                        overall_remain_bw.append(residual_bw_res)\n",
    "                        #print(\"epsilon\", round(epsilon, 2))\n",
    "                        network_overall_epsilon.append(round(epsilon, 2))\n",
    "                        a += 1\n",
    "                        \n",
    "                    from statistics import mean \n",
    "                    mean_network_overall_accepted = mean(network_overall_accepted)\n",
    "                    end = time.time()  \n",
    "                    runtime = end-start\n",
    "                    runtime = round(runtime,2)\n",
    "                    print(\"runtime\",runtime)\n",
    "                    topology_name = str('Netrail')\n",
    "\n",
    "                    overall_res  = []\n",
    "                    sum_overall_res = 0\n",
    "                    for per_episode in global_services.vNetwork_service_details_er:\n",
    "                        res_perepi = []\n",
    "                        sum_res_perepi = 0\n",
    "                        for per_sfc, per_link in per_episode:\n",
    "                            res_persfc = []\n",
    "                            sum_res_persfc = 0\n",
    "                            #print(per_sfc)\n",
    "                            for vnf, val in per_sfc:\n",
    "                                res_persfc.append(val['cpu_req'])\n",
    "                            #print(res_persfc)  \n",
    "                            sum_res_persfc = sum(res_persfc)\n",
    "                            #print(sum_res_persfc)\n",
    "                            res_perepi.append(sum_res_persfc)\n",
    "                        #print(res_perepi)\n",
    "                        sum_res_perepi = sum(res_perepi)\n",
    "                        #print(sum_res_perepi)\n",
    "                        overall_res.append(sum_res_perepi)\n",
    "                    sum_overall_res = sum(overall_res)\n",
    "\n",
    "                    upper_bound = round(tot_avail_res_network/min(overall_res), 2)\n",
    "                    lower_bound = round(tot_avail_res_network/max(overall_res),2)\n",
    "\n",
    "                    theo_bound  = [lower_bound, upper_bound]\n",
    "                    #print(theo_bound)\n",
    "\n",
    "                    Bound_Cal = ['overall_res: ' +str(overall_res), 'sum_overall_res: ' +str(sum_overall_res),\\\n",
    "                         'theo_bound: '  +str(theo_bound), 'min: ' +str(min(overall_res)), 'max: ' +str(max(overall_res))             \n",
    "                        ]\n",
    "                    #print(Bound_Cal)\n",
    "                    \n",
    "                    Epsilon = 1\n",
    "                    overall_vnf = (vnf_per_service*num_sfc)*num_episode\n",
    "                    Information = 'Information: '\n",
    "                    experience_replay = ['Experience Replay:',('Memory Size: '+str(mem_size)), ('Batch Size: '+ str(batch_size)), ('Replace: '+ str(replace))]\n",
    "                    topologyname = ['Tolpology:'+ topology_name, ('Node:'+ str(env.network_len_nodes)), ('Link:' + str(env.len_tot_links))]\n",
    "                    execution = [('Episode: '+ str(num_episode)), ('SFC: ' + str(num_sfc)), ('VNF per SFC: '+ str(vnf_per_service))]\n",
    "                    Info = [Information] + [Algorithm]+ [topologyname] + [Opimizer] + [execution]\n",
    "                    #print(Info)    \n",
    "                    Parameters = 'Parameters: '\n",
    "                    NN_para = ['Neural Network: '+ ('Neuron units: '+ str(Neurons_per_HiddenLayer)), 'Hidden Layer: '+str(Hidden_layer),\\\n",
    "                               'Drop Probability: '+ str(drop_prob)]\n",
    "                    learn_para = ['Learning :'+ ('Learning Rate: ' + str(learning_rate)), ('Epsilon: '+ str(Epsilon)), ('Gamma: '+ str(gamma)),\\\n",
    "                                 ('Epsilon Dec: '+ str(eps_dec)), 'Epsilon Min: '+ str(eps_min), Opimizer]\n",
    "                    RL_para = ['RL rewards function: ' + ('Reward: ' +str(amount_local_reward)) , ('Penalty: ' + str(amount_local_penalty))]\n",
    "\n",
    "                    Decomp = ['Decomp: ' +'switch_reconst: ' +str(switch_reconst),'switch_decomp: ' + str(switch_decomp),\\\n",
    "                              'n_state: ' + str(n_state), ' with No Constraint 2 '\n",
    "                             ]\n",
    "                    para = [Parameters] + [NN_para] + [learn_para]  + [experience_replay] + [RL_para] + [Decomp]\n",
    "                    Topology_resource_init = ['Topology Resouce Initilization: '+ ('Latency: ' + str(threshold_delay)),\\\n",
    "                                              'CPU core: ' + str(core_cpu), 'VNF per Core: ' + str(num_vnf_per_core),\\\n",
    "                                              'Total VNF per core' + str(tot_vnf_per_core)]\n",
    "                    Nodal_outage_para = ['Nodal Outage: ' + 'Outage Percentage: '+str(nodal_outage_percentage),\\\n",
    "                                         'No of nodes outaged: ' + str(round_nodal_outage)]\n",
    "                    # Result\n",
    "                    computation_time = 'Runtime: '+str(runtime)+ ' seconds'\n",
    "                    results = ['Results: ' + computation_time, \n",
    "                               'Total Generated VNF: ' + str(overall_vnf),'Mean of Overall VNF: '+ str(mean_network_overall_accepted),\\\n",
    "                               'SAR: '+str(network_overall_accepted), 'Monolithic_deploy_success: ' + str(monolithic_deploy_success),\\\n",
    "                               'Monolithic_deploy_rejected: ' + str(monolithic_deploy_rejected),\\\n",
    "                               'Network_overall_tot_decompsfc: ' + str(network_overall_tot_decompsfc),\\\n",
    "                               'Network_overall_count_succes_decompvnf: ' + str(network_overall_count_succes_decompvnf),\\\n",
    "                               'network_overall_count_unsucces_decompvnf: ' + str(network_overall_count_unsucces_decompvnf),\\\n",
    "                               'network_overall_sum_mvnf: ' + str(network_overall_sum_mvnf),\\\n",
    "                               'network_overall_count_mvnf: ' +str(network_overall_count_mvnf),\\\n",
    "                               'network_overall_sum_rewards_perepi: '+str(network_overall_sum_rewards_perepi),\\\n",
    "                               'network_overall_epsilon: '+ str(network_overall_epsilon)\n",
    "                              ]\n",
    "                    #print(results)\n",
    "                    resource_residual = ['Resource Residual: ' + 'VNF residual: '+ str(overall_remain_cpu) +\\\n",
    "                                         'Link Residual: '+ str(overall_remain_bw) + \\\n",
    "                                         #'VNF Distribution: '+str(Network_count_numVNF_perNode),\\\n",
    "                                         'overall_count_numVNF_perNode: ' + str(overall_count_numVNF_perNode),\\\n",
    "                                         'overall_count_num_mVNF_perNode: ' + str(overall_count_num_mVNF_perNode),\\\n",
    "                                         'overall_res_exhausted: '+ str(overall_res_exhausted)\n",
    "                                        ]\n",
    "                    priority = ['Priority: ' + 'pr_HL: ' +str(pr_HL) + \\\n",
    "                                'overall_episode_pr_rewardfunc: ' + str(overall_episode_pr_rewardfunc)+\\\n",
    "                                ' overall_sfc_priority: ' +str(overall_sfc_priority)+\\\n",
    "                                ' overall_episode_envpriority: '+ str(overall_episode_envpriority)+\\\n",
    "                                ' copy_overall_episode_envpriority: ' + str(copy_overall_episode_envpriority)+\\\n",
    "                                ' epi_cnt_sfc_starved_before_serving: ' + str( epi_cnt_sfc_starved_before_serving)+\\\n",
    "                                ' overall_cnt_sfc_starved_before_serving: ' + str(overall_cnt_sfc_starved_before_serving)+\\\n",
    "                                ' epi_cnt_sfc_Prclass_starved_before_serving: ' + str( epi_cnt_sfc_Prclass_starved_before_serving)+\\\n",
    "                                ' overall_cnt_sfc_Prclass_starved_before_serving: ' + str(overall_cnt_sfc_Prclass_starved_before_serving)+\\\n",
    "                                ' copy_overall_epi_pr: ' + str(copy_overall_epi_pr)+\\\n",
    "                                ' overall_epi_pr: ' + str(overall_epi_pr)+\\\n",
    "                                ' overall_epi_pr_emb: ' + str(overall_epi_pr_emb)\n",
    "                                #'over_count_micro_pr:' +str(over_count_micro_pr)\n",
    "                               ]\n",
    "                    \n",
    "                    #print(\"overall_emb_sfc: \", overall_emb_sfc, \" overall_emb_sfc_h_rel:\", overall_emb_sfc_h_rel)\n",
    "                    #print(\"overall_ar_sorted_sch_list: \", overall_ar_sorted_sch_list)\n",
    "                    embeded_sfc = ['Emb_SFC: ' +\\\n",
    "                                   'overall_emb_sfc: '+ str(overall_emb_sfc) +\\\n",
    "                                   'overall_UNemb_sfc: '+ str(overall_unemb_sfc) +\\\n",
    "                                   'overall_emb_sfc_h_rel:' + str(overall_emb_sfc_h_rel) +\\\n",
    "                                   'overall_ar_sorted_sch_list: ' +str(overall_ar_sorted_sch_list)\n",
    "                                  ]\n",
    "                    \n",
    "                    traffic_part = ['Traffic: '+ 'overall_epi_traffic_load_dict: ' + str(overall_epi_traffic_load_dict) +\\\n",
    "                                    'overall_epi_traffic_packets_dict: '+str(overall_epi_traffic_packets_dict) +\\\n",
    "                                    'classifier_measurements: ' +str(classifier_measurements)]\n",
    "\n",
    "                    #print(embeded_sfc)\n",
    "                    data = [Info] + [para] + [Topology_resource_init] + [Nodal_outage_para] + [computation_time] + [results] + \\\n",
    "                           [resource_residual] + [extra_note] + [Bound_Cal] + [priority]+[embeded_sfc] +[traffic_part]              \n",
    "                    #print(data)\n",
    "                    \n",
    "                    \n",
    "                    dt = datetime.now()\n",
    "                    # getting the timestamp\n",
    "                    ts = datetime.timestamp(dt)\n",
    "                    # convert to datetime\n",
    "                    date_time = datetime.fromtimestamp(ts)\n",
    "                    # convert timestamp to string in dd-mm-yyyy HH:MM:SS\n",
    "                    str_date_time = date_time.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "                    #print(str_date_time)\n",
    "                    \n",
    "                    fname = str(trial)+'_'+ str(topology_name)+\\\n",
    "                            '_cpucore_' + str(core_cpu)+'_num_vnf_per_core_'+\\\n",
    "                            str(num_vnf_per_core) +\\\n",
    "                            '_mean_' + str(round(mean_network_overall_accepted,3))+\\\n",
    "                            '_sfc_' + str(num_sfc)+\\\n",
    "                            '_epi_'+str(num_episode)+ '_runtime_' +str(runtime)+\\\n",
    "                            '_Clustering status_'+  str(switch_clustering)+\\\n",
    "                            '_switch_sfc_expiring_check_' +str(switch_sfc_expiring_check) +\\\n",
    "                            str(str_date_time)\n",
    "                    \n",
    "                           # '_switch_reconst_' +str(switch_reconst) +\\\n",
    "                            #'_switch_decomp_' + str(switch_decomp) + \\\n",
    "                            #'_switch_priorty_' + str(switch_priorty)+ \\\n",
    "                            #'_pr_model_'+ str(pr_model)+\\\n",
    "                            #'_sch_model_'+ str(sch_model)\n",
    "                            \n",
    "                    print(fname)\n",
    "                    f = open('Data_storage/epi_2000_100sfc_agglo_DT/'+fname,\"a\")\n",
    "                    f.write(str(data))\n",
    "                    f.close()\n",
    "                    print(\"XXXXXXXXXXXXXXXXX\")\n",
    "\n",
    "        if len(overall_sfc_priority) != len(global_services.vNetwork_service_details_er):\n",
    "            status_cnt_ridge = False\n",
    "        else:\n",
    "            status_cnt_ridge = True\n",
    "            #print(\"XXXX\")\n",
    "        #print(\"overall_ar_sorted_sch_list: \",overall_ar_sorted_sch_list)   \n",
    "        #print(\"overall_epi_pr: \", overall_epi_pr)\n",
    "        #print(\"\")\n",
    "        #print(\"Copy_overall_epi_pr: \", copy_overall_epi_pr)\n",
    "        #print(\"overall_epi_pr_emb: \", overall_epi_pr_emb)\n",
    "        #print(\"overall_sfc_priority: \", overall_sfc_priority)\n",
    "        sch_model_overall_cnt_sfc_starved_before_serving[sch_model] = overall_cnt_sfc_starved_before_serving    \n",
    "        sch_model_overall_cnt_sfc_Prclass_starved_before_serving[sch_model] = overall_cnt_sfc_Prclass_starved_before_serving \n",
    "        #print(sch_model_overall_cnt_sfc_starved_before_serving)\n",
    "        #print(sch_model_overall_cnt_sfc_Prclass_starved_before_serving)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fname)\n",
    "fname = fname\n",
    "#'00_Netrail_cpucore_12_num_vnf_per_core_8_mean_40.014_sfc_100_epi_2000_runtime_95484.37_Clustering status_False_switch_sfc_expiring_check_False25-03-2023'#fname #\"Netrail_12_4_100epi_cluster_false_classfier_flase_admission-control_false\"\n",
    "f = open('Data_storage/without_admission_controller/'+fname,\"a\")\n",
    "f.write(str(data))\n",
    "f.close()\n",
    "#print(\"XXXXXXXXXXXXXXXXX\")#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"work now\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
